package org apache lucene analysis fa
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import java io file
import java io ioexception
import java io inputstream
import java io inputstreamreader
import java io reader
import java util hashset
import java util hashtable
import java util set
import org apache lucene analysis analyzer
import org apache lucene analysis lowercasefilter
import org apache lucene analysis stopfilter
import org apache lucene analysis tokenstream
import org apache lucene analysis tokenizer
import org apache lucene analysis wordlistloader
import org apache lucene analysis ar arabiclettertokenizer
import org apache lucene analysis ar arabicnormalizationfilter
/**
* {@link analyzer} for persian.
* <p>
* this analyzer uses {@link arabiclettertokenizer} which implies tokenizing around
* zero-width non-joiner in addition to whitespace. some persian-specific variant forms (such as farsi
* yeh and keheh) are standardized. "stemming" is accomplished via stopwords.
* </p>
*/
public final class persiananalyzer extends analyzer
/**
* file containing default persian stopwords.
*
* default stopword list is from
* http://members.unine.ch/jacques.savoy/clef/index.html the stopword list is
* bsd-licensed.
*
*/
public final static string default_stopword_file
/**
* contains the stopwords used with the stopfilter.
*/
private set stoptable   new hashset
/**
* the comment character in the stopwords file. all lines prefixed with this
* will be ignored
*/
public static final string stopwords_comment
/**
* builds an analyzer with the default stop words:
* {@link #default_stopword_file}.
*/
public persiananalyzer
try
inputstream stream   persiananalyzer class
getresourceasstream default_stopword_file
inputstreamreader reader   new inputstreamreader stream
stoptable   wordlistloader getwordset reader  stopwords_comment
reader close
stream close
catch  ioexception e
// todo: throw ioexception
throw new runtimeexception e
/**
* builds an analyzer with the given stop words.
*/
public persiananalyzer string stopwords
stoptable   stopfilter makestopset stopwords
/**
* builds an analyzer with the given stop words.
*/
public persiananalyzer hashtable stopwords
stoptable   new hashset stopwords keyset
/**
* builds an analyzer with the given stop words. lines can be commented out
* using {@link #stopwords_comment}
*/
public persiananalyzer file stopwords  throws ioexception
stoptable   wordlistloader getwordset stopwords  stopwords_comment
/**
* creates a {@link tokenstream} which tokenizes all the text in the provided
* {@link reader}.
*
* @return a {@link tokenstream} built from a {@link arabiclettertokenizer}
*         filtered with {@link lowercasefilter},
*         {@link arabicnormalizationfilter},
*         {@link persiannormalizationfilter} and persian stop words
*/
public tokenstream tokenstream string fieldname  reader reader
tokenstream result   new arabiclettertokenizer reader
result   new lowercasefilter result
result   new arabicnormalizationfilter result
/* additional persian-specific normalization */
result   new persiannormalizationfilter result
/*
* the order here is important: the stopword list is normalized with the
* above!
*/
result   new stopfilter result  stoptable
return result
private class savedstreams
tokenizer source
tokenstream result
/**
* returns a (possibly reused) {@link tokenstream} which tokenizes all the text
* in the provided {@link reader}.
*
* @return a {@link tokenstream} built from a {@link arabiclettertokenizer}
*         filtered with {@link lowercasefilter},
*         {@link arabicnormalizationfilter},
*         {@link persiannormalizationfilter} and persian stop words
*/
public tokenstream reusabletokenstream string fieldname  reader reader
throws ioexception
savedstreams streams    savedstreams  getprevioustokenstream
if  streams    null
streams   new savedstreams
streams source   new arabiclettertokenizer reader
streams result   new lowercasefilter streams source
streams result   new arabicnormalizationfilter streams result
/* additional persian-specific normalization */
streams result   new persiannormalizationfilter streams result
/*
* the order here is important: the stopword list is normalized with the
* above!
*/
streams result   new stopfilter streams result  stoptable
setprevioustokenstream streams
else
streams source reset reader
return streams result