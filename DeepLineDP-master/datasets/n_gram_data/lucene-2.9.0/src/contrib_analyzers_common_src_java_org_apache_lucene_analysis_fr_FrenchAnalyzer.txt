package org apache lucene analysis fr
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import org apache lucene analysis analyzer
import org apache lucene analysis lowercasefilter
import org apache lucene analysis stopfilter
import org apache lucene analysis tokenstream
import org apache lucene analysis tokenizer
import org apache lucene analysis wordlistloader
import org apache lucene analysis standard standardfilter
import org apache lucene analysis standard standardtokenizer
import java io file
import java io ioexception
import java io reader
import java util hashset
import java util map
import java util set
/**
* {@link analyzer} for french language.
* <p>
* supports an external list of stopwords (words that
* will not be indexed at all) and an external list of exclusions (word that will
* not be stemmed, but indexed).
* a default set of stopwords is used unless an alternative list is specified, but the
* exclusion list is empty by default.
* </p>
*
* @version $id$
*/
public final class frenchanalyzer extends analyzer
/**
* extended list of typical french stopwords.
*/
public final static string french_stop_words
/**
* contains the stopwords used with the {@link stopfilter}.
*/
private set stoptable   new hashset
/**
* contains words that should be indexed but not stemmed.
*/
private set excltable   new hashset
/**
* builds an analyzer with the default stop words ({@link #french_stop_words}).
*/
public frenchanalyzer
stoptable   stopfilter makestopset french_stop_words
/**
* builds an analyzer with the given stop words.
*/
public frenchanalyzer string stopwords
stoptable   stopfilter makestopset stopwords
/**
* builds an analyzer with the given stop words.
* @throws ioexception
*/
public frenchanalyzer file stopwords  throws ioexception
stoptable   new hashset wordlistloader getwordset stopwords
/**
* builds an exclusionlist from an array of strings.
*/
public void setstemexclusiontable string exclusionlist
excltable   stopfilter makestopset exclusionlist
setprevioustokenstream null      force a new stemmer to be created
/**
* builds an exclusionlist from a map.
*/
public void setstemexclusiontable map exclusionlist
excltable   new hashset exclusionlist keyset
setprevioustokenstream null      force a new stemmer to be created
/**
* builds an exclusionlist from the words contained in the given file.
* @throws ioexception
*/
public void setstemexclusiontable file exclusionlist  throws ioexception
excltable   new hashset wordlistloader getwordset exclusionlist
setprevioustokenstream null      force a new stemmer to be created
/**
* creates a {@link tokenstream} which tokenizes all the text in the provided
* {@link reader}.
*
* @return a {@link tokenstream} built from a {@link standardtokenizer}
*         filtered with {@link standardfilter}, {@link stopfilter},
*         {@link frenchstemfilter} and {@link lowercasefilter}
*/
public final tokenstream tokenstream string fieldname  reader reader
if  fieldname    null  throw new illegalargumentexception
if  reader    null  throw new illegalargumentexception
tokenstream result   new standardtokenizer reader
result   new standardfilter result
result   new stopfilter result  stoptable
result   new frenchstemfilter result  excltable
// convert to lowercase after stemming!
result   new lowercasefilter result
return result
private class savedstreams
tokenizer source
tokenstream result
/**
* returns a (possibly reused) {@link tokenstream} which tokenizes all the
* text in the provided {@link reader}.
*
* @return a {@link tokenstream} built from a {@link standardtokenizer}
*         filtered with {@link standardfilter}, {@link stopfilter},
*         {@link frenchstemfilter} and {@link lowercasefilter}
*/
public tokenstream reusabletokenstream string fieldname  reader reader
throws ioexception
savedstreams streams    savedstreams  getprevioustokenstream
if  streams    null
streams   new savedstreams
streams source   new standardtokenizer reader
streams result   new standardfilter streams source
streams result   new stopfilter streams result  stoptable
streams result   new frenchstemfilter streams result  excltable
// convert to lowercase after stemming!
streams result   new lowercasefilter streams result
setprevioustokenstream streams
else
streams source reset reader
return streams result