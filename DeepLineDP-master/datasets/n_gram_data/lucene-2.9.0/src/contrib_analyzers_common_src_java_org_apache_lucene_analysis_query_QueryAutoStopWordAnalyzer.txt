package org apache lucene analysis query
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import org apache lucene index indexreader
import org apache lucene index term
import org apache lucene index termenum
import org apache lucene analysis analyzer
import org apache lucene analysis tokenstream
import org apache lucene analysis stopfilter
import org apache lucene util stringhelper
import java io ioexception
import java io reader
import java util
/**
* an {@link analyzer} used primarily at query time to wrap another analyzer and provide a layer of protection
* which prevents very common words from being passed into queries.
* <p>
* for very large indexes the cost
* of reading termdocs for a very common word can be  high. this analyzer was created after experience with
* a 38 million doc index which had a term in around 50% of docs and was causing termqueries for
* this term to take 2 seconds.
* </p>
* <p>
* use the various "addstopwords" methods in this class to automate the identification and addition of
* stop words found in an already existing index.
* </p>
*/
public class queryautostopwordanalyzer extends analyzer
analyzer delegate
hashmap stopwordsperfield   new hashmap
//the default maximum percentage (40%) of index documents which
//can contain a term, after which the term is considered to be a stop word.
public static final float defaultmaxdocfreqpercent   0 4f
/**
* initializes this analyzer with the analyzer object that actually produces the tokens
*
* @param delegate the choice of {@link analyzer} that is used to produce the token stream which needs filtering
*/
public queryautostopwordanalyzer analyzer delegate
this delegate   delegate
setoverridestokenstreammethod queryautostopwordanalyzer class
/**
* automatically adds stop words for all fields with terms exceeding the defaultmaxdocfreqpercent
*
* @param reader the {@link indexreader} which will be consulted to identify potential stop words that
*               exceed the required document frequency
* @return the number of stop words identified.
* @throws ioexception
*/
public int addstopwords indexreader reader  throws ioexception
return addstopwords reader  defaultmaxdocfreqpercent
/**
* automatically adds stop words for all fields with terms exceeding the maxdocfreqpercent
*
* @param reader     the {@link indexreader} which will be consulted to identify potential stop words that
*                   exceed the required document frequency
* @param maxdocfreq the maximum number of index documents which can contain a term, after which
*                   the term is considered to be a stop word
* @return the number of stop words identified.
* @throws ioexception
*/
public int addstopwords indexreader reader  int maxdocfreq  throws ioexception
int numstopwords   0
collection fieldnames   reader getfieldnames indexreader fieldoption indexed
for  iterator iter   fieldnames iterator    iter hasnext
string fieldname    string  iter next
numstopwords    addstopwords reader  fieldname  maxdocfreq
return numstopwords
/**
* automatically adds stop words for all fields with terms exceeding the maxdocfreqpercent
*
* @param reader        the {@link indexreader} which will be consulted to identify potential stop words that
*                      exceed the required document frequency
* @param maxpercentdocs the maximum percentage (between 0.0 and 1.0) of index documents which
*                      contain a term, after which the word is considered to be a stop word.
* @return the number of stop words identified.
* @throws ioexception
*/
public int addstopwords indexreader reader  float maxpercentdocs  throws ioexception
int numstopwords   0
collection fieldnames   reader getfieldnames indexreader fieldoption indexed
for  iterator iter   fieldnames iterator    iter hasnext
string fieldname    string  iter next
numstopwords    addstopwords reader  fieldname  maxpercentdocs
return numstopwords
/**
* automatically adds stop words for the given field with terms exceeding the maxpercentdocs
*
* @param reader         the {@link indexreader} which will be consulted to identify potential stop words that
*                       exceed the required document frequency
* @param fieldname      the field for which stopwords will be added
* @param maxpercentdocs the maximum percentage (between 0.0 and 1.0) of index documents which
*                       contain a term, after which the word is considered to be a stop word.
* @return the number of stop words identified.
* @throws ioexception
*/
public int addstopwords indexreader reader  string fieldname  float maxpercentdocs  throws ioexception
return addstopwords reader  fieldname   int   reader numdocs     maxpercentdocs
/**
* automatically adds stop words for the given field with terms exceeding the maxpercentdocs
*
* @param reader     the {@link indexreader} which will be consulted to identify potential stop words that
*                   exceed the required document frequency
* @param fieldname  the field for which stopwords will be added
* @param maxdocfreq the maximum number of index documents which
*                   can contain a term, after which the term is considered to be a stop word.
* @return the number of stop words identified.
* @throws ioexception
*/
public int addstopwords indexreader reader  string fieldname  int maxdocfreq  throws ioexception
hashset stopwords   new hashset
string internedfieldname   stringhelper intern fieldname
termenum te   reader terms new term fieldname
term term   te term
while  term    null
if  term field      internedfieldname
break
if  te docfreq   > maxdocfreq
stopwords add term text
if   te next
break
term   te term
stopwordsperfield put fieldname  stopwords
/* if the stopwords for a field are changed,
* then saved streams for that field are erased.
*/
map streammap    map  getprevioustokenstream
if  streammap    null
streammap remove fieldname
return stopwords size
public tokenstream tokenstream string fieldname  reader reader
tokenstream result
try
result   delegate reusabletokenstream fieldname  reader
catch  ioexception e
result   delegate tokenstream fieldname  reader
hashset stopwords    hashset  stopwordsperfield get fieldname
if  stopwords    null
result   new stopfilter result  stopwords
return result
private class savedstreams
/* the underlying stream */
tokenstream wrapped
/*
* when there are no stopwords for the field, refers to wrapped.
* if there stopwords, it is a stopfilter around wrapped.
*/
tokenstream withstopfilter
public tokenstream reusabletokenstream string fieldname  reader reader
throws ioexception
if  overridestokenstreammethod
// lucene-1678: force fallback to tokenstream() if we
// have been subclassed and that subclass overrides
// tokenstream but not reusabletokenstream
return tokenstream fieldname  reader
/* map of savedstreams for each field */
map streammap    map  getprevioustokenstream
if  streammap    null
streammap   new hashmap
setprevioustokenstream streammap
savedstreams streams    savedstreams  streammap get fieldname
if  streams    null
/* an entry for this field does not exist, create one */
streams   new savedstreams
streammap put fieldname  streams
streams wrapped   delegate reusabletokenstream fieldname  reader
/* if there are any stopwords for the field, save the stopfilter */
hashset stopwords    hashset  stopwordsperfield get fieldname
if  stopwords    null
streams withstopfilter   new stopfilter streams wrapped  stopwords
else
streams withstopfilter   streams wrapped
else
/*
* an entry for this field exists, verify the wrapped stream has not
* changed. if it has not, reuse it, otherwise wrap the new stream.
*/
tokenstream result   delegate reusabletokenstream fieldname  reader
if  result    streams wrapped
/* the wrapped analyzer reused the stream */
streams withstopfilter reset
else
/*
* the wrapped analyzer did not. if there are any stopwords for the
* field, create a new stopfilter around the new stream
*/
streams wrapped   result
hashset stopwords    hashset  stopwordsperfield get fieldname
if  stopwords    null
streams withstopfilter   new stopfilter streams wrapped  stopwords
else
streams withstopfilter   streams wrapped
return streams withstopfilter
/**
* provides information on which stop words have been identified for a field
*
* @param fieldname the field for which stop words identified in "addstopwords"
*                  method calls will be returned
* @return the stop words identified for a field
*/
public string getstopwords string fieldname
string result
hashset stopwords    hashset  stopwordsperfield get fieldname
if  stopwords    null
result    string  stopwords toarray new string
else
result   new string
return result
/**
* provides information on which stop words have been identified for all fields
*
* @return the stop words (as terms)
*/
public term getstopwords
arraylist allstopwords   new arraylist
for  iterator iter   stopwordsperfield keyset   iterator    iter hasnext
string fieldname    string  iter next
hashset stopwords    hashset  stopwordsperfield get fieldname
for  iterator iterator   stopwords iterator    iterator hasnext
string text    string  iterator next
allstopwords add new term fieldname  text
return  term  allstopwords toarray new term