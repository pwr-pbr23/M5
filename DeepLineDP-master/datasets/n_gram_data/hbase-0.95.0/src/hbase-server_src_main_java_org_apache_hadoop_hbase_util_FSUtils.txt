/**
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase util
import java io bytearrayinputstream
import java io datainputstream
import java io eofexception
import java io filenotfoundexception
import java io ioexception
import java io inputstream
import java lang reflect method
import java net uri
import java net urisyntaxexception
import java util arraylist
import java util hashmap
import java util list
import java util map
import java util regex pattern
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop classification interfaceaudience
import org apache hadoop classification interfacestability
import org apache hadoop conf configuration
import org apache hadoop fs blocklocation
import org apache hadoop fs fsdatainputstream
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop fs permission fsaction
import org apache hadoop fs permission fspermission
import org apache hadoop hbase clusterid
import org apache hadoop hbase exceptions deserializationexception
import org apache hadoop hbase hcolumndescriptor
import org apache hadoop hbase hconstants
import org apache hadoop hbase hdfsblocksdistribution
import org apache hadoop hbase hregioninfo
import org apache hadoop hbase remoteexceptionhandler
import org apache hadoop hbase exceptions filesystemversionexception
import org apache hadoop hbase master hmaster
import org apache hadoop hbase protobuf protobufutil
import org apache hadoop hbase protobuf generated fsprotos
import org apache hadoop hbase regionserver hregion
import org apache hadoop hdfs distributedfilesystem
import org apache hadoop io ioutils
import org apache hadoop io sequencefile
import org apache hadoop security accesscontrolexception
import org apache hadoop security usergroupinformation
import org apache hadoop util reflectionutils
import org apache hadoop util stringutils
import com google common primitives ints
import com google protobuf invalidprotocolbufferexception
/**
* utility methods for interacting with the underlying file system.
*/
@interfaceaudience public
@interfacestability evolving
public abstract class fsutils
private static final log log   logfactory getlog fsutils class
/** full access permissions (starting point for a umask) */
private static final string full_rwx_permissions
protected fsutils
super
/**
* compare of path component. does not consider schema; i.e. if schemas different but <code>path
* <code> starts with <code>rootpath<code>, then the function returns true
* @param rootpath
* @param path
* @return true if <code>path</code> starts with <code>rootpath</code>
*/
public static boolean isstartingwithpath final path rootpath  final string path
string urirootpath   rootpath touri   getpath
string tailuripath    new path path   touri   getpath
return tailuripath startswith urirootpath
/**
* compare path component of the path uri; e.g. if hdfs://a/b/c and /a/b/c, it will compare the
* '/a/b/c' part. does not consider schema; i.e. if schemas different but path or subpath matches,
* the two will equate.
* @param pathtosearch path we will be trying to match.
* @param pathtail
* @return true if <code>pathtail</code> is tail on the path of <code>pathtosearch</code>
*/
public static boolean ismatchingtail final path pathtosearch  string pathtail
return ismatchingtail pathtosearch  new path pathtail
/**
* compare path component of the path uri; e.g. if hdfs://a/b/c and /a/b/c, it will compare the
* '/a/b/c' part. if you passed in 'hdfs://a/b/c and b/c, it would return true.  does not consider
* schema; i.e. if schemas different but path or subpath matches, the two will equate.
* @param pathtosearch path we will be trying to match.
* @param pathtail
* @return true if <code>pathtail</code> is tail on the path of <code>pathtosearch</code>
*/
public static boolean ismatchingtail final path pathtosearch  final path pathtail
if  pathtosearch depth      pathtail depth    return false
path tailpath   pathtail
string tailname
path tosearch   pathtosearch
string tosearchname
boolean result   false
do
tailname   tailpath getname
if  tailname    null    tailname length   <  0
result   true
break
tosearchname   tosearch getname
if  tosearchname    null    tosearchname length   <  0  break
// move up a parent on each path for next go around.  path doesn't let us go off the end.
tailpath   tailpath getparent
tosearch   tosearch getparent
while tailname equals tosearchname
return result
public static fsutils getinstance filesystem fs  configuration conf
string scheme   fs geturi   getscheme
if  scheme    null
log warn
fs geturi
scheme
class<?> fsutilsclass   conf getclass
scheme      fshdfsutils class      default to hdfs impl
fsutils fsutils    fsutils reflectionutils newinstance fsutilsclass  conf
return fsutils
/**
* delete if exists.
* @param fs filesystem object
* @param dir directory to delete
* @return true if deleted <code>dir</code>
* @throws ioexception e
*/
public static boolean deletedirectory final filesystem fs  final path dir
throws ioexception
return fs exists dir     fs delete dir  true
/**
* create the specified file on the filesystem. by default, this will:
* <ol>
* <li>overwrite the file if it exists</li>
* <li>apply the umask in the configuration (if it is enabled)</li>
* <li>use the fs configured buffer size (or 4096 if not set)</li>
* <li>use the default replication</li>
* <li>use the default block size</li>
* <li>not track progress</li>
* </ol>
*
* @param fs {@link filesystem} on which to write the file
* @param path {@link path} to the file to write
* @return output stream to the created file
* @throws ioexception if the file cannot be created
*/
public static fsdataoutputstream create filesystem fs  path path
fspermission perm  throws ioexception
return create fs  path  perm  true
/**
* create the specified file on the filesystem. by default, this will:
* <ol>
* <li>apply the umask in the configuration (if it is enabled)</li>
* <li>use the fs configured buffer size (or 4096 if not set)</li>
* <li>use the default replication</li>
* <li>use the default block size</li>
* <li>not track progress</li>
* </ol>
*
* @param fs {@link filesystem} on which to write the file
* @param path {@link path} to the file to write
* @param perm
* @param overwrite whether or not the created file should be overwritten.
* @return output stream to the created file
* @throws ioexception if the file cannot be created
*/
public static fsdataoutputstream create filesystem fs  path path
fspermission perm  boolean overwrite  throws ioexception
log debug     path       perm
return fs create path  perm  overwrite
fs getconf   getint    4096
fs getdefaultreplication    fs getdefaultblocksize    null
/**
* get the file permissions specified in the configuration, if they are
* enabled.
*
* @param fs filesystem that the file will be created on.
* @param conf configuration to read for determining if permissions are
*          enabled and which to use
* @param permssionconfkey property key in the configuration to use when
*          finding the permission
* @return the permission to use when creating a new file on the fs. if
*         special permissions are not specified in the configuration, then
*         the default permissions on the the fs will be returned.
*/
public static fspermission getfilepermissions final filesystem fs
final configuration conf  final string permssionconfkey
boolean enablepermissions   conf getboolean
hconstants enable_data_file_umask  false
if  enablepermissions
try
fspermission perm   new fspermission full_rwx_permissions
// make sure that we have a mask, if not, go default.
string mask   conf get permssionconfkey
if  mask    null
return fspermission getdefault
// appy the umask
fspermission umask   new fspermission mask
return perm applyumask umask
catch  illegalargumentexception e
log warn
conf get permssionconfkey
e
return fspermission getdefault
return fspermission getdefault
/**
* checks to see if the specified file system is available
*
* @param fs filesystem
* @throws ioexception e
*/
public static void checkfilesystemavailable final filesystem fs
throws ioexception
if    fs instanceof distributedfilesystem
return
ioexception exception   null
distributedfilesystem dfs    distributedfilesystem  fs
try
if  dfs exists new path
return
catch  ioexception e
exception   remoteexceptionhandler checkioexception e
try
fs close
catch  exception e
log error    e
ioexception io   new ioexception
io initcause exception
throw io
/**
* we use reflection because {@link distributedfilesystem#setsafemode(
* fsconstants.safemodeaction action, boolean ischecked)} is not in hadoop 1.1
*
* @param dfs
* @return whether we're in safe mode
* @throws ioexception
*/
private static boolean isinsafemode distributedfilesystem dfs  throws ioexception
boolean insafemode   false
try
method m   distributedfilesystem class getmethod    new class<?>
org apache hadoop hdfs protocol fsconstants safemodeaction class  boolean class
insafemode    boolean  m invoke dfs
org apache hadoop hdfs protocol fsconstants safemodeaction safemode_get  true
catch  exception e
if  e instanceof ioexception  throw  ioexception  e
// check whether dfs is on safemode.
insafemode   dfs setsafemode
org apache hadoop hdfs protocol fsconstants safemodeaction safemode_get
return insafemode
/**
* check whether dfs is in safemode.
* @param conf
* @throws ioexception
*/
public static void checkdfssafemode final configuration conf
throws ioexception
boolean isinsafemode   false
filesystem fs   filesystem get conf
if  fs instanceof distributedfilesystem
distributedfilesystem dfs    distributedfilesystem fs
isinsafemode   isinsafemode dfs
if  isinsafemode
throw new ioexception
/**
* verifies current version of file system
*
* @param fs filesystem object
* @param rootdir root hbase directory
* @return null if no version file exists, version string otherwise.
* @throws ioexception e
* @throws org.apache.hadoop.hbase.exceptions.deserializationexception
*/
public static string getversion filesystem fs  path rootdir
throws ioexception  deserializationexception
path versionfile   new path rootdir  hconstants version_file_name
filestatus status   null
try
// hadoop 2.0 throws fnfe if directory does not exist.
// hadoop 1.0 returns null if directory does not exist.
status   fs liststatus versionfile
catch  filenotfoundexception fnfe
return null
if  status    null    status length    0  return null
string version   null
byte  content   new byte  getlen  ]
fsdatainputstream s   fs open versionfile
try
ioutils readfully s  content  0  content length
if  protobufutil ispbmagicprefix content
version   parseversionfrom content
else
// presume it pre-pb format.
inputstream is   new bytearrayinputstream content
datainputstream dis   new datainputstream is
try
version   dis readutf
finally
dis close
// update the format
log info     version
setversion fs  rootdir  version  0  hconstants default_version_file_write_attempts
catch  eofexception eof
log warn
finally
s close
return version
/**
* parse the content of the ${hbase_rootdir}/hbase.version file.
* @param bytes the byte content of the hbase.version file.
* @return the version found in the file as a string.
* @throws deserializationexception
*/
static string parseversionfrom final byte  bytes
throws deserializationexception
protobufutil expectpbmagicprefix bytes
int pblen   protobufutil lengthofpbmagic
fsprotos hbaseversionfilecontent builder builder
fsprotos hbaseversionfilecontent newbuilder
fsprotos hbaseversionfilecontent filecontent
try
filecontent   builder mergefrom bytes  pblen  bytes length   pblen  build
return filecontent getversion
catch  invalidprotocolbufferexception e
// convert
throw new deserializationexception e
/**
* create the content to write into the ${hbase_rootdir}/hbase.version file.
* @param version version to persist
* @return serialized protobuf with <code>version</code> content and a bit of pb magic for a prefix.
*/
static byte  toversionbytearray final string version
fsprotos hbaseversionfilecontent builder builder
fsprotos hbaseversionfilecontent newbuilder
return protobufutil prependpbmagic builder setversion version  build   tobytearray
/**
* verifies current version of file system
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @param message if true, issues a message on system.out
*
* @throws ioexception e
* @throws deserializationexception
*/
public static void checkversion filesystem fs  path rootdir  boolean message
throws ioexception  deserializationexception
checkversion fs  rootdir  message  0  hconstants default_version_file_write_attempts
/**
* verifies current version of file system
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @param message if true, issues a message on system.out
* @param wait wait interval
* @param retries number of times to retry
*
* @throws ioexception e
* @throws deserializationexception
*/
public static void checkversion filesystem fs  path rootdir
boolean message  int wait  int retries
throws ioexception  deserializationexception
string version   getversion fs  rootdir
if  version    null
if   metaregionexists fs  rootdir
// rootdir is empty (no version file and no root region)
// just create new version file (hbase-1195)
setversion fs  rootdir  wait  retries
return
else if  version compareto hconstants file_system_version     0  return
// version is deprecated require migration
// output on stdout so user sees it in terminal.
string msg
version
hconstants file_system_version
if  message
system out println     msg
throw new filesystemversionexception msg
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir
throws ioexception
setversion fs  rootdir  hconstants file_system_version  0
hconstants default_version_file_write_attempts
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root
* @param wait time to wait for retry
* @param retries number of times to retry before failing
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir  int wait  int retries
throws ioexception
setversion fs  rootdir  hconstants file_system_version  wait  retries
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root directory
* @param version version to set
* @param wait time to wait for retry
* @param retries number of times to retry before throwing an ioexception
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir  string version
int wait  int retries  throws ioexception
path versionfile   new path rootdir  hconstants version_file_name
while  true
try
fsdataoutputstream s   fs create versionfile
s write toversionbytearray version
s close
log debug     rootdir tostring         version
return
catch  ioexception e
if  retries > 0
log warn     rootdir tostring        e
fs delete versionfile  false
try
if  wait > 0
thread sleep wait
catch  interruptedexception ex
// ignore
retries
else
throw e
/**
* checks that a cluster id file exists in the hbase root directory
* @param fs the root directory filesystem
* @param rootdir the hbase root directory in hdfs
* @param wait how long to wait between retries
* @return <code>true</code> if the file exists, otherwise <code>false</code>
* @throws ioexception if checking the filesystem fails
*/
public static boolean checkclusteridexists filesystem fs  path rootdir
int wait  throws ioexception
while  true
try
path filepath   new path rootdir  hconstants cluster_id_file_name
return fs exists filepath
catch  ioexception ioe
if  wait > 0
log warn     rootdir tostring
wait   stringutils stringifyexception ioe
try
thread sleep wait
catch  interruptedexception ie
thread interrupted
break
else
throw ioe
return false
/**
* returns the value of the unique cluster id stored for this hbase instance.
* @param fs the root directory filesystem
* @param rootdir the path to the hbase root directory
* @return the unique cluster identifier
* @throws ioexception if reading the cluster id file fails
*/
public static clusterid getclusterid filesystem fs  path rootdir
throws ioexception
path idpath   new path rootdir  hconstants cluster_id_file_name
clusterid clusterid   null
filestatus status   fs exists idpath ? fs getfilestatus idpath    null
if  status    null
int len   ints checkedcast status getlen
byte  content   new byte
fsdatainputstream in   fs open idpath
try
in readfully content
catch  eofexception eof
log warn     idpath tostring
finally
in close
try
clusterid   clusterid parsefrom content
catch  deserializationexception e
throw new ioexception     bytes tostring content   e
// if not pb'd, make it so.
if   protobufutil ispbmagicprefix content   rewriteaspb fs  rootdir  idpath  clusterid
return clusterid
else
log warn     idpath tostring
return clusterid
/**
* @param cid
* @throws ioexception
*/
private static void rewriteaspb final filesystem fs  final path rootdir  final path p
final clusterid cid
throws ioexception
// rewrite the file as pb.  move aside the old one first, write new
// then delete the moved-aside file.
path movedasidename   new path p       system currenttimemillis
if   fs rename p  movedasidename   throw new ioexception     p
setclusterid fs  rootdir  cid  100
if   fs delete movedasidename  false
throw new ioexception     movedasidename
log debug
/**
* writes a new unique identifier for this cluster to the "hbase.id" file
* in the hbase root directory
* @param fs the root directory filesystem
* @param rootdir the path to the hbase root directory
* @param clusterid the unique identifier to store
* @param wait how long (in milliseconds) to wait between retries
* @throws ioexception if writing to the filesystem fails and no wait value
*/
public static void setclusterid filesystem fs  path rootdir  clusterid clusterid
int wait  throws ioexception
while  true
try
path filepath   new path rootdir  hconstants cluster_id_file_name
fsdataoutputstream s   fs create filepath
try
s write clusterid tobytearray
finally
s close
if  log isdebugenabled
log debug     filepath tostring         clusterid
return
catch  ioexception ioe
if  wait > 0
log warn     rootdir tostring
wait       stringutils stringifyexception ioe
try
thread sleep wait
catch  interruptedexception ie
thread interrupted
break
else
throw ioe
/**
* verifies root directory path is a valid uri with a scheme
*
* @param root root directory path
* @return passed <code>root</code> argument.
* @throws ioexception if not a valid uri with a scheme
*/
public static path validaterootpath path root  throws ioexception
try
uri rooturi   new uri root tostring
string scheme   rooturi getscheme
if  scheme    null
throw new ioexception
return root
catch  urisyntaxexception e
ioexception io   new ioexception
hconstants hbase_dir
io initcause e
throw io
/**
* checks for the presence of the root path (using the provided conf object) in the given path. if
* it exists, this method removes it and returns the string representation of remaining relative path.
* @param path
* @param conf
* @return string representation of the remaining relative path
* @throws ioexception
*/
public static string removerootpath path path  final configuration conf  throws ioexception
path root   fsutils getrootdir conf
string pathstr   path tostring
// check that the path is absolute... it has the root path in it.
if   pathstr startswith root tostring     return pathstr
// if not, return as it is.
return pathstr substring root tostring   length     1     remove the   too
/**
* if dfs, check safe mode and if so, wait until we clear it.
* @param conf configuration
* @param wait sleep between retries
* @throws ioexception e
*/
public static void waitonsafemode final configuration conf
final long wait
throws ioexception
filesystem fs   filesystem get conf
if    fs instanceof distributedfilesystem   return
distributedfilesystem dfs    distributedfilesystem fs
// make sure dfs is not in safe mode
while  isinsafemode dfs
log info
try
thread sleep wait
catch  interruptedexception e
//continue
/**
* return the 'path' component of a path.  in hadoop, path is an uri.  this
* method returns the 'path' component of a path's uri: e.g. if a path is
* <code>hdfs://example.org:9000/hbase_trunk/testtable/compaction.dir</code>,
* this method returns <code>/hbase_trunk/testtable/compaction.dir</code>.
* this method is useful if you want to print out a path without qualifying
* filesystem instance.
* @param p filesystem path whose 'path' component we are to return.
* @return path portion of the filesystem
*/
public static string getpath path p
return p touri   getpath
/**
* @param c configuration
* @return path to hbase root directory: i.e. <code>hbase.rootdir</code> from
* configuration as a qualified path.
* @throws ioexception e
*/
public static path getrootdir final configuration c  throws ioexception
path p   new path c get hconstants hbase_dir
filesystem fs   p getfilesystem c
return p makequalified fs
public static void setrootdir final configuration c  final path root  throws ioexception
c set hconstants hbase_dir  root tostring
public static void setfsdefault final configuration c  final path root  throws ioexception
c set    root tostring           for hadoop 0 21
c set    root tostring        for hadoop 0 20
/**
* checks if root region exists
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @return true if exists
* @throws ioexception e
*/
public static boolean metaregionexists filesystem fs  path rootdir
throws ioexception
path rootregiondir
hregion getregiondir rootdir  hregioninfo first_meta_regioninfo
return fs exists rootregiondir
/**
* compute hdfs blocks distribution of a given file, or a portion of the file
* @param fs file system
* @param status file status of the file
* @param start start position of the portion
* @param length length of the portion
* @return the hdfs blocks distribution
*/
static public hdfsblocksdistribution computehdfsblocksdistribution
final filesystem fs  filestatus status  long start  long length
throws ioexception
hdfsblocksdistribution blocksdistribution   new hdfsblocksdistribution
blocklocation  blocklocations
fs getfileblocklocations status  start  length
for blocklocation bl   blocklocations
string  hosts   bl gethosts
long len   bl getlength
blocksdistribution addhostsandblockweight hosts  len
return blocksdistribution
/**
* runs through the hbase rootdir and checks all stores have only
* one file in them -- that is, they've been major compacted.  looks
* at root and meta tables too.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this hbase install is major compacted.
* @throws ioexception e
*/
public static boolean ismajorcompacted final filesystem fs
final path hbaserootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table.
filestatus  tabledirs   fs liststatus hbaserootdir  new dirfilter fs
for  filestatus tabledir   tabledirs
// skip the .log directory.  all others should be tables.  inside a table,
// there are compaction.dir directories to skip.  otherwise, all else
// should be regions.  then in each region, should only be family
// directories.  under each of these, should be one file only.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
filestatus regiondirs   fs liststatus d  new dirfilter fs
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name.  now look in region for families.
filestatus familydirs   fs liststatus dd  new dirfilter fs
for  filestatus familydir   familydirs
path family   familydir getpath
// now in family make sure only one file.
filestatus familystatus   fs liststatus family
if  familystatus length > 1
log debug family tostring         familystatus length
return false
return true
// todo move this method out of fsutils. no dependencies to hmaster
/**
* returns the total overall fragmentation percentage. includes .meta. and
* -root- as well.
*
* @param master  the master defining the hbase root and file system.
* @return a map for each table and its percentage.
* @throws ioexception when scanning the directory fails.
*/
public static int gettotaltablefragmentation final hmaster master
throws ioexception
map<string  integer> map   gettablefragmentation master
return map    null    map size   > 0 ? map get       1
/**
* runs through the hbase rootdir and checks how many stores for each table
* have more than one file in them. checks -root- and .meta. too. the total
* percentage across all tables is stored under the special key "-total-".
*
* @param master  the master defining the hbase root and file system.
* @return a map for each table and its percentage.
*
* @throws ioexception when scanning the directory fails.
*/
public static map<string  integer> gettablefragmentation
final hmaster master
throws ioexception
path path   getrootdir master getconfiguration
// since hmaster.getfilesystem() is package private
filesystem fs   path getfilesystem master getconfiguration
return gettablefragmentation fs  path
/**
* runs through the hbase rootdir and checks how many stores for each table
* have more than one file in them. checks -root- and .meta. too. the total
* percentage across all tables is stored under the special key "-total-".
*
* @param fs  the file system to use.
* @param hbaserootdir  the root directory to scan.
* @return a map for each table and its percentage.
* @throws ioexception when scanning the directory fails.
*/
public static map<string  integer> gettablefragmentation
final filesystem fs  final path hbaserootdir
throws ioexception
map<string  integer> frags   new hashmap<string  integer>
int cfcounttotal   0
int cffragtotal   0
dirfilter df   new dirfilter fs
// presumes any directory under hbase.rootdir is a table
filestatus  tabledirs   fs liststatus hbaserootdir  df
for  filestatus tabledir   tabledirs
// skip the .log directory.  all others should be tables.  inside a table,
// there are compaction.dir directories to skip.  otherwise, all else
// should be regions.  then in each region, should only be family
// directories.  under each of these, should be one file only.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
int cfcount   0
int cffrag   0
filestatus regiondirs   fs liststatus d  df
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name, now look in region for families
filestatus familydirs   fs liststatus dd  df
for  filestatus familydir   familydirs
cfcount
cfcounttotal
path family   familydir getpath
// now in family make sure only one file
filestatus familystatus   fs liststatus family
if  familystatus length > 1
cffrag
cffragtotal
// compute percentage per table and store in result list
frags put d getname    math round  float  cffrag   cfcount   100
// set overall percentage for all tables
frags put    math round  float  cffragtotal   cfcounttotal   100
return frags
/**
* expects to find -root- directory.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this a pre020 layout.
* @throws ioexception e
*/
public static boolean ispre020filelayout final filesystem fs
final path hbaserootdir
throws ioexception
path mapfiles   new path new path new path new path hbaserootdir
return fs exists mapfiles
/**
* runs through the hbase rootdir and checks all stores have only
* one file in them -- that is, they've been major compacted.  looks
* at root and meta tables too.  this version differs from
* {@link #ismajorcompacted(filesystem, path)} in that it expects a
* pre-0.20.0 hbase layout on the filesystem.  used migrating.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this hbase install is major compacted.
* @throws ioexception e
*/
public static boolean ismajorcompactedpre020 final filesystem fs
final path hbaserootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table.
filestatus  tabledirs   fs liststatus hbaserootdir  new dirfilter fs
for  filestatus tabledir   tabledirs
// inside a table, there are compaction.dir directories to skip.
// otherwise, all else should be regions.  then in each region, should
// only be family directories.  under each of these, should be a mapfile
// and info directory and in these only one file.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
filestatus regiondirs   fs liststatus d  new dirfilter fs
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name.  now look in region for families.
filestatus familydirs   fs liststatus dd  new dirfilter fs
for  filestatus familydir   familydirs
path family   familydir getpath
filestatus infoandmapfile   fs liststatus family
// assert that only info and mapfile in family dir.
if  infoandmapfile length    0    infoandmapfile length    2
log debug family tostring
infoandmapfile length
return false
// make sure directory named info or mapfile.
for  int ll   0  ll < 2  ll
if  infoandmapfile getpath   getname   equals
infoandmapfile getpath   getname   equals
continue
log debug
infoandmapfile getpath
return false
// now in family, there are 'mapfile' and 'info' subdirs.  just
// look in the 'mapfile' subdir.
filestatus familystatus
fs liststatus new path family
if  familystatus length > 1
log debug family tostring         familystatus length
return false
return true
/**
* a {@link pathfilter} that returns only regular files.
*/
static class filefilter implements pathfilter
private final filesystem fs
public filefilter final filesystem fs
this fs   fs
@override
public boolean accept path p
try
return fs isfile p
catch  ioexception e
log debug     p      e
return false
/**
* a {@link pathfilter} that returns directories.
*/
public static class dirfilter implements pathfilter
private final filesystem fs
public dirfilter final filesystem fs
this fs   fs
@override
public boolean accept path p
boolean isvalid   false
try
if  hconstants hbase_non_user_table_dirs contains p tostring
isvalid   false
else
isvalid   fs getfilestatus p  isdir
catch  ioexception e
log warn     p tostring
e
return isvalid
/**
* heuristic to determine whether is safe or not to open a file for append
* looks both for dfs.support.append and use reflection to search
* for sequencefile.writer.syncfs() or fsdataoutputstream.hflush()
* @param conf
* @return true if append support
*/
public static boolean isappendsupported final configuration conf
boolean append   conf getboolean    false
if  append
try
// todo: the implementation that comes back when we do a createwriter
// may not be using sequencefile so the below is not a definitive test.
// will do for now (hdfs-200).
sequencefile writer class getmethod    new class<?>
append   true
catch  securityexception e
catch  nosuchmethodexception e
append   false
if   append
// look for the 0.21, 0.22, new-style append evidence.
try
fsdataoutputstream class getmethod    new class<?>
append   true
catch  nosuchmethodexception e
append   false
return append
/**
* @param conf
* @return true if this filesystem whose scheme is 'hdfs'.
* @throws ioexception
*/
public static boolean ishdfs final configuration conf  throws ioexception
filesystem fs   filesystem get conf
string scheme   fs geturi   getscheme
return scheme equalsignorecase
/**
* recover file lease. used when a file might be suspect
* to be had been left open by another process.
* @param fs filesystem handle
* @param p path of file to recover lease
* @param conf configuration handle
* @throws ioexception
*/
public abstract void recoverfilelease final filesystem fs  final path p
configuration conf  throws ioexception
/**
* @param fs
* @param rootdir
* @return all the table directories under <code>rootdir</code>. ignore non table hbase folders such as
* .logs, .oldlogs, .corrupt, .meta., and -root- folders.
* @throws ioexception
*/
public static list<path> gettabledirs final filesystem fs  final path rootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table
filestatus  dirs   fs liststatus rootdir  new dirfilter fs
list<path> tabledirs   new arraylist<path> dirs length
for  filestatus dir  dirs
path p   dir getpath
string tablename   p getname
if   hconstants hbase_non_user_table_dirs contains tablename
tabledirs add p
return tabledirs
public static path gettablepath path rootdir  byte  tablename
return gettablepath rootdir  bytes tostring tablename
public static path gettablepath path rootdir  final string tablename
return new path rootdir  tablename
/**
* filter for all dirs that don't start with '.'
*/
public static class regiondirfilter implements pathfilter
// this pattern will accept 0.90+ style hex region dirs and older numeric region dir names.
final public static pattern regiondirpattern   pattern compile
final filesystem fs
public regiondirfilter filesystem fs
this fs   fs
@override
public boolean accept path rd
if   regiondirpattern matcher rd getname    matches
return false
try
return fs getfilestatus rd  isdir
catch  ioexception ioe
// maybe the file was moved or the fs was disconnected.
log warn     rd     ioe
return false
/**
* given a particular table dir, return all the regiondirs inside it, excluding files such as
* .tableinfo
* @param fs a file system for the path
* @param tabledir path to a specific table directory <hbase.rootdir>/<tabledir>
* @return list of paths to valid region directories in table dir.
* @throws ioexception
*/
public static list<path> getregiondirs final filesystem fs  final path tabledir  throws ioexception
// assumes we are in a table dir.
filestatus rds   fs liststatus tabledir  new regiondirfilter fs
list<path> regiondirs   new arraylist<path> rds length
for  filestatus rdfs  rds
path rdpath   rdfs getpath
regiondirs add rdpath
return regiondirs
/**
* filter for all dirs that are legal column family names.  this is generally used for colfam
* dirs <hbase.rootdir>/<tabledir>/<regiondir>/<colfamdir>.
*/
public static class familydirfilter implements pathfilter
final filesystem fs
public familydirfilter filesystem fs
this fs   fs
@override
public boolean accept path rd
try
// throws iae if invalid
hcolumndescriptor islegalfamilyname bytes tobytes rd getname
catch  illegalargumentexception iae
// path name is an invalid family name and thus is excluded.
return false
try
return fs getfilestatus rd  isdir
catch  ioexception ioe
// maybe the file was moved or the fs was disconnected.
log warn     rd     ioe
return false
/**
* given a particular region dir, return all the familydirs inside it
*
* @param fs a file system for the path
* @param regiondir path to a specific region directory
* @return list of paths to valid family directories in region dir.
* @throws ioexception
*/
public static list<path> getfamilydirs final filesystem fs  final path regiondir  throws ioexception
// assumes we are in a region dir.
filestatus fds   fs liststatus regiondir  new familydirfilter fs
list<path> familydirs   new arraylist<path> fds length
for  filestatus fdfs  fds
path fdpath   fdfs getpath
familydirs add fdpath
return familydirs
/**
* filter for hfiles that excludes reference files.
*/
public static class hfilefilter implements pathfilter
// this pattern will accept 0.90+ style hex hfies files but reject reference files
final public static pattern hfilepattern   pattern compile
final filesystem fs
public hfilefilter filesystem fs
this fs   fs
@override
public boolean accept path rd
if   hfilepattern matcher rd getname    matches
return false
try
// only files
return  fs getfilestatus rd  isdir
catch  ioexception ioe
// maybe the file was moved or the fs was disconnected.
log warn     rd     ioe
return false
/**
* @param conf
* @return returns the filesystem of the hbase rootdir.
* @throws ioexception
*/
public static filesystem getcurrentfilesystem configuration conf
throws ioexception
return getrootdir conf  getfilesystem conf
/**
* runs through the hbase rootdir and creates a reverse lookup map for
* table storefile names to the full path.
* <br>
* example...<br>
* key = 3944417774205889744  <br>
* value = hdfs://localhost:51169/user/userid/-root-/70236052/info/3944417774205889744
*
* @param fs  the file system to use.
* @param hbaserootdir  the root directory to scan.
* @return map keyed by storefile name with a value of the full path.
* @throws ioexception when scanning the directory fails.
*/
public static map<string  path> gettablestorefilepathmap
final filesystem fs  final path hbaserootdir
throws ioexception
map<string  path> map   new hashmap<string  path>
// if this method looks similar to 'gettablefragmentation' that is because
// it was borrowed from it.
dirfilter df   new dirfilter fs
// presumes any directory under hbase.rootdir is a table
filestatus  tabledirs   fs liststatus hbaserootdir  df
for  filestatus tabledir   tabledirs
// skip the .log and other non-table directories.  all others should be tables.
// inside a table, there are compaction.dir directories to skip.  otherwise, all else
// should be regions.
path d   tabledir getpath
if  hconstants hbase_non_table_dirs contains d getname
continue
filestatus regiondirs   fs liststatus d  df
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name, now look in region for families
filestatus familydirs   fs liststatus dd  df
for  filestatus familydir   familydirs
path family   familydir getpath
// now in family, iterate over the storefiles and
// put in map
filestatus familystatus   fs liststatus family
for  filestatus sfstatus   familystatus
path sf   sfstatus getpath
map put  sf getname    sf
return map
/**
* calls fs.liststatus() and treats filenotfoundexception as non-fatal
* this accommodates differences between hadoop versions
*
* @param fs file system
* @param dir directory
* @param filter path filter
* @return null if tabledir doesn't exist, otherwise filestatus array
*/
public static filestatus  liststatus final filesystem fs
final path dir  final pathfilter filter  throws ioexception
filestatus  status   null
try
status   filter    null ? fs liststatus dir    fs liststatus dir  filter
catch  filenotfoundexception fnfe
// if directory doesn't exist, return null
log debug dir
if  status    null    status length < 1  return null
return status
/**
* calls fs.liststatus() and treats filenotfoundexception as non-fatal
* this would accommodates differences between hadoop versions
*
* @param fs file system
* @param dir directory
* @return null if tabledir doesn't exist, otherwise filestatus array
*/
public static filestatus liststatus final filesystem fs  final path dir  throws ioexception
return liststatus fs  dir  null
/**
* calls fs.delete() and returns the value returned by the fs.delete()
*
* @param fs
* @param path
* @param recursive
* @return the value returned by the fs.delete()
* @throws ioexception
*/
public static boolean delete final filesystem fs  final path path  final boolean recursive
throws ioexception
return fs delete path  recursive
/**
* calls fs.exists(). checks if the specified path exists
*
* @param fs
* @param path
* @return the value returned by fs.exists()
* @throws ioexception
*/
public static boolean isexists final filesystem fs  final path path  throws ioexception
return fs exists path
/**
* throw an exception if an action is not permitted by a user on a file.
*
* @param ugi
*          the user
* @param file
*          the file
* @param action
*          the action
*/
public static void checkaccess usergroupinformation ugi  filestatus file
fsaction action  throws accesscontrolexception
if  ugi getshortusername   equals file getowner
if  file getpermission   getuseraction   implies action
return
else if  contains ugi getgroupnames    file getgroup
if  file getpermission   getgroupaction   implies action
return
else if  file getpermission   getotheraction   implies action
return
throw new accesscontrolexception         action
file getpath         ugi getshortusername
private static boolean contains string groups  string user
for  string group   groups
if  group equals user
return true
return false
/**
* log the current state of the filesystem from a certain root directory
* @param fs filesystem to investigate
* @param root root file/directory to start logging from
* @param log log to output information
* @throws ioexception if an unexpected exception occurs
*/
public static void logfilesystemstate final filesystem fs  final path root  log log
throws ioexception
log debug
logfstree log  fs  root
/**
* recursive helper to log the state of the fs
*
* @see #logfilesystemstate(filesystem, path, log)
*/
private static void logfstree log log  final filesystem fs  final path root  string prefix
throws ioexception
filestatus files   fsutils liststatus fs  root  null
if  files    null  return
for  filestatus file   files
if  file isdir
log debug prefix   file getpath   getname
logfstree log  fs  file getpath    prefix
else
log debug prefix   file getpath   getname