/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql
import java io datainput
import java io fileinputstream
import java io fileoutputstream
import java io ioexception
import java io serializable
import java util arraylist
import java util hashmap
import java util hashset
import java util iterator
import java util linkedhashmap
import java util linkedlist
import java util list
import java util map
import java util queue
import java util set
import java util concurrent concurrentlinkedqueue
import org apache commons lang stringutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs fsdatainputstream
import org apache hadoop fs path
import org apache hadoop hive common javautils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore metastoreutils
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api schema
import org apache hadoop hive ql exec conditionaltask
import org apache hadoop hive ql exec execdriver
import org apache hadoop hive ql exec fetchtask
import org apache hadoop hive ql exec operator
import org apache hadoop hive ql exec tablescanoperator
import org apache hadoop hive ql exec task
import org apache hadoop hive ql exec taskfactory
import org apache hadoop hive ql exec taskresult
import org apache hadoop hive ql exec taskrunner
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql history hivehistory keys
import org apache hadoop hive ql hooks executewithhookcontext
import org apache hadoop hive ql hooks hook
import org apache hadoop hive ql hooks hookcontext
import org apache hadoop hive ql hooks postexecute
import org apache hadoop hive ql hooks preexecute
import org apache hadoop hive ql hooks readentity
import org apache hadoop hive ql hooks writeentity
import org apache hadoop hive ql lockmgr hivelock
import org apache hadoop hive ql lockmgr hivelockmanager
import org apache hadoop hive ql lockmgr hivelockmanagerctx
import org apache hadoop hive ql lockmgr hivelockmode
import org apache hadoop hive ql lockmgr hivelockobj
import org apache hadoop hive ql lockmgr hivelockobject
import org apache hadoop hive ql lockmgr hivelockobject hivelockobjectdata
import org apache hadoop hive ql lockmgr lockexception
import org apache hadoop hive ql log perflogger
import org apache hadoop hive ql metadata authorizationexception
import org apache hadoop hive ql metadata dummypartition
import org apache hadoop hive ql metadata hive
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql optimizer ppr partitionpruner
import org apache hadoop hive ql parse astnode
import org apache hadoop hive ql parse abstractsemanticanalyzerhook
import org apache hadoop hive ql parse basesemanticanalyzer
import org apache hadoop hive ql parse hivesemanticanalyzerhookcontext
import org apache hadoop hive ql parse hivesemanticanalyzerhookcontextimpl
import org apache hadoop hive ql parse importsemanticanalyzer
import org apache hadoop hive ql parse parsecontext
import org apache hadoop hive ql parse parsedriver
import org apache hadoop hive ql parse parseutils
import org apache hadoop hive ql parse prunedpartitionlist
import org apache hadoop hive ql parse semanticanalyzer
import org apache hadoop hive ql parse semanticanalyzerfactory
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql parse variablesubstitution
import org apache hadoop hive ql plan hiveoperation
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql processors commandprocessor
import org apache hadoop hive ql processors commandprocessorresponse
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive ql session sessionstate loghelper
import org apache hadoop hive serde2 bytestream
import org apache hadoop hive shims shimloader
import org apache hadoop mapred clusterstatus
import org apache hadoop mapred jobclient
import org apache hadoop mapred jobconf
import org apache hadoop util reflectionutils
public class driver implements commandprocessor
static final private log log   logfactory getlog driver class getname
static final private loghelper console   new loghelper log
private int maxrows   100
bytestream output bos   new bytestream output
private hiveconf conf
private datainput resstream
private context ctx
private queryplan plan
private schema schema
private hivelockmanager hivelockmgr
private string errormessage
private string sqlstate
// a limit on the number of threads that can be launched
private int maxthreads
private static final int sleep_time   2000
protected int trycount   integer max_value
private boolean checklockmanager
boolean supportconcurrency   conf getboolvar hiveconf confvars hive_support_concurrency
if   supportconcurrency
return false
if   hivelockmgr    null
try
setlockmanager
catch  semanticexception e
errormessage       e getmessage
sqlstate   errormsg findsqlstate e getmessage
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return false
// the reason that we set the lock manager for the cxt here is because each
// query has its own ctx object. the hivelockmgr is shared accross the
// same instance of driver, which can run multiple queries.
ctx sethivelockmgr hivelockmgr
return hivelockmgr    null
private void setlockmanager   throws semanticexception
boolean supportconcurrency   conf getboolvar hiveconf confvars hive_support_concurrency
if  supportconcurrency
string lockmgr   conf getvar hiveconf confvars hive_lock_manager
if   lockmgr    null      lockmgr isempty
throw new semanticexception errormsg lockmgr_not_specified getmsg
try
hivelockmgr    hivelockmanager  reflectionutils newinstance conf getclassbyname lockmgr
conf
hivelockmgr setcontext new hivelockmanagerctx conf
catch  exception e
// set hivelockmgr to null just in case this invalid manager got set to
// next query's ctx.
if  hivelockmgr    null
try
hivelockmgr close
catch  lockexception e1
//nothing can do here
hivelockmgr   null
throw new semanticexception errormsg lockmgr_not_initialized getmsg     e getmessage
public void init
operator resetid
/**
* return the status information about the map-reduce cluster
*/
public clusterstatus getclusterstatus   throws exception
clusterstatus cs
try
jobconf job   new jobconf conf  execdriver class
jobclient jc   new jobclient job
cs   jc getclusterstatus
catch  exception e
e printstacktrace
throw e
log info     cs tostring
return cs
public schema getschema
return schema
/**
* get a schema with fields represented with native hive types
*/
public static schema getschema basesemanticanalyzer sem  hiveconf conf
schema schema   null
// if we have a plan, prefer its logical result schema if it's
// available; otherwise, try digging out a fetch task; failing that,
// give up.
if  sem    null
// can't get any info without a plan
else if  sem getresultschema      null
list<fieldschema> lst   sem getresultschema
schema   new schema lst  null
else if  sem getfetchtask      null
fetchtask ft   sem getfetchtask
tabledesc td   ft gettbldesc
// partitioned tables don't have tabledesc set on the fetchtask. instead
// they have a list of partitiondesc objects, each with a table desc.
// let's
// try to fetch the desc for the first partition and use it's
// deserializer.
if  td    null    ft getwork      null    ft getwork   getpartdesc      null
if  ft getwork   getpartdesc   size   > 0
td   ft getwork   getpartdesc   get 0  gettabledesc
if  td    null
log info
else
string tablename
list<fieldschema> lst   null
try
lst   metastoreutils getfieldsfromdeserializer tablename  td getdeserializer
catch  exception e
log warn
org apache hadoop util stringutils stringifyexception e
if  lst    null
schema   new schema lst  null
if  schema    null
schema   new schema
log info     schema
return schema
/**
* get a schema with fields represented with thrift ddl types
*/
public schema getthriftschema   throws exception
schema schema
try
schema   getschema
if  schema    null
list<fieldschema> lst   schema getfieldschemas
// go over the schema and convert type to thrift type
if  lst    null
for  fieldschema f   lst
f settype metastoreutils typetothrifttype f gettype
catch  exception e
e printstacktrace
throw e
log info     schema
return schema
/**
* return the maximum number of rows returned by getresults
*/
public int getmaxrows
return maxrows
/**
* set the maximum number of rows returned by getresults
*/
public void setmaxrows int maxrows
this maxrows   maxrows
public boolean hasreducetasks list<task<? extends serializable>> tasks
if  tasks    null
return false
boolean hasreduce   false
for  task<? extends serializable> task   tasks
if  task hasreduce
return true
hasreduce    hasreduce    hasreducetasks task getchildtasks
return hasreduce
/**
* for backwards compatibility with current tests
*/
public driver hiveconf conf
this conf   conf
public driver
if  sessionstate get      null
conf   sessionstate get   getconf
/**
* compile a new query. any currently-planned query associated with this driver is discarded.
*
* @param command
*          the sql query to compile.
*/
public int compile string command
return compile command  true
/**
* hold state variables specific to each query being executed, that may not
* be consistent in the overall sessionstate
*/
private static class querystate
private hiveoperation op
private string cmd
private boolean init   false
/**
* initialize the querystate with the query state variables
*/
public void init hiveoperation op  string cmd
this op   op
this cmd   cmd
this init   true
public boolean isinitialized
return this init
public hiveoperation getop
return this op
public string getcmd
return this cmd
public void savesession querystate qs
sessionstate oldss   sessionstate get
if  oldss    null    oldss gethiveoperation      null
qs init oldss gethiveoperation    oldss getcmd
public void restoresession querystate qs
sessionstate ss   sessionstate get
if  ss    null    qs    null    qs isinitialized
ss setcmd qs getcmd
ss setcommandtype qs getop
/**
* compile a new query, but potentially reset taskid counter.  not resetting task counter
* is useful for generating re-entrant ql queries.
* @param command  the hiveql query to compile
* @param resettaskids resets taskid counter if true.
* @return 0 for ok
*/
public int compile string command  boolean resettaskids
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger compile
//holder for parent command type/string when executing reentrant queries
querystate querystate   new querystate
if  plan    null
close
plan   null
if  resettaskids
taskfactory resetid
savesession querystate
try
command   new variablesubstitution   substitute conf command
ctx   new context conf
ctx settrycount gettrycount
ctx setcmd command
ctx sethdfscleanup true
parsedriver pd   new parsedriver
astnode tree   pd parse command  ctx
tree   parseutils findrootnonnulltoken tree
basesemanticanalyzer sem   semanticanalyzerfactory get conf  tree
list<abstractsemanticanalyzerhook> sahooks
gethooks hiveconf confvars semantic_analyzer_hook
abstractsemanticanalyzerhook class
// do semantic analysis and plan generation
if  sahooks    null
hivesemanticanalyzerhookcontext hookctx   new hivesemanticanalyzerhookcontextimpl
hookctx setconf conf
for  abstractsemanticanalyzerhook hook   sahooks
tree   hook preanalyze hookctx  tree
sem analyze tree  ctx
hookctx update sem
for  abstractsemanticanalyzerhook hook   sahooks
hook postanalyze hookctx  sem getroottasks
else
sem analyze tree  ctx
log info
// validate the plan
sem validate
plan   new queryplan command  sem  perflogger getstarttime perflogger driver_run
// test only - serialize the query plan and deserialize it
if    equalsignorecase system getproperty
string queryplanfilename   ctx getlocalscratchdir true    path separator_char
log info     queryplanfilename
queryplanfilename   new path queryplanfilename  touri   getpath
// serialize the queryplan
fileoutputstream fos   new fileoutputstream queryplanfilename
utilities serializequeryplan plan  fos
fos close
// deserialize the queryplan
fileinputstream fis   new fileinputstream queryplanfilename
queryplan newplan   utilities deserializequeryplan fis  conf
fis close
// use the deserialized plan
plan   newplan
// initialize fetchtask right here
if  plan getfetchtask      null
plan getfetchtask   initialize conf  plan  null
// get the output schema
schema   getschema sem  conf
//do the authorization check
if  hiveconf getboolvar conf
hiveconf confvars hive_authorization_enabled
try
perflogger perflogbegin log  perflogger do_authorization
doauthorization sem
catch  authorizationexception authexp
console printerror     authexp getmessage
return 403
finally
perflogger perflogend log  perflogger do_authorization
//restore state after we're done executing a specific query
return 0
catch  exception e
errormsg error   errormsg geterrormsg e getmessage
errormessage       e getclass   getsimplename
if  error    errormsg generic_error
errormessage         error geterrorcode
errormessage        e getmessage
sqlstate   error getsqlstate
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return error geterrorcode
finally
perflogger perflogend log  perflogger compile
restoresession querystate
private void doauthorization basesemanticanalyzer sem
throws hiveexception  authorizationexception
hashset<readentity> inputs   sem getinputs
hashset<writeentity> outputs   sem getoutputs
sessionstate ss   sessionstate get
hiveoperation op   ss gethiveoperation
hive db   sem getdb
if  op    null
if  op equals hiveoperation createtable_as_select
op equals hiveoperation createtable
ss getauthorizer   authorize
db getdatabase db getcurrentdatabase     null
hiveoperation createtable_as_select getoutputrequiredprivileges
else
if  op equals hiveoperation import
importsemanticanalyzer isa    importsemanticanalyzer  sem
if   isa existstable
ss getauthorizer   authorize
db getdatabase db getcurrentdatabase     null
hiveoperation createtable_as_select getoutputrequiredprivileges
if  outputs    null    outputs size   > 0
for  writeentity write   outputs
if  write gettype      writeentity type partition
partition part   db getpartition write gettable    write
getpartition   getspec    false
if  part    null
ss getauthorizer   authorize write getpartition    null
op getoutputrequiredprivileges
continue
if  write gettable      null
ss getauthorizer   authorize write gettable    null
op getoutputrequiredprivileges
if  inputs    null    inputs size   > 0
map<table  list<string>> tab2cols   new hashmap<table  list<string>>
map<partition  list<string>> part2cols   new hashmap<partition  list<string>>
map<string  boolean> tableusepartlevelauth   new hashmap<string  boolean>
for  readentity read   inputs
if  read getpartition      null
table tbl   read gettable
string tblname   tbl gettablename
if  tableusepartlevelauth get tblname     null
boolean usepartlevelpriv    tbl getparameters   get

equalsignorecase tbl getparameters   get
if  usepartlevelpriv
tableusepartlevelauth put tblname  boolean true
else
tableusepartlevelauth put tblname  boolean false
if  op equals hiveoperation createtable_as_select
op equals hiveoperation query
semanticanalyzer querysem    semanticanalyzer  sem
parsecontext parsectx   querysem getparsecontext
map<tablescanoperator  table> tsotopmap   parsectx gettoptotable
for  map entry<string  operator<? extends operatordesc>> topopmap   querysem
getparsecontext   gettopops   entryset
operator<? extends operatordesc> topop   topopmap getvalue
if  topop instanceof tablescanoperator
tsotopmap containskey topop
tablescanoperator tablescanop    tablescanoperator  topop
table tbl   tsotopmap get tablescanop
list<integer> neededcolumnids   tablescanop getneededcolumnids
list<fieldschema> columns   tbl getcols
list<string> cols   new arraylist<string>
if  neededcolumnids    null    neededcolumnids size   > 0
for  int i   0  i < neededcolumnids size    i
cols add columns get neededcolumnids get i   getname
else
for  int i   0  i < columns size    i
cols add columns get i  getname
//map may not contain all sources, since input list may have been optimized out
//or non-existent tho such sources may still be referenced by the tablescanoperator
//if it's null then the partition probably doesn't exist so let's use table permission
if  tbl ispartitioned
tableusepartlevelauth get tbl gettablename       boolean true
string alias_id   topopmap getkey
prunedpartitionlist partslist   partitionpruner prune parsectx
gettoptotable   get topop   parsectx getoptopartpruner
get topop   parsectx getconf    alias_id  parsectx
getprunedpartitions
set<partition> parts   new hashset<partition>
parts addall partslist getconfirmedpartns
parts addall partslist getunknownpartns
for  partition part   parts
list<string> existingcols   part2cols get part
if  existingcols    null
existingcols   new arraylist<string>
existingcols addall cols
part2cols put part  existingcols
else
list<string> existingcols   tab2cols get tbl
if  existingcols    null
existingcols   new arraylist<string>
existingcols addall cols
tab2cols put tbl  existingcols
//cache the results for table authorization
set<string> tableauthchecked   new hashset<string>
for  readentity read   inputs
table tbl   null
if  read getpartition      null
tbl   read getpartition   gettable
// use partition level authorization
if  tableusepartlevelauth get tbl gettablename       boolean true
list<string> cols   part2cols get read getpartition
if  cols    null    cols size   > 0
ss getauthorizer   authorize read getpartition   gettable
read getpartition    cols  op getinputrequiredprivileges

else
ss getauthorizer   authorize read getpartition
op getinputrequiredprivileges    null
continue
else if  read gettable      null
tbl   read gettable
// if we reach here, it means it needs to do a table authorization
// check, and the table authorization may already happened because of other
// partitions
if  tbl    null     tableauthchecked contains tbl gettablename
list<string> cols   tab2cols get tbl
if  cols    null    cols size   > 0
ss getauthorizer   authorize tbl  null  cols
op getinputrequiredprivileges    null
else
ss getauthorizer   authorize tbl  op getinputrequiredprivileges

tableauthchecked add tbl gettablename
/**
* @return the current query plan associated with this driver, if any.
*/
public queryplan getplan
return plan
/**
* @param t
*          the table to be locked
* @param p
*          the partition to be locked
* @param mode
*          the mode of the lock (shared/exclusive) get the list of objects to be locked. if a
*          partition needs to be locked (in any mode), all its parents should also be locked in
*          shared mode.
**/
private list<hivelockobj> getlockobjects table t  partition p  hivelockmode mode
throws semanticexception
list<hivelockobj> locks   new linkedlist<hivelockobj>
hivelockobjectdata lockdata
new hivelockobjectdata plan getqueryid
string valueof system currenttimemillis
plan getquerystr
if  t    null
locks add new hivelockobj new hivelockobject t  lockdata   mode
mode   hivelockmode shared
locks add new hivelockobj new hivelockobject t getdbname    lockdata   mode
return locks
if  p    null
if    p instanceof dummypartition
locks add new hivelockobj new hivelockobject p  lockdata   mode
// all the parents are locked in shared mode
mode   hivelockmode shared
// for dummy partitions, only partition name is needed
string name   p getname
if  p instanceof dummypartition
name   p getname   split
string partialname
string partns   name split
int len   p instanceof dummypartition ? partns length   partns length   1
map<string  string> partialspec   new linkedhashmap<string  string>
for  int idx   0  idx < len  idx
string partn   partns
partialname    partn
string namevalue   partn split
assert namevalue length    2
partialspec put namevalue  namevalue
try
locks add new hivelockobj
new hivelockobject new dummypartition p gettable    p gettable   getdbname
p gettable   gettablename
partialname
partialspec   lockdata   mode
partialname
catch  hiveexception e
throw new semanticexception e getmessage
locks add new hivelockobj new hivelockobject p gettable    lockdata   mode
locks add new hivelockobj new hivelockobject p gettable   getdbname    lockdata   mode
return locks
/**
* acquire read and write locks needed by the statement. the list of objects to be locked are
* obtained from he inputs and outputs populated by the compiler. the lock acuisition scheme is
* pretty simple. if all the locks cannot be obtained, error out. deadlock is avoided by making
* sure that the locks are lexicographically sorted.
**/
public int acquirereadwritelocks
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger acquire_read_write_locks
try
boolean supportconcurrency   conf getboolvar hiveconf confvars hive_support_concurrency
if   supportconcurrency
return 0
list<hivelockobj> lockobjects   new arraylist<hivelockobj>
// sort all the inputs, outputs.
// if a lock needs to be acquired on any partition, a read lock needs to be acquired on all
// its parents also
for  readentity input   plan getinputs
if  input gettype      readentity type table
lockobjects addall getlockobjects input gettable    null  hivelockmode shared
else
lockobjects addall getlockobjects null  input getpartition    hivelockmode shared
for  writeentity output   plan getoutputs
if  output gettyp      writeentity type table
lockobjects addall getlockobjects output gettable    null
output iscomplete   ? hivelockmode exclusive   hivelockmode shared
else if  output gettyp      writeentity type partition
lockobjects addall getlockobjects null  output getpartition    hivelockmode exclusive
// in case of dynamic queries, it is possible to have incomplete dummy partitions
else if  output gettyp      writeentity type dummypartition
lockobjects addall getlockobjects null  output getpartition    hivelockmode shared
if  lockobjects isempty       ctx isneedlockmgr
return 0
hivelockobjectdata lockdata
new hivelockobjectdata plan getqueryid
string valueof system currenttimemillis
plan getquerystr
// lock the database also
try
hive db   hive get conf
lockobjects add new hivelockobj
new hivelockobject db getcurrentdatabase    lockdata
hivelockmode shared
catch  hiveexception e
throw new semanticexception e getmessage
list<hivelock> hivelocks   ctx gethivelockmgr   lock lockobjects  false
if  hivelocks    null
throw new semanticexception errormsg lock_cannot_be_acquired getmsg
else
ctx sethivelocks hivelocks
return  0
catch  semanticexception e
errormessage       e getmessage
sqlstate   errormsg findsqlstate e getmessage
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return  10
catch  lockexception e
errormessage       e getmessage
sqlstate   errormsg findsqlstate e getmessage
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return  10
finally
perflogger perflogend log  perflogger acquire_read_write_locks
/**
* @param hivelocks
*          list of hive locks to be released release all the locks specified. if some of the
*          locks have already been released, ignore them
**/
private void releaselocks list<hivelock> hivelocks
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger release_locks
if  hivelocks    null
ctx gethivelockmgr   releaselocks hivelocks
ctx sethivelocks null
perflogger perflogend log  perflogger release_locks
public commandprocessorresponse run string command  throws commandneedretryexception
errormessage   null
sqlstate   null
if   validateconfvariables
return new commandprocessorresponse 12  errormessage  sqlstate
hivedriverrunhookcontext hookcontext   new hivedriverrunhookcontextimpl conf  command
// get all the driver run hooks and pre-execute them.
list<hivedriverrunhook> driverrunhooks
try
driverrunhooks   gethooks hiveconf confvars hive_driver_run_hooks  hivedriverrunhook class
for  hivedriverrunhook driverrunhook   driverrunhooks
driverrunhook predriverrun hookcontext
catch  exception e
errormessage       utilities getnamemessage e
sqlstate   errormsg findsqlstate e getmessage
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return new commandprocessorresponse 12  errormessage  sqlstate
// reset the perf logger
perflogger perflogger   perflogger getperflogger true
perflogger perflogbegin log  perflogger driver_run
perflogger perflogbegin log  perflogger time_to_submit
int ret   compile command
if  ret    0
releaselocks ctx gethivelocks
return new commandprocessorresponse ret  errormessage  sqlstate
boolean requirelock   false
boolean cklock   checklockmanager
if  cklock
boolean lockonlymapred   hiveconf getboolvar conf  hiveconf confvars hive_lock_mapred_only
if lockonlymapred
queue<task<? extends serializable>> taskqueue   new linkedlist<task<? extends serializable>>
taskqueue addall plan getroottasks
while  taskqueue peek      null
task<? extends serializable> tsk   taskqueue remove
requirelock   requirelock    tsk requirelock
if requirelock
break
if  tsk instanceof conditionaltask
taskqueue addall   conditionaltask tsk  getlisttasks
if tsk getchildtasks     null
taskqueue addall tsk getchildtasks
// does not add back up task here, because back up task should be the same
// type of the original task.
else
requirelock   true
if  requirelock
ret   acquirereadwritelocks
if  ret    0
releaselocks ctx gethivelocks
return new commandprocessorresponse ret  errormessage  sqlstate
ret   execute
if  ret    0
//if needrequirelock is false, the release here will do nothing because there is no lock
releaselocks ctx gethivelocks
return new commandprocessorresponse ret  errormessage  sqlstate
//if needrequirelock is false, the release here will do nothing because there is no lock
releaselocks ctx gethivelocks
perflogger perflogend log  perflogger driver_run
perflogger close log  plan
// take all the driver run hooks and post-execute them.
try
for  hivedriverrunhook driverrunhook   driverrunhooks
driverrunhook postdriverrun hookcontext
catch  exception e
errormessage       utilities getnamemessage e
sqlstate   errormsg findsqlstate e getmessage
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return new commandprocessorresponse 12  errormessage  sqlstate
return new commandprocessorresponse ret
/**
* validate configuration variables.
*
* @return
*/
private boolean validateconfvariables
boolean valid   true
if    conf getboolvar hiveconf confvars hive_hadoop_supports_subdirectories
conf getboolvar hiveconf confvars hive_internal_ddl_list_bucketing_enable
conf getboolvar hiveconf confvars hadoopmapredinputdirrecursive       conf
getboolvar hiveconf confvars hiveoptlistbucketing
errormessage
errormsg support_dir_must_true_for_list_bucketing getmsg
sqlstate   errormsg findsqlstate errormessage
console printerror errormessage
valid   false
return valid
/**
* returns a set of hooks specified in a configuration variable.
*
* see gethooks(hiveconf.confvars hookconfvar, class<t> clazz)
* @param hookconfvar
* @return
* @throws exception
*/
private list<hook> gethooks hiveconf confvars hookconfvar  throws exception
return gethooks hookconfvar  hook class
/**
* returns the hooks specified in a configuration variable.  the hooks are returned in a list in
* the order they were specified in the configuration variable.
*
* @param hookconfvar the configuration variable specifying a comma separated list of the hook
*                    class names.
* @param clazz       the super type of the hooks.
* @return            a list of the hooks cast as the type specified in clazz, in the order
*                    they are listed in the value of hookconfvar
* @throws exception
*/
private <t extends hook> list<t> gethooks hiveconf confvars hookconfvar  class<t> clazz
throws exception
list<t> hooks   new arraylist<t>
string cshooks   conf getvar hookconfvar
if  cshooks    null
return hooks
cshooks   cshooks trim
if  cshooks equals
return hooks
string hookclasses   cshooks split
for  string hookclass   hookclasses
try
t hook
t  class forname hookclass trim    true  javautils getclassloader    newinstance
hooks add hook
catch  classnotfoundexception e
console printerror hookconfvar varname       e getmessage
throw e
return hooks
public int execute   throws commandneedretryexception
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger driver_execute
boolean noname   stringutils isempty conf getvar hiveconf confvars hadoopjobname
int maxlen   conf getintvar hiveconf confvars hivejobnamelength
string queryid   plan getqueryid
string querystr   plan getquerystr
conf setvar hiveconf confvars hivequeryid  queryid
conf setvar hiveconf confvars hivequerystring  querystr
maxthreads   hiveconf getintvar conf  hiveconf confvars execparallethreadnumber
try
log info     querystr
plan setstarted
if  sessionstate get      null
sessionstate get   gethivehistory   startquery querystr
conf getvar hiveconf confvars hivequeryid
sessionstate get   gethivehistory   logplanprogress plan
resstream   null
hookcontext hookcontext   new hookcontext plan  conf  ctx getpathtocs
hookcontext sethooktype hookcontext hooktype pre_exec_hook
for  hook peh   gethooks hiveconf confvars preexechooks
if  peh instanceof executewithhookcontext
perflogger perflogbegin log  perflogger pre_hook   peh getclass   getname
executewithhookcontext  peh  run hookcontext
perflogger perflogend log  perflogger pre_hook   peh getclass   getname
else if  peh instanceof preexecute
perflogger perflogbegin log  perflogger pre_hook   peh getclass   getname
preexecute  peh  run sessionstate get    plan getinputs    plan getoutputs
shimloader gethadoopshims   getugiforconf conf
perflogger perflogend log  perflogger pre_hook   peh getclass   getname
int jobs   utilities getmrtasks plan getroottasks    size
if  jobs > 0
console printinfo     jobs
if  sessionstate get      null
sessionstate get   gethivehistory   setqueryproperty queryid  keys query_num_tasks
string valueof jobs
sessionstate get   gethivehistory   setidtotablemap plan getidtotablenamemap
string jobname   utilities abbreviate querystr  maxlen   6
// a runtime that launches runnable tasks as separate threads through
// taskrunners
// as soon as a task isrunnable, it is put in a queue
// at any time, at most maxthreads tasks can be running
// the main thread polls the taskrunners to check if they have finished.
queue<task<? extends serializable>> runnable   new concurrentlinkedqueue<task<? extends serializable>>
map<taskresult  taskrunner> running   new hashmap<taskresult  taskrunner>
drivercontext drivercxt   new drivercontext runnable  ctx
ctx sethdfscleanup true
sessionstate get   setlastmapredstatslist new arraylist<mapredstats>
sessionstate get   setstacktraces new hashmap<string  list<list<string>>>
sessionstate get   setlocalmaprederrors new hashmap<string  list<string>>
// add root tasks to runnable
for  task<? extends serializable> tsk   plan getroottasks
drivercxt addtorunnable tsk
perflogger perflogend log  perflogger time_to_submit
// loop while you either have tasks running, or tasks queued up
while  running size      0    runnable peek      null
// launch upto maxthreads tasks
while  runnable peek      null    running size   < maxthreads
task<? extends serializable> tsk   runnable remove
launchtask tsk  queryid  noname  running  jobname  jobs  drivercxt
// poll the tasks to see which one completed
taskresult tskres   polltasks running keyset
taskrunner tskrun   running remove tskres
task<? extends serializable> tsk   tskrun gettask
hookcontext addcompletetask tskrun
int exitval   tskres getexitval
if  exitval    0
if  tsk ifretrycmdwhenfail
if  running size      0
taskcleanup
// in case we decided to run everything in local mode, restore the
// the jobtracker setting to its initial value
ctx restoreoriginaltracker
throw new commandneedretryexception
task<? extends serializable> backuptask   tsk getandinitbackuptask
if  backuptask    null
errormessage       exitval
tsk getclass   getname
errormsg em   errormsg geterrormsg exitval
if  em    null
errormessage         em getmsg
console printerror errormessage
errormessage       backuptask getclass   getname
console printerror errormessage
// add backup task to runnable
if  drivercontext islaunchable backuptask
drivercxt addtorunnable backuptask
continue
else
hookcontext sethooktype hookcontext hooktype on_failure_hook
// get all the failure execution hooks and execute them.
for  hook ofh   gethooks hiveconf confvars onfailurehooks
perflogger perflogbegin log  perflogger failure_hook   ofh getclass   getname
executewithhookcontext  ofh  run hookcontext
perflogger perflogend log  perflogger failure_hook   ofh getclass   getname
errormessage       exitval
tsk getclass   getname
errormsg em   errormsg geterrormsg exitval
if  em    null
errormessage         em getmsg
sqlstate
console printerror errormessage
if  running size      0
taskcleanup
// in case we decided to run everything in local mode, restore the
// the jobtracker setting to its initial value
ctx restoreoriginaltracker
return exitval
if  sessionstate get      null
sessionstate get   gethivehistory   settaskproperty queryid  tsk getid
keys task_ret_code  string valueof exitval
sessionstate get   gethivehistory   endtask queryid  tsk
if  tsk getchildtasks      null
for  task<? extends serializable> child   tsk getchildtasks
if  drivercontext islaunchable child
drivercxt addtorunnable child
// in case we decided to run everything in local mode, restore the
// the jobtracker setting to its initial value
ctx restoreoriginaltracker
// remove incomplete outputs.
// some incomplete outputs may be added at the beginning, for eg: for dynamic partitions.
// remove them
hashset<writeentity> remoutputs   new hashset<writeentity>
for  writeentity output   plan getoutputs
if   output iscomplete
remoutputs add output
for  writeentity output   remoutputs
plan getoutputs   remove output
hookcontext sethooktype hookcontext hooktype post_exec_hook
// get all the post execution hooks and execute them.
for  hook peh   gethooks hiveconf confvars postexechooks
if  peh instanceof executewithhookcontext
perflogger perflogbegin log  perflogger post_hook   peh getclass   getname
executewithhookcontext  peh  run hookcontext
perflogger perflogend log  perflogger post_hook   peh getclass   getname
else if  peh instanceof postexecute
perflogger perflogbegin log  perflogger post_hook   peh getclass   getname
postexecute  peh  run sessionstate get    plan getinputs    plan getoutputs
sessionstate get      null ? sessionstate get   getlineagestate   getlineageinfo
null   shimloader gethadoopshims   getugiforconf conf
perflogger perflogend log  perflogger post_hook   peh getclass   getname
if  sessionstate get      null
sessionstate get   gethivehistory   setqueryproperty queryid  keys query_ret_code
string valueof 0
sessionstate get   gethivehistory   printrowcount queryid
catch  commandneedretryexception e
throw e
catch  exception e
ctx restoreoriginaltracker
if  sessionstate get      null
sessionstate get   gethivehistory   setqueryproperty queryid  keys query_ret_code
string valueof 12
// todo: do better with handling types of exception here
errormessage       utilities getnamemessage e
sqlstate
console printerror errormessage
org apache hadoop util stringutils stringifyexception e
return  12
finally
if  sessionstate get      null
sessionstate get   gethivehistory   endquery queryid
if  noname
conf setvar hiveconf confvars hadoopjobname
perflogger perflogend log  perflogger driver_execute
if  sessionstate get   getlastmapredstatslist      null
sessionstate get   getlastmapredstatslist   size   > 0
long totalcpu   0
console printinfo
for  int i   0  i < sessionstate get   getlastmapredstatslist   size    i
console printinfo     i       sessionstate get   getlastmapredstatslist   get i
totalcpu    sessionstate get   getlastmapredstatslist   get i  getcpumsec
console printinfo     utilities formatmsectostr totalcpu
plan setdone
if  sessionstate get      null
try
sessionstate get   gethivehistory   logplanprogress plan
catch  exception e
console printinfo
return  0
/**
* launches a new task
*
* @param tsk
*          task being launched
* @param queryid
*          id of the query containing the task
* @param noname
*          whether the task has a name set
* @param running
*          map from taskresults to taskrunners
* @param jobname
*          name of the task, if it is a map-reduce job
* @param jobs
*          number of map-reduce jobs
* @param cxt
*          the driver context
*/
public void launchtask task<? extends serializable> tsk  string queryid  boolean noname
map<taskresult  taskrunner> running  string jobname  int jobs  drivercontext cxt
if  sessionstate get      null
sessionstate get   gethivehistory   starttask queryid  tsk  tsk getclass   getname
if  tsk ismapredtask        tsk instanceof conditionaltask
if  noname
conf setvar hiveconf confvars hadoopjobname  jobname       tsk getid
cxt inccurjobno 1
console printinfo     cxt getcurjobno         jobs
tsk initialize conf  plan  cxt
taskresult tskres   new taskresult
taskrunner tskrun   new taskrunner tsk  tskres
// launch task
if  hiveconf getboolvar conf  hiveconf confvars execparallel     tsk ismapredtask
// launch it in the parallel mode, as a separate thread only for mr tasks
tskrun start
else
tskrun runsequential
running put tskres  tskrun
return
/**
* cleans up remaining tasks in case of failure
*/
public void taskcleanup
// the currently existing shutdown hooks will be automatically called,
// killing the map-reduce processes.
// the non mr processes will be killed as well.
system exit 9
/**
* polls running tasks to see if a task has ended.
*
* @param results
*          set of result objects for running tasks
* @return the result object for any completed/failed task
*/
public taskresult polltasks set<taskresult> results
iterator<taskresult> resultiterator   results iterator
while  true
while  resultiterator hasnext
taskresult tskres   resultiterator next
if  tskres isrunning      false
return tskres
// in this loop, nothing was found
// sleep 10 seconds and restart
try
thread sleep sleep_time
catch  interruptedexception ie
// do nothing
resultiterator   results iterator
public boolean getresults arraylist<string> res  throws ioexception  commandneedretryexception
if  plan    null    plan getfetchtask      null
fetchtask ft   plan getfetchtask
ft setmaxrows maxrows
return ft fetch res
if  resstream    null
resstream   ctx getstream
if  resstream    null
return false
int numrows   0
string row   null
while  numrows < maxrows
if  resstream    null
if  numrows > 0
return true
else
return false
bos reset
utilities streamstatus ss
try
ss   utilities readcolumn resstream  bos
if  bos getcount   > 0
row   new string bos getdata    0  bos getcount
else if  ss    utilities streamstatus terminated
row   new string
if  row    null
numrows
res add row
catch  ioexception e
console printerror     e getmessage
res   null
return false
if  ss    utilities streamstatus eof
resstream   ctx getstream
return true
public int gettrycount
return trycount
public void settrycount int trycount
this trycount   trycount
public int close
try
if  plan    null
fetchtask fetchtask   plan getfetchtask
if  null    fetchtask
try
fetchtask clearfetch
catch  exception e
log debug    e
if  ctx    null
ctx clear
if  null    resstream
try
fsdatainputstream  resstream  close
catch  exception e
log debug    e
catch  exception e
console printerror     utilities getnamemessage e
org apache hadoop util stringutils stringifyexception e
return 13
return 0
public void destroy
if  ctx    null
releaselocks ctx gethivelocks
if  hivelockmgr    null
try
hivelockmgr close
catch lockexception e
log warn
org apache hadoop util stringutils stringifyexception e
public org apache hadoop hive ql plan api query getqueryplan   throws ioexception
return plan getqueryplan