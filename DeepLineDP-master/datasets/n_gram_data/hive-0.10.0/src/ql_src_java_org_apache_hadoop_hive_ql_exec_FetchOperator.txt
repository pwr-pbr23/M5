/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java io ioexception
import java io serializable
import java util arraylist
import java util arrays
import java util hashmap
import java util iterator
import java util list
import java util map
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hive common fileutils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive ql io hivecontextawarerecordreader
import org apache hadoop hive ql io hiveinputformat
import org apache hadoop hive ql io hiverecordreader
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata virtualcolumn
import org apache hadoop hive ql parse splitsample
import org apache hadoop hive ql plan fetchwork
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql session sessionstate loghelper
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 objectinspector delegatedobjectinspectorfactory
import org apache hadoop hive serde2 objectinspector inspectableobject
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspectorfactory
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop hive serde2 objectinspector primitive primitiveobjectinspectorfactory
import org apache hadoop io writable
import org apache hadoop io writablecomparable
import org apache hadoop mapred inputformat
import org apache hadoop mapred inputsplit
import org apache hadoop mapred jobconf
import org apache hadoop mapred recordreader
import org apache hadoop mapred reporter
import org apache hadoop util reflectionutils
/**
* fetchtask implementation.
**/
public class fetchoperator implements serializable
static log log   logfactory getlog fetchoperator class getname
static loghelper console   new loghelper log
private boolean isnativetable
private fetchwork work
private operator<?> operator        operator tree for processing row further  option
private int splitnum
private partitiondesc currpart
private tabledesc currtbl
private boolean tbldatadone
private boolean hasvc
private boolean ispartitioned
private structobjectinspector vcsoi
private list<virtualcolumn> vccols
private execmappercontext context
private transient recordreader<writablecomparable  writable> currrecreader
private transient fetchinputformatsplit inputsplits
private transient inputformat inputformat
private transient jobconf job
private transient writablecomparable key
private transient writable value
private transient writable vcvalues
private transient deserializer serde
private transient iterator<path> iterpath
private transient iterator<partitiondesc> iterpartdesc
private transient path currpath
private transient structobjectinspector objectinspector
private transient structobjectinspector rowobjectinspector
private transient object row
public fetchoperator
public fetchoperator fetchwork work  jobconf job
this job   job
this work   work
initialize
public fetchoperator fetchwork work  jobconf job  operator<?> operator
list<virtualcolumn> vccols
this job   job
this work   work
this operator   operator
this vccols   vccols
initialize
private void initialize
if  hasvc   vccols    null     vccols isempty
list<string> names   new arraylist<string> vccols size
list<objectinspector> inspectors   new arraylist<objectinspector> vccols size
for  virtualcolumn vc   vccols
inspectors add primitiveobjectinspectorfactory getprimitivewritableobjectinspector
vc gettypeinfo   getprimitivecategory
names add vc getname
vcsoi   objectinspectorfactory getstandardstructobjectinspector names  inspectors
vcvalues   new writable
ispartitioned   work ispartitioned
tbldatadone   false
if  hasvc    ispartitioned
row   new object
else if  hasvc    ispartitioned
row   new object
else
row   new object
if  work gettbldesc      null
isnativetable    work gettbldesc   isnonnative
else
isnativetable   true
setupexeccontext
private void setupexeccontext
if  hasvc    work getsplitsample      null
context   new execmappercontext
if  operator    null
operator setexeccontext context
public fetchwork getwork
return work
public void setwork fetchwork work
this work   work
public int getsplitnum
return splitnum
public void setsplitnum int splitnum
this splitnum   splitnum
public partitiondesc getcurrpart
return currpart
public void setcurrpart partitiondesc currpart
this currpart   currpart
public tabledesc getcurrtbl
return currtbl
public void setcurrtbl tabledesc currtbl
this currtbl   currtbl
public boolean istbldatadone
return tbldatadone
public void settbldatadone boolean tbldatadone
this tbldatadone   tbldatadone
public boolean isemptytable
return work gettbldir      null     work getpartdir      null    work getpartdir   isempty
/**
* a cache of inputformat instances.
*/
private static map<class  inputformat<writablecomparable  writable>> inputformats   new hashmap<class  inputformat<writablecomparable  writable>>
@suppresswarnings
static inputformat<writablecomparable  writable> getinputformatfromcache class inputformatclass
configuration conf  throws ioexception
if   inputformats containskey inputformatclass
try
inputformat<writablecomparable  writable> newinstance    inputformat<writablecomparable  writable>  reflectionutils
newinstance inputformatclass  conf
inputformats put inputformatclass  newinstance
catch  exception e
throw new ioexception
inputformatclass getname        e
return inputformats get inputformatclass
private structobjectinspector settabledesc tabledesc table  throws exception
deserializer serde   table getdeserializerclass   newinstance
serde initialize job  table getproperties
return createrowinspector getcurrent serde
private structobjectinspector setprtndesc partitiondesc partition  throws exception
deserializer serde   partition getdeserializerclass   newinstance
serde initialize job  partition getproperties
string pcols   partition gettabledesc   getproperties   getproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
string partkeys   pcols trim   split
row   createpartvalue partkeys  partition getpartspec
return createrowinspector getcurrent serde   partkeys
private structobjectinspector setprtndesc tabledesc table  throws exception
deserializer serde   table getdeserializerclass   newinstance
serde initialize job  table getproperties
string pcols   table getproperties   getproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
string partkeys   pcols trim   split
row   null
return createrowinspector getcurrent serde   partkeys
private structobjectinspector getcurrent deserializer serde  throws serdeexception
objectinspector current   serde getobjectinspector
if  objectinspector    null
current   delegatedobjectinspectorfactory reset objectinspector  current
else
current   delegatedobjectinspectorfactory wrap current
return objectinspector    structobjectinspector  current
private structobjectinspector createrowinspector structobjectinspector current
throws serdeexception
return hasvc ? objectinspectorfactory getunionstructobjectinspector
arrays aslist current  vcsoi     current
private structobjectinspector createrowinspector structobjectinspector current  string partkeys
throws serdeexception
list<string> partnames   new arraylist<string>
list<objectinspector> partobjectinspectors   new arraylist<objectinspector>
for  string key   partkeys
partnames add key
partobjectinspectors add primitiveobjectinspectorfactory javastringobjectinspector
structobjectinspector partobjectinspector   objectinspectorfactory
getstandardstructobjectinspector partnames  partobjectinspectors
return objectinspectorfactory getunionstructobjectinspector
hasvc ? arrays aslist current  partobjectinspector  vcsoi
arrays aslist current  partobjectinspector
private list<string> createpartvalue string partkeys  map<string  string> partspec
list<string> partvalues   new arraylist<string>
for  string key   partkeys
partvalues add partspec get key
return partvalues
private void getnextpath   throws exception
// first time
if  iterpath    null
if  work isnotpartitioned
if   tbldatadone
currpath   work gettbldirpath
currtbl   work gettbldesc
if  isnativetable
filesystem fs   currpath getfilesystem job
if  fs exists currpath
filestatus fstats   liststatusunderpath fs  currpath
for  filestatus fstat   fstats
if  fstat getlen   > 0
tbldatadone   true
break
else
tbldatadone   true
if   tbldatadone
currpath   null
return
else
currtbl   null
currpath   null
return
else
iterpath   fetchwork convertstringtopatharray work getpartdir    iterator
iterpartdesc   work getpartdesc   iterator
while  iterpath hasnext
path nxt   iterpath next
partitiondesc prt   null
if  iterpartdesc    null
prt   iterpartdesc next
filesystem fs   nxt getfilesystem job
if  fs exists nxt
filestatus fstats   liststatusunderpath fs  nxt
for  filestatus fstat   fstats
if  fstat getlen   > 0
currpath   nxt
if  iterpartdesc    null
currpart   prt
return
private recordreader<writablecomparable  writable> getrecordreader   throws exception
if  currpath    null
getnextpath
if  currpath    null
return null
// not using fileinputformat.setinputpaths() here because it forces a
// connection
// to the default file system - which may or may not be online during pure
// metadata
// operations
job set    org apache hadoop util stringutils escapestring currpath
tostring
partitiondesc tmp
if  currtbl    null
tmp   currpart
else
tmp   new partitiondesc currtbl  null
class<? extends inputformat> formatter   tmp getinputfileformatclass
inputformat   getinputformatfromcache formatter  job
utilities copytablejobpropertiestoconf tmp gettabledesc    job
inputsplit splits   inputformat getsplits job  1
fetchinputformatsplit inputsplits   new fetchinputformatsplit
for  int i   0  i < splits length  i
inputsplits   new fetchinputformatsplit splits  formatter getname
if  work getsplitsample      null
inputsplits   splitsampling work getsplitsample    inputsplits
this inputsplits   inputsplits
splitnum   0
serde   tmp getdeserializerclass   newinstance
serde initialize job  tmp getproperties
if  log isdebugenabled
log debug
serde getobjectinspector   gettypename
log debug     tmp getproperties
if  currpart    null
setprtndesc currpart
if  splitnum >  inputsplits length
if  currrecreader    null
currrecreader close
currrecreader   null
currpath   null
return getrecordreader
final fetchinputformatsplit target   inputsplits
@suppresswarnings
final recordreader<writablecomparable  writable> reader
inputformat getrecordreader target getinputsplit    job  reporter null
if  hasvc    work getsplitsample      null
currrecreader   new hiverecordreader<writablecomparable  writable> reader  job
@override
public boolean donext writablecomparable key  writable value  throws ioexception
// if current pos is larger than shrinkedlength which is calculated for
// each split by table sampling, stop fetching any more (early exit)
if  target shrinkedlength > 0
context getiocxt   getcurrentblockstart   > target shrinkedlength
return false
return super donext key  value
hivecontextawarerecordreader currrecreader
initiocontext target  job  inputformat getclass    reader
else
currrecreader   reader
splitnum
key   currrecreader createkey
value   currrecreader createvalue
return currrecreader
private fetchinputformatsplit splitsampling splitsample splitsample
fetchinputformatsplit splits
long totalsize   0
for  fetchinputformatsplit split  splits
totalsize    split getlength
list<fetchinputformatsplit> result   new arraylist<fetchinputformatsplit>
long targetsize    long   totalsize   splitsample getpercent     100d
int startindex   splitsample getseednum   % splits length
long size   0
for  int i   0  i < splits length  i
fetchinputformatsplit split   splits
result add split
long splitglength   split getlength
if  size   splitglength >  targetsize
if  size   splitglength > targetsize
split shrinkedlength   targetsize   size
break
size    splitglength
return result toarray new fetchinputformatsplit
/**
* get the next row and push down it to operator tree.
* currently only used by fetchtask.
**/
public boolean pushrow   throws ioexception  hiveexception
inspectableobject row   getnextrow
if  row    null
operator process row o  0
return row    null
private transient final inspectableobject inspectable   new inspectableobject
/**
* get the next row. the fetch context is modified appropriately.
*
**/
public inspectableobject getnextrow   throws ioexception
try
while  true
if  context    null
context resetrow
if  currrecreader    null
currrecreader   getrecordreader
if  currrecreader    null
return null
boolean ret   currrecreader next key  value
if  ret
if  operator    null    context    null    context inputfilechanged
// the child operators cleanup if input file has changed
try
operator cleanupinputfilechanged
catch  hiveexception e
throw new ioexception e
if  hasvc
vcvalues   mapoperator populatevirtualcolumnvalues context  vccols  vcvalues  serde
row   vcvalues
row   serde deserialize value
if  hasvc    ispartitioned
inspectable o   row
inspectable oi   rowobjectinspector
return inspectable
inspectable o   row
inspectable oi   serde getobjectinspector
return inspectable
else
currrecreader close
currrecreader   null
catch  exception e
throw new ioexception e
/**
* clear the context, if anything needs to be done.
*
**/
public void clearfetchcontext   throws hiveexception
try
if  currrecreader    null
currrecreader close
currrecreader   null
if  operator    null
operator close false
operator   null
if  context    null
context clear
context   null
this currtbl   null
this currpath   null
this iterpath   null
this iterpartdesc   null
catch  exception e
throw new hiveexception     e getmessage
org apache hadoop util stringutils stringifyexception e
/**
* used for bucket map join
*/
public void setupcontext list<path> paths
this iterpath   paths iterator
if  work isnotpartitioned
this currtbl   work gettbldesc
else
this iterpartdesc   work getpartdescs paths  iterator
setupexeccontext
/**
* returns output objectinspector, never null
*/
public objectinspector getoutputobjectinspector   throws hiveexception
try
if  work isnotpartitioned
return settabledesc work gettbldesc
list<partitiondesc> listparts   work getpartdesc
if  listparts    null    listparts isempty
return setprtndesc work gettbldesc
return setprtndesc listparts get 0
catch  exception e
throw new hiveexception     e getmessage
org apache hadoop util stringutils stringifyexception e
finally
currpart   null
/**
* lists status for all files under a given path. whether or not this is recursive depends on the
* setting of job configuration parameter mapred.input.dir.recursive.
*
* @param fs
*          file system
*
* @param p
*          path in file system
*
* @return list of file status entries
*/
private filestatus liststatusunderpath filesystem fs  path p  throws ioexception
hiveconf hiveconf   new hiveconf job  fetchoperator class
boolean recursive   hiveconf getboolvar hiveconf confvars hadoopmapredinputdirrecursive
if   recursive
return fs liststatus p
list<filestatus> results   new arraylist<filestatus>
for  filestatus stat   fs liststatus p
fileutils liststatusrecursively fs  stat  results
return results toarray new filestatus
// for split sampling. shrinkedlength is checked against iocontext.getcurrentblockstart,
// which is from recordreader.getpos(). so some inputformats which does not support getpos()
// like hivehbasetableinputformat cannot be used with this (todo)
private static class fetchinputformatsplit extends hiveinputformat hiveinputsplit
// shrinked size for this split. counter part of this in normal mode is
// inputsplitshim.shrinkedlength.
// what's different is that this is evaluated by unit of row using recordreader.getpos()
// and that is evaluated by unit of split using inputsplt.getlength().
private long shrinkedlength    1
public fetchinputformatsplit inputsplit split  string name
super split  name