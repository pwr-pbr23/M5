/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java beans defaultpersistencedelegate
import java beans encoder
import java beans exceptionlistener
import java beans expression
import java beans statement
import java beans xmldecoder
import java beans xmlencoder
import java io bufferedreader
import java io bytearrayinputstream
import java io bytearrayoutputstream
import java io datainput
import java io eofexception
import java io file
import java io fileinputstream
import java io filenotfoundexception
import java io ioexception
import java io inputstream
import java io inputstreamreader
import java io outputstream
import java io printstream
import java io serializable
import java io unsupportedencodingexception
import java net uri
import java net url
import java net urlclassloader
import java sql connection
import java sql drivermanager
import java sql preparedstatement
import java sql sqlexception
import java sql sqltransientexception
import java text simpledateformat
import java util arraylist
import java util arrays
import java util calendar
import java util collection
import java util collections
import java util hashmap
import java util hashset
import java util iterator
import java util linkedhashmap
import java util list
import java util map
import java util properties
import java util random
import java util set
import java util uuid
import java util concurrent concurrenthashmap
import java util concurrent executionexception
import java util concurrent future
import java util concurrent linkedblockingqueue
import java util concurrent threadpoolexecutor
import java util concurrent timeunit
import java util regex matcher
import java util regex pattern
import org apache commons lang stringutils
import org apache commons lang wordutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop filecache distributedcache
import org apache hadoop fs contentsummary
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop hive common hiveinterruptcallback
import org apache hadoop hive common hiveinterruptutils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api order
import org apache hadoop hive ql context
import org apache hadoop hive ql errormsg
import org apache hadoop hive ql queryplan
import org apache hadoop hive ql exec filesinkoperator recordwriter
import org apache hadoop hive ql io contentsummaryinputformat
import org apache hadoop hive ql io hivefileformatutils
import org apache hadoop hive ql io hiveignorekeytextoutputformat
import org apache hadoop hive ql io hiveinputformat
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive ql io hivesequencefileoutputformat
import org apache hadoop hive ql io rcfile
import org apache hadoop hive ql io reworkmapredinputformat
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql plan dynamicpartitionctx
import org apache hadoop hive ql plan exprnodecolumndesc
import org apache hadoop hive ql plan exprnodeconstantdesc
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan exprnodegenericfuncdesc
import org apache hadoop hive ql plan filesinkdesc
import org apache hadoop hive ql plan groupbydesc
import org apache hadoop hive ql plan mapredlocalwork
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan planutils
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan planutils expressiontypes
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive ql stats statsfactory
import org apache hadoop hive ql stats statspublisher
import org apache hadoop hive ql udf generic genericudf
import org apache hadoop hive ql udf generic genericudfopand
import org apache hadoop hive ql udf generic genericudfopequal
import org apache hadoop hive ql udf generic genericudfopor
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 serializer
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive shims shimloader
import org apache hadoop io ioutils
import org apache hadoop io sequencefile
import org apache hadoop io writable
import org apache hadoop io sequencefile compressiontype
import org apache hadoop io compress compressioncodec
import org apache hadoop io compress defaultcodec
import org apache hadoop mapred fileoutputformat
import org apache hadoop mapred inputformat
import org apache hadoop mapred jobconf
import org apache hadoop mapred reporter
import org apache hadoop mapred sequencefileinputformat
import org apache hadoop mapred sequencefileoutputformat
import org apache hadoop util reflectionutils
import org apache hadoop util shell
/**
* utilities.
*
*/
@suppresswarnings
public final class utilities
/**
* the object in the reducer are composed of these top level fields.
*/
public static string hadoop_local_fs
/**
* reducefield.
*
*/
public static enum reducefield
key  value  alias
private utilities
// prevent instantiation
}
private static map<string  mapredwork> gworkmap   collections
synchronizedmap new hashmap<string  mapredwork>
private static final log log   logfactory getlog utilities class getname
public static void clearmapredwork configuration job
try
path planpath   new path hiveconf getvar job  hiveconf confvars plan
filesystem fs   planpath getfilesystem job
if  fs exists planpath
try
fs delete planpath  true
catch  ioexception e
e printstacktrace
}
}
catch  exception e
finally
// where a single process works with multiple plans - we must clear
// the cache before working with the next plan.
string jobid   gethivejobid job
if  jobid    null
gworkmap remove jobid
}
}
}
public static mapredwork getmapredwork configuration job
mapredwork gwork   null
try
string jobid   gethivejobid job
assert jobid    null
gwork   gworkmap get jobid
if  gwork    null
string jtconf   shimloader gethadoopshims   getjoblauncherrpcaddress job
string path
if  jtconf equals
string planpath   hiveconf getvar job  hiveconf confvars plan
path   new path planpath  touri   getpath
} else {
path       jobid
}
inputstream in   new fileinputstream path
mapredwork ret   deserializemapredwork in  job
gwork   ret
gwork initialize
gworkmap put jobid  gwork
}
return  gwork
catch  exception e
e printstacktrace
throw new runtimeexception e
}
}
public static list<string> getfieldschemastring list<fieldschema> fl
if  fl    null
return null
}
arraylist<string> ret   new arraylist<string>
for  fieldschema f   fl
ret add f getname         f gettype
f getcomment      null ?      f getcomment
}
return ret
}
/**
* java 1.5 workaround. from http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5015403
*/
public static class enumdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
return new expression enum class     new object  oldinstance getclass
enum<?>  oldinstance  name
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return oldinstance    newinstance
}
}
public static class mapdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
map oldmap    map  oldinstance
hashmap newmap   new hashmap oldmap
return new expression newmap  hashmap class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
public static class setdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
set oldset    set  oldinstance
hashset newset   new hashset oldset
return new expression newset  hashset class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
public static class listdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
list oldlist    list  oldinstance
arraylist newlist   new arraylist oldlist
return new expression newlist  arraylist class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
public static void setmapredwork configuration job  mapredwork w  string hivescratchdir
try
// this is the unique job id, which is kept in jobconf as part of the plan file name
string jobid   uuid randomuuid   tostring
path planpath   new path hivescratchdir  jobid
hiveconf setvar job  hiveconf confvars plan  planpath touri   tostring
// use the default file system of the job
filesystem fs   planpath getfilesystem job
fsdataoutputstream out   fs create planpath
serializemapredwork w  out
// serialize the plan to the default hdfs instance
// except for hadoop local mode execution where we should be
// able to get the plan directly from the cache
if   shimloader gethadoopshims   islocalmode job
// set up distributed cache
distributedcache createsymlink job
string uriwithlink   planpath touri   tostring         jobid
distributedcache addcachefile new uri uriwithlink   job
// set replication of the plan file to a high number. we use the same
// replication factor as used by the hadoop jobclient for job.xml etc.
short replication    short  job getint    10
fs setreplication planpath  replication
}
// cache the plan in this process
w initialize
gworkmap put jobid  w
catch  exception e
e printstacktrace
throw new runtimeexception e
}
}
public static string gethivejobid configuration job
string planpath   hiveconf getvar job  hiveconf confvars plan
if  planpath    null
return  new path planpath   getname
}
return null
}
public static string serializeexpression exprnodedesc expr
bytearrayoutputstream baos   new bytearrayoutputstream
xmlencoder encoder   new xmlencoder baos
try
encoder writeobject expr
finally
encoder close
}
try
return baos tostring
catch  unsupportedencodingexception ex
throw new runtimeexception    ex
}
}
public static exprnodedesc deserializeexpression string s  configuration conf
byte bytes
try
bytes   s getbytes
catch  unsupportedencodingexception ex
throw new runtimeexception    ex
}
bytearrayinputstream bais   new bytearrayinputstream bytes
xmldecoder decoder   new xmldecoder bais  null  null
try
exprnodedesc expr    exprnodedesc  decoder readobject
return expr
finally
decoder close
}
}
/**
* serialize a single task.
*/
public static void serializetasks task<? extends serializable> t  outputstream out
xmlencoder e   null
try
e   new xmlencoder out
// workaround for java 1.5
e setpersistencedelegate expressiontypes class  new enumdelegate
e setpersistencedelegate groupbydesc mode class  new enumdelegate
e setpersistencedelegate operator progresscounter class  new enumdelegate
e writeobject t
finally
if  null    e
e close
}
}
}
public static class collectionpersistencedelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
return new expression oldinstance  oldinstance getclass       null
}
@override
protected void initialize class type  object oldinstance  object newinstance  encoder out
iterator ite     collection  oldinstance  iterator
while  ite hasnext
out writestatement new statement oldinstance     new object  ite next
}
}
}
/**
* serialize the whole query plan.
*/
public static void serializequeryplan queryplan plan  outputstream out
xmlencoder e   new xmlencoder out
e setexceptionlistener new exceptionlistener
public void exceptionthrown exception e
log warn org apache hadoop util stringutils stringifyexception e
throw new runtimeexception    e
}
// workaround for java 1.5
e setpersistencedelegate expressiontypes class  new enumdelegate
e setpersistencedelegate groupbydesc mode class  new enumdelegate
e setpersistencedelegate operator progresscounter class  new enumdelegate
e setpersistencedelegate org datanucleus sco backed map class  new mapdelegate
e setpersistencedelegate org datanucleus sco backed list class  new listdelegate
e writeobject plan
e close
}
/**
* deserialize the whole query plan.
*/
public static queryplan deserializequeryplan inputstream in  configuration conf
xmldecoder d   null
try
d   new xmldecoder in  null  null
queryplan ret    queryplan  d readobject
return  ret
finally
if  null    d
d close
}
}
}
/**
* serialize the mapredwork object to an output stream. do not use this to write to standard
* output since it closes the output stream. do use mapredwork.toxml() instead.
*/
public static void serializemapredwork mapredwork w  outputstream out
xmlencoder e   null
try
e   new xmlencoder out
// workaround for java 1.5
e setpersistencedelegate expressiontypes class  new enumdelegate
e setpersistencedelegate groupbydesc mode class  new enumdelegate
e writeobject w
finally
if  null    e
e close
}
}
}
public static mapredwork deserializemapredwork inputstream in  configuration conf
xmldecoder d   null
try
d   new xmldecoder in  null  null
mapredwork ret    mapredwork  d readobject
return  ret
finally
if  null    d
d close
}
}
}
/**
* serialize the mapredlocalwork object to an output stream. do not use this to write to standard
* output since it closes the output stream. do use mapredwork.toxml() instead.
*/
public static void serializemapredlocalwork mapredlocalwork w  outputstream out
xmlencoder e   null
try
e   new xmlencoder out
// workaround for java 1.5
e setpersistencedelegate expressiontypes class  new enumdelegate
e setpersistencedelegate groupbydesc mode class  new enumdelegate
e writeobject w
finally
if  null    e
e close
}
}
}
public static mapredlocalwork deserializemapredlocalwork inputstream in  configuration conf
xmldecoder d   null
try
d   new xmldecoder in  null  null
mapredlocalwork ret    mapredlocalwork  d readobject
return  ret
finally
if  null    d
d close
}
}
}
/**
* tuple.
*
* @param <t>
* @param <v>
*/
public static class tuple<t  v>
private final t one
private final v two
public tuple t one  v two
this one   one
this two   two
}
public t getone
return this one
}
public v gettwo
return this two
}
}
public static tabledesc defaulttd
static
// by default we expect ^a separated strings
// this tabledesc does not provide column names. we should always use
// planutils.getdefaulttabledesc(string separatorcode, string columns)
// or getbinarysortabletabledesc(list<fieldschema> fieldschemas) when
// we know the column names.
defaulttd   planutils getdefaulttabledesc     utilities ctrlacode
}
public static final int carriagereturncode   13
public static final int newlinecode   10
public static final int tabcode   9
public static final int ctrlacode   1
public static final string indent
// note: when ddl supports specifying what string to represent null,
// we should specify "null" to represent null in the temp table, and then
// we can make the following translation deprecated.
public static string nullstringstorage
public static string nullstringoutput
public static random randgen   new random
/**
* gets the task id if we are running as a hadoop job. gets a random number otherwise.
*/
public static string gettaskid configuration hconf
string taskid    hconf    null  ? null   hconf get
if   taskid    null     taskid equals
return      math abs randgen nextint
} else {
/*
* extract the task and attempt id from the hadoop taskid. in version 17 the leading component
* was 'task_'. thereafter the leading component is 'attempt_'. in 17 - hadoop also seems to
* have used _map_ and _reduce_ to denote map/reduce task types
*/
string ret   taskid replaceall       replaceall
return  ret
}
}
public static hashmap makemap object    olist
hashmap ret   new hashmap
for  int i   0  i < olist length  i    2
ret put olist  olist
}
return  ret
}
public static properties makeproperties string    olist
properties ret   new properties
for  int i   0  i < olist length  i    2
ret setproperty olist  olist
}
return  ret
}
public static arraylist makelist object    olist
arraylist ret   new arraylist
for  object element   olist
ret add element
}
return  ret
}
/**
* streamprinter.
*
*/
public static class streamprinter extends thread
inputstream is
string type
printstream os
public streamprinter inputstream is  string type  printstream os
this is   is
this type   type
this os   os
}
@override
public void run
bufferedreader br   null
try
inputstreamreader isr   new inputstreamreader is
br   new bufferedreader isr
string line   null
if  type    null
while   line   br readline       null
os println type       line
}
} else {
while   line   br readline       null
os println line
}
}
br close
br null
catch  ioexception ioe
ioe printstacktrace
finally
ioutils closestream br
}
}
}
public static tabledesc gettabledesc table tbl
return  new tabledesc tbl getdeserializer   getclass    tbl getinputformatclass    tbl
getoutputformatclass    tbl getschema
}
// column names and column types are all delimited by comma
public static tabledesc gettabledesc string cols  string coltypes
return  new tabledesc lazysimpleserde class  sequencefileinputformat class
hivesequencefileoutputformat class  utilities makeproperties
org apache hadoop hive serde serdeconstants serialization_format      utilities ctrlacode
org apache hadoop hive serde serdeconstants list_columns  cols
org apache hadoop hive serde serdeconstants list_column_types  coltypes
}
public static partitiondesc getpartitiondesc partition part  throws hiveexception
return  new partitiondesc part
}
public static partitiondesc getpartitiondescfromtabledesc tabledesc tbldesc  partition part
throws hiveexception
return new partitiondesc part  tbldesc
}
public static void addmapwork mapredwork mr  table tbl  string alias  operator<?> work
mr addmapwork tbl getdatalocation   getpath    alias  work  new partitiondesc
gettabledesc tbl    linkedhashmap<string  string>  null
}
private static string getoptreeskel_helper operator<?> op  string indent
if  op    null
return
}
stringbuilder sb   new stringbuilder
sb append indent
sb append op tostring
sb append
if  op getchildoperators      null
for  object child   op getchildoperators
sb append getoptreeskel_helper  operator<?>  child  indent
}
}
return sb tostring
}
public static string getoptreeskel operator<?> op
return getoptreeskel_helper op
}
private static boolean iswhitespace int c
if  c     1
return false
}
return character iswhitespace  char  c
}
public static boolean contentsequal inputstream is1  inputstream is2  boolean ignorewhitespace
throws ioexception
try
if   is1    is2      is1    null    is2    null
return true
}
if  is1    null    is2    null
return false
}
while  true
int c1   is1 read
while  ignorewhitespace    iswhitespace c1
c1   is1 read
}
int c2   is2 read
while  ignorewhitespace    iswhitespace c2
c2   is2 read
}
if  c1     1    c2     1
return true
}
if  c1    c2
break
}
}
catch  filenotfoundexception e
e printstacktrace
}
return false
}
/**
* convert "from src insert blah blah" to "from src insert ... blah"
*/
public static string abbreviate string str  int max
str   str trim
int len   str length
int suffixlength   20
if  len <  max
return str
}
suffixlength   math min suffixlength   max   3    2
string rev   stringutils reverse str
// get the last few words
string suffix   wordutils abbreviate rev  0  suffixlength
suffix   stringutils reverse suffix
// first few ..
string prefix   stringutils abbreviate str  max   suffix length
return prefix   suffix
}
public static final string nstr
/**
* streamstatus.
*
*/
public static enum streamstatus
eof  terminated
}
public static streamstatus readcolumn datainput in  outputstream out  throws ioexception
boolean foundcrchar   false
while  true
int b
try
b   in readbyte
catch  eofexception e
return streamstatus eof
}
// default new line characters on windows are "crlf" so detect if there are any windows
// native newline characters and handle them.
if  shell windows
// if the cr is not followed by the lf on windows then add it back to the stream and
// proceed with next characters in the input stream.
if  foundcrchar    b    utilities newlinecode
out write utilities carriagereturncode
foundcrchar   false
}
if  b    utilities carriagereturncode
foundcrchar   true
continue
}
}
if  b    utilities newlinecode
return streamstatus terminated
}
out write b
}
// unreachable
}
/**
* convert an output stream to a compressed output stream based on codecs and compression options
* specified in the job configuration.
*
* @param jc
*          job configuration
* @param out
*          output stream to be converted into compressed output stream
* @return compressed output stream
*/
public static outputstream createcompressedstream jobconf jc  outputstream out
throws ioexception
boolean iscompressed   fileoutputformat getcompressoutput jc
return createcompressedstream jc  out  iscompressed
}
/**
* convert an output stream to a compressed output stream based on codecs codecs in the job
* configuration. caller specifies directly whether file is compressed or not
*
* @param jc
*          job configuration
* @param out
*          output stream to be converted into compressed output stream
* @param iscompressed
*          whether the output stream needs to be compressed or not
* @return compressed output stream
*/
public static outputstream createcompressedstream jobconf jc  outputstream out
boolean iscompressed  throws ioexception
if  iscompressed
class<? extends compressioncodec> codecclass   fileoutputformat getoutputcompressorclass jc
defaultcodec class
compressioncodec codec    compressioncodec  reflectionutils newinstance codecclass  jc
return codec createoutputstream out
} else {
return  out
}
}
/**
* based on compression option and configured output codec - get extension for output file. this
* is only required for text files - not sequencefiles
*
* @param jc
*          job configuration
* @param iscompressed
*          whether the output file is compressed or not
* @return the required file extension (example: .gz)
* @deprecated use {@link #getfileextension(jobconf, boolean, hiveoutputformat)}
*/
@deprecated
public static string getfileextension jobconf jc  boolean iscompressed
return getfileextension jc  iscompressed  new hiveignorekeytextoutputformat
}
/**
* based on compression option, output format, and configured output codec -
* get extension for output file. text files require an extension, whereas
* others, like sequence files, do not.
* <p>
* the property <code>hive.output.file.extension</code> is used to determine
* the extension - if set, it will override other logic for choosing an
* extension.
*
* @param jc
*          job configuration
* @param iscompressed
*          whether the output file is compressed or not
* @param hiveoutputformat
*          the output format, used to detect if the format is text
* @return the required file extension (example: .gz)
*/
public static string getfileextension jobconf jc  boolean iscompressed
hiveoutputformat<?  ?> hiveoutputformat
string extension   hiveconf getvar jc  hiveconf confvars output_file_extension
if   stringutils isempty extension
return extension
}
if   hiveoutputformat instanceof hiveignorekeytextoutputformat     iscompressed
class<? extends compressioncodec> codecclass   fileoutputformat getoutputcompressorclass jc
defaultcodec class
compressioncodec codec    compressioncodec  reflectionutils newinstance codecclass  jc
return codec getdefaultextension
}
return
}
/**
* create a sequencefile output stream based on job configuration.
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @param keyclass
*          java class for key
* @param valclass
*          java class for value
* @return output stream over the created sequencefile
*/
public static sequencefile writer createsequencewriter jobconf jc  filesystem fs  path file
class<?> keyclass  class<?> valclass  throws ioexception
boolean iscompressed   fileoutputformat getcompressoutput jc
return createsequencewriter jc  fs  file  keyclass  valclass  iscompressed
}
/**
* create a sequencefile output stream based on job configuration uses user supplied compression
* flag (rather than obtaining it from the job configuration).
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @param keyclass
*          java class for key
* @param valclass
*          java class for value
* @return output stream over the created sequencefile
*/
public static sequencefile writer createsequencewriter jobconf jc  filesystem fs  path file
class<?> keyclass  class<?> valclass  boolean iscompressed  throws ioexception
compressioncodec codec   null
compressiontype compressiontype   compressiontype none
class codecclass   null
if  iscompressed
compressiontype   sequencefileoutputformat getoutputcompressiontype jc
codecclass   fileoutputformat getoutputcompressorclass jc  defaultcodec class
codec    compressioncodec  reflectionutils newinstance codecclass  jc
}
return  sequencefile createwriter fs  jc  file  keyclass  valclass  compressiontype  codec
}
/**
* create a rcfile output stream based on job configuration uses user supplied compression flag
* (rather than obtaining it from the job configuration).
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @return output stream over the created rcfile
*/
public static rcfile writer creatercfilewriter jobconf jc  filesystem fs  path file
boolean iscompressed  throws ioexception
compressioncodec codec   null
class<?> codecclass   null
if  iscompressed
codecclass   fileoutputformat getoutputcompressorclass jc  defaultcodec class
codec    compressioncodec  reflectionutils newinstance codecclass  jc
}
return new rcfile writer fs  jc  file  null  codec
}
/**
* shamelessly cloned from genericoptionsparser.
*/
public static string realfile string newfile  configuration conf  throws ioexception
path path   new path newfile
uri pathuri   path touri
filesystem fs
if  pathuri getscheme      null
fs   filesystem getlocal conf
} else {
fs   path getfilesystem conf
}
if   fs exists path
return null
}
string file   path makequalified fs  tostring
// for compatibility with hadoop 0.17, change file:/a/b/c to file:///a/b/c
if  stringutils startswith file         stringutils startswith file
file       file substring   length
}
return file
}
public static list<string> mergeuniqelems list<string> src  list<string> dest
if  dest    null
return src
}
if  src    null
return dest
}
int pos   0
while  pos < dest size
if   src contains dest get pos
src add dest get pos
}
pos
}
return src
}
private static final string tmpprefix
private static final string tasktmpprefix
public static path totasktemppath path orig
if  orig getname   indexof tasktmpprefix     0
return orig
}
return new path orig getparent    tasktmpprefix   orig getname
}
public static path totasktemppath string orig
return totasktemppath new path orig
}
public static path totemppath path orig
if  orig getname   indexof tmpprefix     0
return orig
}
return new path orig getparent    tmpprefix   orig getname
}
/**
* given a path, convert to a temporary path.
*/
public static path totemppath string orig
return totemppath new path orig
}
/**
* detect if the supplied file is a temporary path.
*/
public static boolean istemppath filestatus file
string name   file getpath   getname
// in addition to detecting hive temporary files, we also check hadoop
// temporary folders that used to show up in older releases
return  name startswith       name startswith tmpprefix
}
/**
* rename src to dst, or in the case dst already exists, move files in src to dst. if there is an
* existing file with the same name, the new file's name will be appended with "_1", "_2", etc.
*
* @param fs
*          the filesystem where src and dst are on.
* @param src
*          the src directory
* @param dst
*          the target directory
* @throws ioexception
*/
public static void rename filesystem fs  path src  path dst  throws ioexception  hiveexception
if   fs rename src  dst
throw new hiveexception     src       dst
}
}
/**
* rename src to dst, or in the case dst already exists, move files in src to dst. if there is an
* existing file with the same name, the new file's name will be appended with "_1", "_2", etc.
*
* @param fs
*          the filesystem where src and dst are on.
* @param src
*          the src directory
* @param dst
*          the target directory
* @throws ioexception
*/
public static void renameormovefiles filesystem fs  path src  path dst  throws ioexception
hiveexception {
if   fs exists dst
if   fs rename src  dst
throw new hiveexception     src       dst
}
} else {
// move file by file
filestatus files   fs liststatus src
for  filestatus file   files
path srcfilepath   file getpath
string filename   srcfilepath getname
path dstfilepath   new path dst  filename
if  file isdir
renameormovefiles fs  srcfilepath  dstfilepath
}
else
if  fs exists dstfilepath
int suffix   0
do
suffix
dstfilepath   new path dst  filename       suffix
while  fs exists dstfilepath
}
if   fs rename srcfilepath  dstfilepath
throw new hiveexception     src       dst
}
}
}
}
}
/**
* the first group will contain the task id. the second group is the optional extension. the file
* name looks like: "0_0" or "0_0.gz". there may be a leading prefix (tmp_). since gettaskid() can
* return an integer only - this should match a pure integer as well. {1,3} is used to limit
* matching for attempts #'s 0-999.
*/
private static final pattern file_name_to_task_id_regex
pattern compile
/**
* this retruns prefix part + taskid for bucket join for partitioned table
*/
private static final pattern file_name_prefixed_task_id_regex
pattern compile
/**
* this breaks a prefixed bucket number into the prefix and the taskid
*/
private static final pattern prefixed_task_id_regex
pattern compile
/**
* get the task id from the filename. it is assumed that the filename is derived from the output
* of gettaskid
*
* @param filename
*          filename to extract taskid from
*/
public static string gettaskidfromfilename string filename
return getidfromfilename filename  file_name_to_task_id_regex
}
/**
* get the part-spec + task id from the filename. it is assumed that the filename is derived
* from the output of gettaskid
*
* @param filename
*          filename to extract taskid from
*/
public static string getprefixedtaskidfromfilename string filename
return getidfromfilename filename  file_name_prefixed_task_id_regex
}
private static string getidfromfilename string filename  pattern pattern
string taskid   filename
int dirend   filename lastindexof path separator
if  dirend     1
taskid   filename substring dirend   1
}
matcher m   pattern matcher taskid
if   m matches
log warn     filename
taskid
} else {
taskid   m group 1
}
log debug     filename       taskid
return taskid
}
public static string getfilenamefromdirname string dirname
int dirend   dirname lastindexof path separator
if  dirend     1
return dirname substring dirend   1
}
return dirname
}
/**
* replace the task id from the filename. it is assumed that the filename is derived from the
* output of gettaskid
*
* @param filename
*          filename to replace taskid "0_0" or "0_0.gz" by 33 to "33_0" or "33_0.gz"
*/
public static string replacetaskidfromfilename string filename  int bucketnum
return replacetaskidfromfilename filename  string valueof bucketnum
}
public static string replacetaskidfromfilename string filename  string fileid
string taskid   gettaskidfromfilename filename
string newtaskid   replacetaskid taskid  fileid
string ret   replacetaskidfromfilename filename  taskid  newtaskid
return  ret
}
private static string replacetaskid string taskid  int bucketnum
return replacetaskid taskid  string valueof bucketnum
}
/**
* returns strbucketnum with enough 0's prefixing the task id portion of the string to make it
* equal in length to taskid
*
* @param taskid - the taskid used as a template for length
* @param strbucketnum - the bucket number of the output, may or may not be prefixed
* @return
*/
private static string replacetaskid string taskid  string strbucketnum
matcher m   prefixed_task_id_regex matcher strbucketnum
if   m matches
log warn     strbucketnum
return adjustbucketnumlen strbucketnum  taskid
} else {
string adjustedbucketnum   adjustbucketnumlen m group 2   taskid
return  m group 1     null ?     m group 1     adjustedbucketnum
}
}
/**
* adds 0's to the beginning of bucketnum until bucketnum and taskid are the same length.
*
* @param bucketnum - the bucket number, should not be prefixed
* @param taskid - the taskid used as a template for length
* @return
*/
private static string adjustbucketnumlen string bucketnum  string taskid
int bucketnumlen   bucketnum length
int taskidlen   taskid length
stringbuffer s   new stringbuffer
for  int i   0  i < taskidlen   bucketnumlen  i
s append
}
return s tostring     bucketnum
}
/**
* replace the oldtaskid appearing in the filename by the newtaskid. the string oldtaskid could
* appear multiple times, we should only replace the last one.
*
* @param filename
* @param oldtaskid
* @param newtaskid
* @return
*/
private static string replacetaskidfromfilename string filename  string oldtaskid
string newtaskid
string spl   filename split oldtaskid
if   spl length    0      spl length    1
return filename replaceall oldtaskid  newtaskid
}
stringbuffer snew   new stringbuffer
for  int idx   0  idx < spl length   1  idx
if  idx > 0
snew append oldtaskid
}
snew append spl
}
snew append newtaskid
snew append spl
return snew tostring
}
/**
* get all file status from a root path and recursively go deep into certain levels.
*
* @param path
*          the root path
* @param level
*          the depth of directory should explore
* @param fs
*          the file system
* @return array of filestatus
* @throws ioexception
*/
public static filestatus getfilestatusrecurse path path  int level  filesystem fs
throws ioexception
// construct a path pattern (e.g., /*/*) to find all dynamically generated paths
stringbuilder sb = new stringbuilder(path.touri().getpath());
for (int i = 0; i < level; ++i) {
sb.append(path.separator).append("*");
}
path pathpattern = new path(path, sb.tostring());
return fs.globstatus(pathpattern);
}
public static void mvfiletofinalpath(string specpath, configuration hconf,
boolean success, log log, dynamicpartitionctx dpctx, filesinkdesc conf,
reporter reporter) throws ioexception,
hiveexception {
filesystem fs = (new path(specpath)).getfilesystem(hconf);
path tmppath = utilities.totemppath(specpath);
path tasktmppath = utilities.totasktemppath(specpath);
path intermediatepath = new path(tmppath.getparent(), tmppath.getname()
+ ".intermediate");
path finalpath = new path(specpath);
if (success) {
if (fs.exists(tmppath)) {
// step1: rename tmp output folder to intermediate path. after this
// point, updates from speculative tasks still writing to tmppath
// will not appear in finalpath.
log.info("moving tmp dir: " + tmppath + " to: " + intermediatepath);
utilities.rename(fs, tmppath, intermediatepath);
// step2: remove any tmp file or double-committed output files
arraylist<string> emptybuckets =
utilities.removetemporduplicatefiles(fs, intermediatepath, dpctx);
// create empty buckets if necessary
if (emptybuckets.size() > 0) {
createemptybuckets(hconf, emptybuckets, conf, reporter);
}
// step3: move to the file destination
log.info("moving tmp dir: " + intermediatepath + " to: " + finalpath);
utilities.renameormovefiles(fs, intermediatepath, finalpath);
}
} else {
fs.delete(tmppath, true);
}
fs.delete(tasktmppath, true);
}
/**
* check the existence of buckets according to bucket specification. create empty buckets if
* needed.
*
* @param hconf
* @param paths a list of empty buckets to create
* @param conf the definition of the filesink.
* @param reporter the mapreduce reporter object
* @throws hiveexception
* @throws ioexception
*/
private static void createemptybuckets configuration hconf  arraylist<string> paths
filesinkdesc conf  reporter reporter
throws hiveexception  ioexception
jobconf jc
if  hconf instanceof jobconf
jc   new jobconf hconf
} else {
// test code path
jc   new jobconf hconf  execdriver class
}
hiveoutputformat<?  ?> hiveoutputformat   null
class<? extends writable> outputclass   null
boolean iscompressed   conf getcompressed
tabledesc tableinfo   conf gettableinfo
try
serializer serializer    serializer  tableinfo getdeserializerclass   newinstance
serializer initialize null  tableinfo getproperties
outputclass   serializer getserializedclass
hiveoutputformat   conf gettableinfo   getoutputfileformatclass   newinstance
catch  serdeexception e
throw new hiveexception e
catch  instantiationexception e
throw new hiveexception e
catch  illegalaccessexception e
throw new hiveexception e
}
for  string p   paths
path path   new path p
recordwriter writer   hivefileformatutils getrecordwriter
jc  hiveoutputformat  outputclass  iscompressed
tableinfo getproperties    path  reporter
writer close false
log info     path
}
}
/**
* remove all temporary files and duplicate (double-committed) files from a given directory.
*/
public static void removetemporduplicatefiles filesystem fs  path path  throws ioexception
removetemporduplicatefiles fs  path  null
}
/**
* remove all temporary files and duplicate (double-committed) files from a given directory.
*
* @return a list of path names corresponding to should-be-created empty buckets.
*/
public static arraylist<string> removetemporduplicatefiles filesystem fs  path path
dynamicpartitionctx dpctx  throws ioexception
if  path    null
return null
}
arraylist<string> result   new arraylist<string>
if  dpctx    null
filestatus parts   getfilestatusrecurse path  dpctx getnumdpcols    fs
hashmap<string  filestatus> taskidtofile   null
for  int i   0  i < parts length    i
assert parts isdir         parts getpath
filestatus items   fs liststatus parts getpath
// remove empty directory since dp insert should not generate empty partitions.
// empty directories could be generated by crashed task/scriptoperator
if  items length    0
if   fs delete parts getpath    true
log error     parts getpath
throw new ioexception     parts getpath
}
}
taskidtofile   removetemporduplicatefiles items  fs
// if the table is bucketed and enforce bucketing, we should check and generate all buckets
if  dpctx getnumbuckets   > 0    taskidtofile    null
// refresh the file list
items   fs liststatus parts getpath
// get the missing buckets and generate empty buckets
string taskid1   taskidtofile keyset   iterator   next
path bucketpath   taskidtofile values   iterator   next   getpath
for  int j   0  j < dpctx getnumbuckets      j
string taskid2   replacetaskid taskid1  j
if   taskidtofile containskey taskid2
// create empty bucket, file name should be derived from taskid2
string path2   replacetaskidfromfilename bucketpath touri   getpath   tostring    j
result add path2
}
}
}
}
} else {
filestatus items   fs liststatus path
removetemporduplicatefiles items  fs
}
return result
}
public static hashmap<string  filestatus> removetemporduplicatefiles filestatus items
filesystem fs  throws ioexception
if  items    null    fs    null
return null
}
hashmap<string  filestatus> taskidtofile   new hashmap<string  filestatus>
for  filestatus one   items
if  istemppath one
if   fs delete one getpath    true
throw new ioexception     one getpath
}
} else {
string taskid   getprefixedtaskidfromfilename one getpath   getname
filestatus otherfile   taskidtofile get taskid
if  otherfile    null
taskidtofile put taskid  one
} else {
// compare the file sizes of all the attempt files for the same task, the largest win
// any attempt files could contain partial results (due to task failures or
// speculative runs), but the largest should be the correct one since the result
// of a successful run should never be smaller than a failed/speculative run.
filestatus todelete   null
if  otherfile getlen   >  one getlen
todelete   one
} else {
todelete   otherfile
taskidtofile put taskid  one
}
long len1   todelete getlen
long len2   taskidtofile get taskid  getlen
if   fs delete todelete getpath    true
throw new ioexception     todelete getpath
taskidtofile get taskid  getpath
} else {
log warn     todelete getpath
len1       taskidtofile get taskid  getpath
len2
}
}
}
}
return taskidtofile
}
public static string getnamemessage exception e
return e getclass   getname         e getmessage
}
public static string getresourcefiles configuration conf  sessionstate resourcetype t
// fill in local files to be added to the task environment
sessionstate ss   sessionstate get
set<string> files    ss    null  ? null   ss list_resource t  null
if  files    null
list<string> realfiles   new arraylist<string> files size
for  string one   files
try
realfiles add realfile one  conf
catch  ioexception e
throw new runtimeexception     one
e getmessage    e
}
}
return stringutils join realfiles
} else {
return
}
}
/**
* add new elements to the classpath.
*
* @param newpaths
*          array of classpath elements
*/
public static classloader addtoclasspath classloader cloader  string newpaths  throws exception
urlclassloader loader    urlclassloader  cloader
list<url> curpath   arrays aslist loader geturls
arraylist<url> newpath   new arraylist<url>
// get a list with the current classpath components
for  url onepath   curpath
newpath add onepath
}
curpath   newpath
for  string onestr   newpaths
// special processing for hadoop-17. file:// needs to be removed
if  stringutils indexof onestr        0
onestr   stringutils substring onestr  7
}
url oneurl    new file onestr   tourl
if   curpath contains oneurl
curpath add oneurl
}
}
return new urlclassloader curpath toarray new url   loader
}
/**
* remove elements from the classpath.
*
* @param pathstoremove
*          array of classpath elements
*/
public static void removefromclasspath string pathstoremove  throws exception
thread curthread   thread currentthread
urlclassloader loader    urlclassloader  curthread getcontextclassloader
set<url> newpath   new hashset<url> arrays aslist loader geturls
for  string onestr   pathstoremove
// special processing for hadoop-17. file:// needs to be removed
if  stringutils indexof onestr        0
onestr   stringutils substring onestr  7
}
url oneurl    new file onestr   tourl
newpath remove oneurl
}
loader   new urlclassloader newpath toarray new url
curthread setcontextclassloader loader
sessionstate get   getconf   setclassloader loader
}
public static string formatbinarystring byte array  int start  int length
stringbuilder sb   new stringbuilder
for  int i   start  i < start   length  i
sb append
sb append array < 0 ? array   256   array   0
}
return sb tostring
}
public static list<string> getcolumnnamesfromsortcols list<order> sortcols
list<string> names   new arraylist<string>
for  order o   sortcols
names add o getcol
}
return names
}
public static list<string> getcolumnnamesfromfieldschema list<fieldschema> partcols
list<string> names   new arraylist<string>
for  fieldschema o   partcols
names add o getname
}
return names
}
public static list<string> getcolumnnames properties props
list<string> names   new arraylist<string>
string colnames   props getproperty serdeconstants list_columns
string cols   colnames trim   split
if  cols    null
for  string col   cols
if  col    null     col trim   equals
names add col
}
}
}
return names
}
public static list<string> getcolumntypes properties props
list<string> names   new arraylist<string>
string colnames   props getproperty serdeconstants list_column_types
string cols   colnames trim   split
if  cols    null
for  string col   cols
if  col    null     col trim   equals
names add col
}
}
}
return names
}
public static void validatecolumnnames list<string> colnames  list<string> checkcols
throws semanticexception
iterator<string> checkcolsiter   checkcols iterator
while  checkcolsiter hasnext
string tocheck   checkcolsiter next
boolean found   false
iterator<string> colnamesiter   colnames iterator
while  colnamesiter hasnext
string colname   colnamesiter next
if  tocheck equalsignorecase colname
found   true
break
}
}
if   found
throw new semanticexception errormsg invalid_column getmsg
}
}
}
/**
* gets the default notification interval to send progress updates to the tracker. useful for
* operators that may not output data for a while.
*
* @param hconf
* @return the interval in milliseconds
*/
public static int getdefaultnotificationinterval configuration hconf
int notificationinterval
integer expinterval   integer decode hconf get
if  expinterval    null
notificationinterval   expinterval intvalue     2
} else {
// 5 minutes
notificationinterval   5   60   1000
}
return notificationinterval
}
/**
* copies the storage handler properties configured for a table descriptor to a runtime job
* configuration.
*
* @param tbl
*          table descriptor from which to read
*
* @param job
*          configuration which receives configured properties
*/
public static void copytablejobpropertiestoconf tabledesc tbl  jobconf job
map<string  string> jobproperties   tbl getjobproperties
if  jobproperties    null
return
}
for  map entry<string  string> entry   jobproperties entryset
job set entry getkey    entry getvalue
}
}
public static object getinputsummarylock   new object
/**
* calculate the total size of input files.
*
* @param ctx
*          the hadoop job context
* @param work
*          map reduce job plan
* @param filter
*          filter to apply to the input paths before calculating size
* @return the summary of all the input paths.
* @throws ioexception
*/
public static contentsummary getinputsummary context ctx  mapredwork work  pathfilter filter
throws ioexception
long summary    0  0  0
list<string> pathneedprocess   new arraylist<string>
// since multiple threads could call this method concurrently, locking
// this method will avoid number of threads out of control.
synchronized  getinputsummarylock
// for each input path, calculate the total size.
for  string path   work getpathtoaliases   keyset
path p   new path path
if  filter    null     filter accept p
continue
}
contentsummary cs   ctx getcs path
if  cs    null
if  path    null
continue
}
pathneedprocess add path
} else {
summary    cs getlength
summary    cs getfilecount
summary    cs getdirectorycount
}
}
// process the case when name node call is needed
final map<string  contentsummary> resultmap   new concurrenthashmap<string  contentsummary>
arraylist<future<?>> results   new arraylist<future<?>>
final threadpoolexecutor executor
int maxthreads   ctx getconf   getint    0
if  pathneedprocess size   > 1    maxthreads > 1
int numexecutors   math min pathneedprocess size    maxthreads
log info     numexecutors
executor   new threadpoolexecutor numexecutors  numexecutors  60  timeunit seconds
new linkedblockingqueue<runnable>
} else {
executor   null
}
hiveinterruptcallback interrup   hiveinterruptutils add new hiveinterruptcallback
@override
public void interrupt
if  executor    null
executor shutdownnow
}
}
try
configuration conf   ctx getconf
jobconf jobconf   new jobconf conf
for  string path   pathneedprocess
final path p   new path path
final string pathstr   path
// all threads share the same configuration and jobconf based on the
// assumption that they are thread safe if only read operations are
// executed. it is not stated in hadoop's javadoc, the sourcce codes
// clearly showed that they made efforts for it and we believe it is
// thread safe. will revisit this piece of codes if we find the assumption
// is not correct.
final configuration myconf   conf
final jobconf myjobconf   jobconf
final partitiondesc partdesc   work getpathtopartitioninfo   get
p tostring
runnable r   new runnable
public void run
try
contentsummary resultcs
class<? extends inputformat> inputformatcls   partdesc
getinputfileformatclass
inputformat inputformatobj   hiveinputformat getinputformatfromcache
inputformatcls  myjobconf
if  inputformatobj instanceof contentsummaryinputformat
resultcs     contentsummaryinputformat  inputformatobj  getcontentsummary p
myjobconf
} else {
filesystem fs   p getfilesystem myconf
resultcs   fs getcontentsummary p
}
resultmap put pathstr  resultcs
catch  ioexception e
// we safely ignore this exception for summary data.
// we don't update the cache to protect it from polluting other
// usages. the worst case is that ioexception will always be
// retried for another getinputsummary(), which is fine as
// ioexception is not considered as a common case.
log info     pathstr
}
}
if  executor    null
r run
} else {
future<?> result   executor submit r
results add result
}
}
if  executor    null
for  future<?> result   results
boolean executordone   false
do
try
result get
executordone   true
catch  interruptedexception e
log info    e
thread currentthread   interrupt
break
catch  executionexception e
throw new ioexception e
}
while   executordone
}
executor shutdown
}
hiveinterruptutils checkinterrupted
for  map entry<string  contentsummary> entry   resultmap entryset
contentsummary cs   entry getvalue
summary    cs getlength
summary    cs getfilecount
summary    cs getdirectorycount
ctx addcs entry getkey    cs
log info     entry getkey         cs getlength
cs getfilecount         cs getdirectorycount
}
return new contentsummary summary  summary  summary
finally
hiveinterruptutils remove interrup
}
}
}
public static boolean isemptypath jobconf job  string dirpath  context ctx
throws exception
contentsummary cs   ctx getcs dirpath
if  cs    null
log info     dirpath       cs getlength
cs getfilecount         cs getdirectorycount
return  cs getlength      0    cs getfilecount      0    cs getdirectorycount   <  1
} else {
log info     dirpath
}
path p   new path dirpath
return isemptypath job  p
}
public static boolean isemptypath jobconf job  path dirpath  throws exception
filesystem inpfs   dirpath getfilesystem job
if  inpfs exists dirpath
filestatus fstats   inpfs liststatus dirpath
if  fstats length > 0
return false
}
}
return true
}
public static list<execdriver> getmrtasks list<task<? extends serializable>> tasks
list<execdriver> mrtasks   new arraylist<execdriver>
if  tasks    null
getmrtasks tasks  mrtasks
}
return mrtasks
}
private static void getmrtasks list<task<? extends serializable>> tasks  list<execdriver> mrtasks
for  task<? extends serializable> task   tasks
if  task instanceof execdriver     mrtasks contains  execdriver  task
mrtasks add  execdriver  task
}
if  task getdependenttasks      null
getmrtasks task getdependenttasks    mrtasks
}
}
}
public static boolean supportcombinefileinputformat
return shimloader gethadoopshims   getcombinefileinputformat      null
}
/**
* construct a list of full partition spec from dynamic partition context and the directory names
* corresponding to these dynamic partitions.
*/
public static list<linkedhashmap<string  string>> getfulldpspecs configuration conf
dynamicpartitionctx dpctx  throws hiveexception
try
path loadpath   new path dpctx getrootpath
filesystem fs   loadpath getfilesystem conf
int numdpcols   dpctx getnumdpcols
filestatus status   utilities getfilestatusrecurse loadpath  numdpcols  fs
if  status length    0
log warn
return null
}
// partial partition specification
map<string  string> partspec   dpctx getpartspec
// list of full partition specification
list<linkedhashmap<string  string>> fullpartspecs   new arraylist<linkedhashmap<string  string>>
// for each dynamically created dp directory, construct a full partition spec
// and load the partition based on that
for  int i   0  i < status length    i
// get the dynamically created directory
path partpath   status getpath
assert fs getfilestatus partpath  isdir         partpath
// generate a full partition specification
linkedhashmap<string  string> fullpartspec   new linkedhashmap<string  string> partspec
warehouse makespecfromname fullpartspec  partpath
fullpartspecs add fullpartspec
}
return fullpartspecs
catch  ioexception e
throw new hiveexception e
}
}
public static statspublisher getstatspublisher jobconf jc
string statsimplementationclass   hiveconf getvar jc  hiveconf confvars hivestatsdbclass
if  statsfactory setimplementation statsimplementationclass  jc
return statsfactory getstatspublisher
} else {
return null
}
}
public static void setcolumnnamelist jobconf jobconf  operator op
rowschema rowschema   op getschema
if  rowschema    null
return
}
stringbuilder columnnames   new stringbuilder
for  columninfo colinfo   rowschema getsignature
if  columnnames length   > 0
columnnames append
}
columnnames append colinfo getinternalname
}
string columnnamesstring   columnnames tostring
jobconf set serdeconstants list_columns  columnnamesstring
}
public static void setcolumntypelist jobconf jobconf  operator op
rowschema rowschema   op getschema
if  rowschema    null
return
}
stringbuilder columntypes   new stringbuilder
for  columninfo colinfo   rowschema getsignature
if  columntypes length   > 0
columntypes append
}
columntypes append colinfo gettype   gettypename
}
string columntypesstring   columntypes tostring
jobconf set serdeconstants list_column_types  columntypesstring
}
public static void validatepartspec table tbl  map<string  string> partspec
throws semanticexception
list<fieldschema> parts   tbl getpartitionkeys
set<string> partcols   new hashset<string> parts size
for  fieldschema col   parts
partcols add col getname
}
for  string col   partspec keyset
if   partcols contains col
throw new semanticexception errormsg nonexistpartcol getmsg col
}
}
}
public static string suffix
public static string generatepath string baseuri  string dumpfileprefix
byte tag  string bigbucketfilename
string path   new string baseuri   path separator       dumpfileprefix   tag
bigbucketfilename   suffix
return path
}
public static string generatefilename byte tag  string bigbucketfilename
string filename   new string     tag       bigbucketfilename   suffix
return filename
}
public static string generatetmpuri string baseuri  string id
string tmpfileuri   new string baseuri   path separator       id
return tmpfileuri
}
public static string generatetaruri string baseuri  string filename
string tmpfileuri   new string baseuri   path separator   filename
return tmpfileuri
}
public static string generatetaruri path baseuri  string filename
string tmpfileuri   new string baseuri   path separator   filename
return tmpfileuri
}
public static string generatetarfilename string name
string tmpfileuri   new string name
return tmpfileuri
}
public static string generatepath path baseuri  string filename
string path   new string baseuri   path separator   filename
return path
}
public static string now
calendar cal   calendar getinstance
simpledateformat sdf   new simpledateformat
return sdf format cal gettime
}
public static double showtime long time
double result    double  time    double  1000
return result
}
/**
* check if a function can be pushed down to jdo.
* now only {=, and, or} are supported.
* @param func a generic function.
* @return true if this function can be pushed down to jdo filter.
*/
private static boolean supportedjdofuncs genericudf func
return func instanceof genericudfopequal
func instanceof genericudfopand
func instanceof genericudfopor
}
/**
* check if the partition pruning expression can be pushed down to jdo filtering.
* the partition expression contains only partition columns.
* the criteria that an expression can be pushed down are that:
*  1) the expression only contains function specified in supportedjdofuncs().
*     now only {=, and, or} can be pushed down.
*  2) the partition column type and the constant type have to be string. this is
*     restriction by the current jdo filtering implementation.
* @param tab the table that contains the partition columns.
* @param expr the partition pruning expression
* @return null if the partition pruning expression can be pushed down to jdo filtering.
*/
public static string checkjdopushdown table tab  exprnodedesc expr
if  expr instanceof exprnodeconstantdesc
// jdo filter now only support string typed literal -- see filter.g and expressiontree.java
object value     exprnodeconstantdesc expr  getvalue
if  value instanceof string
return null
}
return     value
else if  expr instanceof exprnodecolumndesc
// jdo filter now only support string typed literal -- see filter.g and expressiontree.java
typeinfo type   expr gettypeinfo
if  type gettypename   equals serdeconstants string_type_name
string colname     exprnodecolumndesc expr  getcolumn
for  fieldschema fs  tab getpartcols
if  fs getname   equals colname
if  fs gettype   equals serdeconstants string_type_name
return null
}
return     fs getname
}
}
assert false      cannot find the partition column
} else {
return     expr getexprstring
}
else if  expr instanceof exprnodegenericfuncdesc
exprnodegenericfuncdesc funcdesc    exprnodegenericfuncdesc  expr
genericudf func   funcdesc getgenericudf
if   supportedjdofuncs func
return     expr getexprstring
}
list<exprnodedesc> children   funcdesc getchildexprs
for  exprnodedesc child  children
string message   checkjdopushdown tab  child
if  message    null
return message
}
}
return null
}
return     expr getexprstring
}
/**
* the check here is kind of not clean. it first use a for loop to go through
* all input formats, and choose the ones that extend reworkmapredinputformat
* to a set. and finally go through the reworkmapredinputformat set, and call
* rework for each one.
*
* technically all these can be avoided if all hive's input formats can share
* a same interface. as in today's hive and hadoop, it is not possible because
* a lot of hive's input formats are in hadoop's code. and most of hadoop's
* input formats just extend inputformat interface.
*
* @param task
* @param reworkmapredwork
* @param conf
* @throws semanticexception
*/
public static void reworkmapredwork task<? extends serializable> task
boolean reworkmapredwork  hiveconf conf  throws semanticexception
if  reworkmapredwork     task instanceof mapredtask
try
mapredwork mapredwork     mapredtask  task  getwork
set<class<? extends inputformat>> reworkinputformats   new hashset<class<? extends inputformat>>
for  partitiondesc part   mapredwork getpathtopartitioninfo   values
class<? extends inputformat> inputformatcls   part
getinputfileformatclass
if  reworkmapredinputformat class isassignablefrom inputformatcls
reworkinputformats add inputformatcls
}
}
if  reworkinputformats size   > 0
for  class<? extends inputformat> inputformatcls   reworkinputformats
reworkmapredinputformat inst    reworkmapredinputformat  reflectionutils
newinstance inputformatcls  null
inst rework conf  mapredwork
}
}
catch  ioexception e
throw new semanticexception e
}
}
}
public static class sqlcommand<t>
public t run preparedstatement stmt  throws sqlexception
return null
}
}
/**
* retry sql execution with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param cmd the sql command
* @param stmt the prepared statement of sql.
* @param basewindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static <t> t executewithretry sqlcommand<t> cmd  preparedstatement stmt
int basewindow  int maxretries   throws sqlexception
random r   new random
t result   null
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
result   cmd run stmt
return result
catch  sqltransientexception e
log warn     failures        e getmessage
if  failures >  maxretries
throw e
}
long waittime   getrandomwaittime basewindow  failures  r
try
thread sleep waittime
catch  interruptedexception iex
}
catch  sqlexception e
// throw other types of sqlexceptions (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* retry connecting to a database with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param connectionstring the jdbc connection string.
* @param waitwindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static connection connectwithretry string connectionstring
int waitwindow  int maxretries  throws sqlexception
random r   new random
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
connection conn   drivermanager getconnection connectionstring
return conn
catch  sqltransientexception e
if  failures >  maxretries
log error     e
throw e
}
long waittime   utilities getrandomwaittime waitwindow  failures  r
try
thread sleep waittime
catch  interruptedexception e1
}
catch  sqlexception e
// just throw other types (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* retry preparing a sql statement with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param conn a jdbc connection.
* @param stmt the sql statement to be prepared.
* @param waitwindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static preparedstatement preparewithretry connection conn  string stmt
int waitwindow  int maxretries  throws sqlexception
random r   new random
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
return conn preparestatement stmt
catch  sqltransientexception e
if  failures >  maxretries
log error     stmt       e
throw e
}
long waittime   utilities getrandomwaittime waitwindow  failures  r
try
thread sleep waittime
catch  interruptedexception e1
}
catch  sqlexception e
// just throw other types (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* introducing a random factor to the wait time before another retry.
* the wait time is dependent on # of failures and a random factor.
* at the first time of getting an exception , the wait time
* is a random number between 0..basewindow msec. if the first retry
* still fails, we will wait basewindow msec grace period before the 2nd retry.
* also at the second retry, the waiting window is expanded to 2*basewindow msec
* alleviating the request rate from the server. similarly the 3rd retry
* will wait 2*basewindow msec. grace period before retry and the waiting window is
* expanded to 3*basewindow msec and so on.
* @param basewindow the base waiting window.
* @param failures number of failures so far.
* @param r a random generator.
* @return number of milliseconds for the next wait time.
*/
public static long getrandomwaittime int basewindow  int failures  random r
return  long
basewindow   failures          grace period for the last round of attempt
basewindow    failures   1    r nextdouble        expanding time window for each failure
}
public static final char sqlescapechar
/**
* escape the '_', '%', as well as the escape characters inside the string key.
* @param key the string that will be used for the sql like operator.
* @return a string with escaped '_' and '%'.
*/
public static string escapesqllike string key
stringbuffer sb   new stringbuffer key length
for  char c  key tochararray
switch c
case
case
case sqlescapechar
sb append sqlescapechar
// fall through
default
sb append c
break
}
}
return sb tostring
}
/**
* format number of milliseconds to strings
*
* @param msec milliseconds
* @return a formatted string like "x days y hours z minutes a seconds b msec"
*/
public static string formatmsectostr long msec
long day    1  hour    1  minute    1  second    1
long ms   msec % 1000
long timeleft   msec   1000
if  timeleft > 0
second   timeleft % 60
timeleft    60
if  timeleft > 0
minute   timeleft % 60
timeleft    60
if  timeleft > 0
hour   timeleft % 24
day   timeleft   24
}
}
}
stringbuilder sb   new stringbuilder
if  day     1
sb append day
}
if  hour     1
sb append hour
}
if  minute     1
sb append minute
}
if  second     1
sb append second
}
sb append ms
return sb tostring
}
public static class getbuiltinutilsclass   throws classnotfoundexception
return class forname
}
}