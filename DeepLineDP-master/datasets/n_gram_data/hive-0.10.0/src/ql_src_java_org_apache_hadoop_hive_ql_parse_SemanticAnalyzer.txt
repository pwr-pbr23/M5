/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql parse
import java io ioexception
import java io serializable
import java util arraylist
import java util hashmap
import java util hashset
import java util iterator
import java util linkedhashmap
import java util list
import java util map
import java util map entry
import java util set
import java util treeset
import java util regex pattern
import java util regex patternsyntaxexception
import org antlr runtime tree tree
import org apache commons lang stringutils
import org apache hadoop fs contentsummary
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop hive common fileutils
import org apache hadoop hive common javautils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive conf hiveconf confvars
import org apache hadoop hive metastore tabletype
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive metastore api order
import org apache hadoop hive ql context
import org apache hadoop hive ql errormsg
import org apache hadoop hive ql queryproperties
import org apache hadoop hive ql exec abstractmapjoinoperator
import org apache hadoop hive ql exec archiveutils
import org apache hadoop hive ql exec columninfo
import org apache hadoop hive ql exec columnstatstask
import org apache hadoop hive ql exec conditionaltask
import org apache hadoop hive ql exec execdriver
import org apache hadoop hive ql exec fetchtask
import org apache hadoop hive ql exec filesinkoperator
import org apache hadoop hive ql exec functioninfo
import org apache hadoop hive ql exec functionregistry
import org apache hadoop hive ql exec groupbyoperator
import org apache hadoop hive ql exec joinoperator
import org apache hadoop hive ql exec mapjoinoperator
import org apache hadoop hive ql exec mapredtask
import org apache hadoop hive ql exec operator
import org apache hadoop hive ql exec operatorfactory
import org apache hadoop hive ql exec recordreader
import org apache hadoop hive ql exec recordwriter
import org apache hadoop hive ql exec reducesinkoperator
import org apache hadoop hive ql exec rowschema
import org apache hadoop hive ql exec selectoperator
import org apache hadoop hive ql exec statstask
import org apache hadoop hive ql exec tablescanoperator
import org apache hadoop hive ql exec task
import org apache hadoop hive ql exec taskfactory
import org apache hadoop hive ql exec udfargumentexception
import org apache hadoop hive ql exec unionoperator
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql hooks readentity
import org apache hadoop hive ql hooks writeentity
import org apache hadoop hive ql io combinehiveinputformat
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive ql lib defaultgraphwalker
import org apache hadoop hive ql lib defaultruledispatcher
import org apache hadoop hive ql lib dispatcher
import org apache hadoop hive ql lib graphwalker
import org apache hadoop hive ql lib node
import org apache hadoop hive ql lib nodeprocessor
import org apache hadoop hive ql lib rule
import org apache hadoop hive ql lib ruleregexp
import org apache hadoop hive ql metadata dummypartition
import org apache hadoop hive ql metadata hive
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata hiveutils
import org apache hadoop hive ql metadata invalidtableexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql metadata virtualcolumn
import org apache hadoop hive ql optimizer genmrfilesink1
import org apache hadoop hive ql optimizer genmroperator
import org apache hadoop hive ql optimizer genmrproccontext
import org apache hadoop hive ql optimizer genmrproccontext genmapredctx
import org apache hadoop hive ql optimizer genmrredsink1
import org apache hadoop hive ql optimizer genmrredsink2
import org apache hadoop hive ql optimizer genmrredsink3
import org apache hadoop hive ql optimizer genmrredsink4
import org apache hadoop hive ql optimizer genmrtablescan1
import org apache hadoop hive ql optimizer genmrunion1
import org apache hadoop hive ql optimizer genmapredutils
import org apache hadoop hive ql optimizer mapjoinfactory
import org apache hadoop hive ql optimizer optimizer
import org apache hadoop hive ql optimizer physical physicalcontext
import org apache hadoop hive ql optimizer physical physicaloptimizer
import org apache hadoop hive ql optimizer unionproc unionproccontext
import org apache hadoop hive ql parse basesemanticanalyzer tablespec spectype
import org apache hadoop hive ql plan aggregationdesc
import org apache hadoop hive ql plan columnstatsdesc
import org apache hadoop hive ql plan columnstatswork
import org apache hadoop hive ql plan createtabledesc
import org apache hadoop hive ql plan createtablelikedesc
import org apache hadoop hive ql plan createviewdesc
import org apache hadoop hive ql plan ddlwork
import org apache hadoop hive ql plan dynamicpartitionctx
import org apache hadoop hive ql plan exprnodecolumndesc
import org apache hadoop hive ql plan exprnodeconstantdesc
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan exprnodegenericfuncdesc
import org apache hadoop hive ql plan exprnodenulldesc
import org apache hadoop hive ql plan extractdesc
import org apache hadoop hive ql plan fetchwork
import org apache hadoop hive ql plan filesinkdesc
import org apache hadoop hive ql plan filterdesc
import org apache hadoop hive ql plan filterdesc sampledesc
import org apache hadoop hive ql plan forwarddesc
import org apache hadoop hive ql plan groupbydesc
import org apache hadoop hive ql plan hiveoperation
import org apache hadoop hive ql plan joinconddesc
import org apache hadoop hive ql plan joindesc
import org apache hadoop hive ql plan lateralviewforwarddesc
import org apache hadoop hive ql plan lateralviewjoindesc
import org apache hadoop hive ql plan limitdesc
import org apache hadoop hive ql plan loadfiledesc
import org apache hadoop hive ql plan loadtabledesc
import org apache hadoop hive ql plan mapjoindesc
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan movework
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan planutils
import org apache hadoop hive ql plan reducesinkdesc
import org apache hadoop hive ql plan scriptdesc
import org apache hadoop hive ql plan selectdesc
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive ql plan udtfdesc
import org apache hadoop hive ql plan uniondesc
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive ql session sessionstate resourcetype
import org apache hadoop hive ql udf generic genericudafevaluator
import org apache hadoop hive ql udf generic genericudafevaluator mode
import org apache hadoop hive ql udf generic genericudfhash
import org apache hadoop hive ql udf generic genericudfopor
import org apache hadoop hive ql udf generic genericudtf
import org apache hadoop hive ql util objectpair
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 metadatatypedcolumnsetserde
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive serde2 objectinspector constantobjectinspector
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspector category
import org apache hadoop hive serde2 objectinspector structfield
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive serde2 typeinfo typeinfofactory
import org apache hadoop hive serde2 typeinfo typeinfoutils
import org apache hadoop hive shims shimloader
import org apache hadoop mapred inputformat
/**
* implementation of the semantic analyzer.
*/
public class semanticanalyzer extends basesemanticanalyzer
private hashmap<tablescanoperator  exprnodedesc> optopartpruner
private hashmap<tablescanoperator  prunedpartitionlist> optopartlist
private hashmap<string  operator<? extends operatordesc>> topops
private hashmap<string  operator<? extends operatordesc>> topselops
private linkedhashmap<operator<? extends operatordesc>  opparsecontext> opparsectx
private list<loadtabledesc> loadtablework
private list<loadfiledesc> loadfilework
private map<joinoperator  qbjointree> joincontext
private final hashmap<tablescanoperator  table> toptotable
private qb qb
private astnode ast
private int desttableid
private unionproccontext uctx
list<abstractmapjoinoperator<? extends mapjoindesc>> listmapjoinopsnoreducer
private hashmap<tablescanoperator  sampledesc> optosamplepruner
private final map<tablescanoperator  map<string  exprnodedesc>> optoparttoskewedpruner
/**
* a map for the split sampling, from ailias to an instance of splitsample
* that describes percentage and number.
*/
private final hashmap<string  splitsample> nametosplitsample
map<groupbyoperator  set<string>> groupoptoinputtables
map<string  prunedpartitionlist> prunedpartitions
private list<fieldschema> resultschema
private createviewdesc createvwdesc
private arraylist<string> viewsexpanded
private astnode viewselect
private final unparsetranslator unparsetranslator
private final globallimitctx globallimitctx   new globallimitctx
//prefix for column names auto generated by hive
private final string autogencolaliasprfxlbl
private final boolean autogencolaliasprfxincludefuncname
//max characters when auto generating the column name with func name
private static final int autogen_colalias_prfx_maxlength   20
private static class phase1ctx
string dest
int nextnum
public semanticanalyzer hiveconf conf  throws semanticexception
super conf
optopartpruner   new hashmap<tablescanoperator  exprnodedesc>
optopartlist   new hashmap<tablescanoperator  prunedpartitionlist>
optosamplepruner   new hashmap<tablescanoperator  sampledesc>
nametosplitsample   new hashmap<string  splitsample>
topops   new hashmap<string  operator<? extends operatordesc>>
topselops   new hashmap<string  operator<? extends operatordesc>>
loadtablework   new arraylist<loadtabledesc>
loadfilework   new arraylist<loadfiledesc>
opparsectx   new linkedhashmap<operator<? extends operatordesc>  opparsecontext>
joincontext   new hashmap<joinoperator  qbjointree>
toptotable   new hashmap<tablescanoperator  table>
desttableid   1
uctx   null
listmapjoinopsnoreducer   new arraylist<abstractmapjoinoperator<? extends mapjoindesc>>
groupoptoinputtables   new hashmap<groupbyoperator  set<string>>
prunedpartitions   new hashmap<string  prunedpartitionlist>
unparsetranslator   new unparsetranslator
autogencolaliasprfxlbl   hiveconf getvar conf
hiveconf confvars hive_autogen_columnalias_prefix_label
autogencolaliasprfxincludefuncname   hiveconf getboolvar conf
hiveconf confvars hive_autogen_columnalias_prefix_includefuncname
queryproperties   new queryproperties
optoparttoskewedpruner   new hashmap<tablescanoperator  map<string  exprnodedesc>>
@override
protected void reset
super reset
loadtablework clear
loadfilework clear
topops clear
topselops clear
desttableid   1
idtotablenamemap clear
qb   null
ast   null
uctx   null
joincontext clear
opparsectx clear
groupoptoinputtables clear
prunedpartitions clear
public void initparsectx parsecontext pctx
optopartpruner   pctx getoptopartpruner
optopartlist   pctx getoptopartlist
optosamplepruner   pctx getoptosamplepruner
topops   pctx gettopops
topselops   pctx gettopselops
opparsectx   pctx getopparsectx
loadtablework   pctx getloadtablework
loadfilework   pctx getloadfilework
joincontext   pctx getjoincontext
ctx   pctx getcontext
desttableid   pctx getdesttableid
idtotablenamemap   pctx getidtotablenamemap
uctx   pctx getuctx
listmapjoinopsnoreducer   pctx getlistmapjoinopsnoreducer
qb   pctx getqb
groupoptoinputtables   pctx getgroupoptoinputtables
prunedpartitions   pctx getprunedpartitions
fetchtask   pctx getfetchtask
setlineageinfo pctx getlineageinfo
public parsecontext getparsecontext
return new parsecontext conf  qb  ast  optopartpruner  optopartlist  topops
topselops  opparsectx  joincontext  toptotable  loadtablework
loadfilework  ctx  idtotablenamemap  desttableid  uctx
listmapjoinopsnoreducer  groupoptoinputtables  prunedpartitions
optosamplepruner  globallimitctx  nametosplitsample  inputs  roottasks
optoparttoskewedpruner
@suppresswarnings
public void dophase1qbexpr astnode ast  qbexpr qbexpr  string id  string alias
throws semanticexception
assert  ast gettoken      null
switch  ast gettoken   gettype
case hiveparser tok_query
qb qb   new qb id  alias  true
dophase1 ast  qb  initphase1ctx
qbexpr setopcode qbexpr opcode nullop
qbexpr setqb qb
break
case hiveparser tok_union
qbexpr setopcode qbexpr opcode union
// query 1
assert  ast getchild 0     null
qbexpr qbexpr1   new qbexpr alias
dophase1qbexpr  astnode  ast getchild 0   qbexpr1  id
alias
qbexpr setqbexpr1 qbexpr1
// query 2
assert  ast getchild 0     null
qbexpr qbexpr2   new qbexpr alias
dophase1qbexpr  astnode  ast getchild 1   qbexpr2  id
alias
qbexpr setqbexpr2 qbexpr2
break
private linkedhashmap<string  astnode> dophase1getaggregationsfromselect
astnode selexpr
// iterate over the selects search for aggregation trees.
// use string as keys to eliminate duplicate trees.
linkedhashmap<string  astnode> aggregationtrees   new linkedhashmap<string  astnode>
for  int i   0  i < selexpr getchildcount      i
astnode sel    astnode  selexpr getchild i  getchild 0
dophase1getallaggregations sel  aggregationtrees
return aggregationtrees
private void dophase1getcolumnaliasesfromselect
astnode selectexpr  qbparseinfo qbp
for  int i   0  i < selectexpr getchildcount      i
astnode selexpr    astnode  selectexpr getchild i
if   selexpr gettoken   gettype      hiveparser tok_selexpr      selexpr getchildcount      2
string columnalias   unescapeidentifier selexpr getchild 1  gettext
qbp setexprtocolumnalias  astnode  selexpr getchild 0   columnalias
/**
* dfs-scan the expressiontree to find all aggregation subtrees and put them
* in aggregations.
*
* @param expressiontree
* @param aggregations
*          the key to the hashtable is the tostringtree() representation of
*          the aggregation subtree.
*/
private void dophase1getallaggregations astnode expressiontree
hashmap<string  astnode> aggregations
int exprtokentype   expressiontree gettoken   gettype
if  exprtokentype    hiveparser tok_function
exprtokentype    hiveparser tok_functiondi
exprtokentype    hiveparser tok_functionstar
assert  expressiontree getchildcount      0
if  expressiontree getchild 0  gettype      hiveparser identifier
string functionname   unescapeidentifier expressiontree getchild 0
gettext
if  functionregistry getgenericudafresolver functionname     null
aggregations put expressiontree tostringtree    expressiontree
functioninfo fi   functionregistry getfunctioninfo functionname
if   fi isnative
unparsetranslator addidentifiertranslation  astnode  expressiontree
getchild 0
return
for  int i   0  i < expressiontree getchildcount    i
dophase1getallaggregations  astnode  expressiontree getchild i
aggregations
private list<astnode> dophase1getdistinctfuncexprs
hashmap<string  astnode> aggregationtrees  throws semanticexception
list<astnode> exprs   new arraylist<astnode>
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
assert  value    null
if  value gettoken   gettype      hiveparser tok_functiondi
exprs add value
return exprs
public static string generateerrormessage astnode ast  string message
stringbuilder sb   new stringbuilder
sb append ast getline
sb append
sb append ast getcharpositioninline
sb append
sb append message
sb append
sb append errormsg gettext ast
sb append
return sb tostring
/**
* goes though the tabref tree and finds the alias for the table. once found,
* it records the table name-> alias association in aliastotabs. it also makes
* an association from the alias to the table ast in parse info.
*
* @return the alias of the table
*/
private string processtable qb qb  astnode tabref  throws semanticexception
// for each table reference get the table name
// and the alias (if alias is not present, the table name
// is used as an alias)
boolean tablesamplepresent   false
boolean splitsamplepresent   false
int aliasindex   0
if  tabref getchildcount      2
// tablename tablesample
// or
// tablename alias
astnode ct    astnode  tabref getchild 1
if  ct gettoken   gettype      hiveparser tok_tablebucketsample
tablesamplepresent   true
else if  ct gettoken   gettype      hiveparser tok_tablesplitsample
splitsamplepresent   true
else
aliasindex   1
else if  tabref getchildcount      3
// table name table sample alias
aliasindex   2
astnode ct    astnode  tabref getchild 1
if  ct gettoken   gettype      hiveparser tok_tablebucketsample
tablesamplepresent   true
else if  ct gettoken   gettype      hiveparser tok_tablesplitsample
splitsamplepresent   true
astnode tabletree    astnode   tabref getchild 0
string tabidname   getunescapedname tabletree
string alias
if  aliasindex    0
alias   unescapeidentifier tabref getchild aliasindex  gettext
else
alias   getunescapedunqualifiedtablename tabletree
// if the alias is already there then we have a conflict
if  qb exists alias
throw new semanticexception errormsg ambiguous_table_alias getmsg tabref
getchild aliasindex
if  tablesamplepresent
astnode sampleclause    astnode  tabref getchild 1
arraylist<astnode> samplecols   new arraylist<astnode>
if  sampleclause getchildcount   > 2
for  int i   2  i < sampleclause getchildcount    i
samplecols add  astnode  sampleclause getchild i
// todo: for now only support sampling on up to two columns
// need to change it to list of columns
if  samplecols size   > 2
throw new semanticexception generateerrormessage
astnode  tabref getchild 0
errormsg sample_restriction getmsg
qb getparseinfo   settabsample
alias
new tablesample
unescapeidentifier sampleclause getchild 0  gettext
unescapeidentifier sampleclause getchild 1  gettext
samplecols
if  unparsetranslator isenabled
for  astnode samplecol   samplecols
unparsetranslator addidentifiertranslation  astnode  samplecol
getchild 0
else if  splitsamplepresent
// only combinehiveinputformat supports this optimize
string inputformat   hiveconf getvar conf  hiveconf confvars hiveinputformat
if   inputformat equals
combinehiveinputformat class getname
throw new semanticexception generateerrormessage  astnode  tabref getchild 1
inputformat
astnode sampleclause    astnode  tabref getchild 1
string alias_id   getaliasid alias  qb
string strpercentage   unescapeidentifier sampleclause getchild 0  gettext
double percent   double valueof strpercentage  doublevalue
if  percent < 0     percent > 100
throw new semanticexception generateerrormessage sampleclause
nametosplitsample put alias_id  new splitsample
percent  conf getintvar confvars hivesamplerandomnum
// insert this map into the stats
qb settabalias alias  tabidname
qb addalias alias
qb getparseinfo   setsrcforalias alias  tabletree
unparsetranslator addtablenametranslation tabletree  db getcurrentdatabase
if  aliasindex    0
unparsetranslator addidentifiertranslation  astnode  tabref
getchild aliasindex
return alias
private string processsubquery qb qb  astnode subq  throws semanticexception
// this is a subquery and must have an alias
if  subq getchildcount      2
throw new semanticexception errormsg no_subquery_alias getmsg subq
astnode subqref    astnode  subq getchild 0
string alias   unescapeidentifier subq getchild 1  gettext
// recursively do the first phase of semantic analysis for the subquery
qbexpr qbexpr   new qbexpr alias
dophase1qbexpr subqref  qbexpr  qb getid    alias
// if the alias is already there then we have a conflict
if  qb exists alias
throw new semanticexception errormsg ambiguous_table_alias getmsg subq
getchild 1
// insert this map into the stats
qb setsubqalias alias  qbexpr
qb addalias alias
unparsetranslator addidentifiertranslation  astnode  subq getchild 1
return alias
private boolean isjointoken astnode node
if   node gettoken   gettype      hiveparser tok_join
node gettoken   gettype      hiveparser tok_crossjoin
node gettoken   gettype      hiveparser tok_leftouterjoin
node gettoken   gettype      hiveparser tok_rightouterjoin
node gettoken   gettype      hiveparser tok_fullouterjoin
node gettoken   gettype      hiveparser tok_leftsemijoin
node gettoken   gettype      hiveparser tok_uniquejoin
return true
return false
/**
* given the ast with tok_join as the root, get all the aliases for the tables
* or subqueries in the join.
*
* @param qb
* @param join
* @throws semanticexception
*/
@suppresswarnings
private void processjoin qb qb  astnode join  throws semanticexception
int numchildren   join getchildcount
if   numchildren    2      numchildren    3
join gettoken   gettype      hiveparser tok_uniquejoin
throw new semanticexception generateerrormessage join
for  int num   0  num < numchildren  num
astnode child    astnode  join getchild num
if  child gettoken   gettype      hiveparser tok_tabref
processtable qb  child
else if  child gettoken   gettype      hiveparser tok_subquery
processsubquery qb  child
else if  child gettoken   gettype      hiveparser tok_lateral_view
// select * from src1 lateral view udtf() as mytable join src2 ...
// is not supported. instead, the lateral view must be in a subquery
// select * from (select * from src1 lateral view udtf() as mytable) a
// join src2 ...
throw new semanticexception errormsg lateral_view_with_join
getmsg join
else if  isjointoken child
processjoin qb  child
/**
* given the ast with tok_lateral_view as the root, get the alias for the
* table or subquery in the lateral view and also make a mapping from the
* alias to all the lateral view ast's.
*
* @param qb
* @param lateralview
* @return the alias for the table/subquery
* @throws semanticexception
*/
private string processlateralview qb qb  astnode lateralview
throws semanticexception
int numchildren   lateralview getchildcount
assert  numchildren    2
astnode next    astnode  lateralview getchild 1
string alias   null
switch  next gettoken   gettype
case hiveparser tok_tabref
alias   processtable qb  next
break
case hiveparser tok_subquery
alias   processsubquery qb  next
break
case hiveparser tok_lateral_view
alias   processlateralview qb  next
break
default
throw new semanticexception errormsg lateral_view_invalid_child
getmsg lateralview
alias   alias tolowercase
qb getparseinfo   addlateralviewforalias alias  lateralview
qb addalias alias
return alias
/**
* phase 1: (including, but not limited to):
*
* 1. gets all the aliases for all the tables / subqueries and makes the
* appropriate mapping in aliastotabs, aliastosubq 2. gets the location of the
* destination and names the clase "inclause" + i 3. creates a map from a
* string representation of an aggregation tree to the actual aggregation ast
* 4. creates a mapping from the clause name to the select expression ast in
* desttoselexpr 5. creates a mapping from a table alias to the lateral view
* ast's in aliastolateralviews
*
* @param ast
* @param qb
* @param ctx_1
* @throws semanticexception
*/
@suppresswarnings
public boolean dophase1 astnode ast  qb qb  phase1ctx ctx_1
throws semanticexception
boolean phase1result   true
qbparseinfo qbp   qb getparseinfo
boolean skiprecursion   false
if  ast gettoken      null
skiprecursion   true
switch  ast gettoken   gettype
case hiveparser tok_selectdi
qb countseldi
// fall through
case hiveparser tok_select
qb countsel
qbp setselexprforclause ctx_1 dest  ast
if    astnode  ast getchild 0   gettoken   gettype      hiveparser tok_hintlist
qbp sethints  astnode  ast getchild 0
linkedhashmap<string  astnode> aggregations   dophase1getaggregationsfromselect ast
dophase1getcolumnaliasesfromselect ast  qbp
qbp setaggregationexprsforclause ctx_1 dest  aggregations
qbp setdistinctfuncexprsforclause ctx_1 dest
dophase1getdistinctfuncexprs aggregations
break
case hiveparser tok_where
qbp setwhrexprforclause ctx_1 dest  ast
break
case hiveparser tok_insert_into
string currentdatabase   db getcurrentdatabase
string tab_name   getunescapedname  astnode ast getchild 0  getchild 0   currentdatabase
qbp addinsertintotable tab_name
case hiveparser tok_destination
ctx_1 dest       ctx_1 nextnum
ctx_1 nextnum
// is there a insert in the subquery
if  qbp getissubq
astnode ch    astnode  ast getchild 0
if   ch gettoken   gettype      hiveparser tok_dir
astnode  ch getchild 0   gettoken   gettype      hiveparser tok_tmp_file
throw new semanticexception errormsg no_insert_insubquery
getmsg ast
qbp setdestforclause ctx_1 dest   astnode  ast getchild 0
break
case hiveparser tok_from
int child_count   ast getchildcount
if  child_count    1
throw new semanticexception generateerrormessage ast
child_count
// check if this is a subquery / lateral view
astnode frm    astnode  ast getchild 0
if  frm gettoken   gettype      hiveparser tok_tabref
processtable qb  frm
else if  frm gettoken   gettype      hiveparser tok_subquery
processsubquery qb  frm
else if  frm gettoken   gettype      hiveparser tok_lateral_view
processlateralview qb  frm
else if  isjointoken frm
queryproperties sethasjoin true
processjoin qb  frm
qbp setjoinexpr frm
break
case hiveparser tok_clusterby
// get the clusterby aliases - these are aliased to the entries in the
// select list
queryproperties sethasclusterby true
qbp setclusterbyexprforclause ctx_1 dest  ast
break
case hiveparser tok_distributeby
// get the distribute by aliases - these are aliased to the entries in
// the
// select list
queryproperties sethasdistributeby true
qbp setdistributebyexprforclause ctx_1 dest  ast
if  qbp getclusterbyforclause ctx_1 dest     null
throw new semanticexception generateerrormessage ast
errormsg clusterby_distributeby_conflict getmsg
else if  qbp getorderbyforclause ctx_1 dest     null
throw new semanticexception generateerrormessage ast
errormsg orderby_distributeby_conflict getmsg
break
case hiveparser tok_sortby
// get the sort by aliases - these are aliased to the entries in the
// select list
queryproperties sethassortby true
qbp setsortbyexprforclause ctx_1 dest  ast
if  qbp getclusterbyforclause ctx_1 dest     null
throw new semanticexception generateerrormessage ast
errormsg clusterby_sortby_conflict getmsg
else if  qbp getorderbyforclause ctx_1 dest     null
throw new semanticexception generateerrormessage ast
errormsg orderby_sortby_conflict getmsg
break
case hiveparser tok_orderby
// get the order by aliases - these are aliased to the entries in the
// select list
queryproperties sethasorderby true
qbp setorderbyexprforclause ctx_1 dest  ast
if  qbp getclusterbyforclause ctx_1 dest     null
throw new semanticexception generateerrormessage ast
errormsg clusterby_orderby_conflict getmsg
break
case hiveparser tok_groupby
case hiveparser tok_rollup_groupby
case hiveparser tok_cube_groupby
case hiveparser tok_grouping_sets
// get the groupby aliases - these are aliased to the entries in the
// select list
queryproperties sethasgroupby true
if  qbp getjoinexpr      null
queryproperties sethasjoinfollowedbygroupby true
if  qbp getselforclause ctx_1 dest  gettoken   gettype      hiveparser tok_selectdi
throw new semanticexception generateerrormessage ast
errormsg select_distinct_with_groupby getmsg
qbp setgroupbyexprforclause ctx_1 dest  ast
skiprecursion   true
// rollup and cubes are syntactic sugar on top of grouping sets
if  ast gettoken   gettype      hiveparser tok_rollup_groupby
qbp getdestrollups   add ctx_1 dest
else if  ast gettoken   gettype      hiveparser tok_cube_groupby
qbp getdestcubes   add ctx_1 dest
else if  ast gettoken   gettype      hiveparser tok_grouping_sets
qbp getdestgroupingsets   add ctx_1 dest
break
case hiveparser tok_having
qbp sethavingexprforclause ctx_1 dest  ast
qbp addaggregationexprsforclause ctx_1 dest  dophase1getaggregationsfromselect ast
break
case hiveparser tok_limit
qbp setdestlimit ctx_1 dest  new integer ast getchild 0  gettext
break
case hiveparser tok_analyze
// case of analyze command
string table_name   getunescapedname  astnode ast getchild 0  getchild 0
qb settabalias table_name  table_name
qb addalias table_name
qb getparseinfo   setisanalyzecommand true
// allow analyze the whole table and dynamic partitions
hiveconf setvar conf  hiveconf confvars dynamicpartitioningmode
hiveconf setvar conf  hiveconf confvars hivemapredmode
break
case hiveparser tok_union
// currently, we dont support subq1 union subq2 - the user has to
// explicitly say:
// select * from (subq1 union subq2) subqalias
if   qbp getissubq
throw new semanticexception generateerrormessage ast
errormsg union_notin_subq getmsg
case hiveparser tok_insert
astnode destination    astnode  ast getchild 0
tree tab   destination getchild 0
// proceed if ast contains partition & if not exists
if  destination getchildcount      2
tab getchildcount      2
destination getchild 1  gettype      hiveparser tok_ifnotexists
string tablename   tab getchild 0  getchild 0  gettext
tree partitions   tab getchild 1
int childcount   partitions getchildcount
hashmap<string  string> partition   new hashmap<string  string>
for  int i   0  i < childcount  i
string partitionname   partitions getchild i  getchild 0  gettext
tree pvalue   partitions getchild i  getchild 1
if  pvalue    null
break
string partitionval   stripquotes pvalue gettext
partition put partitionname  partitionval
// if it is a dynamic partition throw the exception
if  childcount    partition size
try
table table   db gettable tablename
partition parmetadata   db getpartition table  partition  false
// check partition exists if it exists skip the overwrite
if  parmetadata    null
phase1result   false
skiprecursion   true
log info
parmetadata tostring
break
catch  hiveexception e
log info    e
else
throw new semanticexception errormsg insert_into_dynamicpartition_ifnotexists
getmsg partition tostring
default
skiprecursion   false
break
if   skiprecursion
// iterate over the rest of the children
int child_count   ast getchildcount
for  int child_pos   0  child_pos < child_count    phase1result    child_pos
// recurse
phase1result   phase1result    dophase1  astnode  ast getchild child_pos   qb  ctx_1
return phase1result
private void getmetadata qbexpr qbexpr  throws semanticexception
if  qbexpr getopcode      qbexpr opcode nullop
getmetadata qbexpr getqb
else
getmetadata qbexpr getqbexpr1
getmetadata qbexpr getqbexpr2
@suppresswarnings
public void getmetadata qb qb  throws semanticexception
try
log info
// go over the tables and populate the related structures.
// we have to materialize the table alias list since we might
// modify it in the middle for view rewrite.
list<string> tabaliases   new arraylist<string> qb gettabaliases
map<string  string> aliastoviewname   new hashmap<string  string>
for  string alias   tabaliases
string tab_name   qb gettabnameforalias alias
table tab   null
try
tab   db gettable tab_name
catch  invalidtableexception ite
throw new semanticexception errormsg invalid_table getmsg qb
getparseinfo   getsrcforalias alias
// disallow insert into on bucketized tables
if qb getparseinfo   isinsertintotable tab getdbname    tab gettablename
tab getnumbuckets   > 0
throw new semanticexception errormsg insert_into_bucketized_table
getmsg     tab_name
// we check offline of the table, as if people only select from an
// non-existing partition of an offline table, the partition won't
// be added to inputs and validate() won't have the information to
// check the table's offline status.
// todo: modify the code to remove the checking here and consolidate
// it in validate()
//
if  tab isoffline
throw new semanticexception errormsg offline_table_or_partition
getmsg     getunescapedname qb getparseinfo   getsrcforalias alias
if  tab isview
if  qb getparseinfo   isanalyzecommand
throw new semanticexception errormsg analyze_view getmsg
string fullviewname   tab getdbname     tab gettablename
// prevent view cycles
if viewsexpanded contains fullviewname
throw new semanticexception     fullviewname
stringutils join viewsexpanded
fullviewname
replaceviewreferencewithdefinition qb  tab  tab_name  alias
aliastoviewname put alias  fullviewname
// this is the last time we'll see the table objects for views, so add it to the inputs
// now
inputs add new readentity tab
continue
if   inputformat class isassignablefrom tab getinputformatclass
throw new semanticexception generateerrormessage
qb getparseinfo   getsrcforalias alias
errormsg invalid_input_format_type getmsg
qb getmetadata   setsrcforalias alias  tab
if  qb getparseinfo   isanalyzecommand
tablespec ts   new tablespec db  conf   astnode  ast getchild 0
if  ts spectype    spectype dynamic_partition       dynamic partitions
try
ts partitions   db getpartitionsbynames ts tablehandle  ts partspec
catch  hiveexception e
throw new semanticexception generateerrormessage qb getparseinfo   getsrcforalias alias
ts partspec   e
qb getparseinfo   addtablespec alias  ts
log info
// go over the subqueries and getmetadata for these
for  string alias   qb getsubqaliases
boolean wasview   aliastoviewname containskey alias
if  wasview
viewsexpanded add aliastoviewname get alias
qbexpr qbexpr   qb getsubqforalias alias
getmetadata qbexpr
if  wasview
viewsexpanded remove viewsexpanded size   1
log info
// go over all the destination structures and populate the related
// metadata
qbparseinfo qbp   qb getparseinfo
for  string name   qbp getclausenamesfordest
astnode ast   qbp getdestforclause name
switch  ast gettoken   gettype
case hiveparser tok_tab
tablespec ts   new tablespec db  conf  ast
if  ts tablehandle isview
throw new semanticexception errormsg dml_against_view getmsg
class<?> outputformatclass   ts tablehandle getoutputformatclass
if   hiveoutputformat class isassignablefrom outputformatclass
throw new semanticexception errormsg invalid_output_format_type
getmsg ast      outputformatclass tostring
// tablespec ts is got from the query (user specified),
// which means the user didn't specify partitions in their query,
// but whether the table itself is partitioned is not know.
if  ts spectype    spectype static_partition
// this is a table or dynamic partition
qb getmetadata   setdestforalias name  ts tablehandle
// has dynamic as well as static partitions
if  ts partspec    null    ts partspec size   > 0
qb getmetadata   setpartspecforalias name  ts partspec
else
// this is a partition
qb getmetadata   setdestforalias name  ts parthandle
if  hiveconf getboolvar conf  hiveconf confvars hivestatsautogather
// set that variable to automatically collect stats during the mapreduce job
qb getparseinfo   setisinserttotable true
// add the table spec for the destination table.
qb getparseinfo   addtablespec ts tablename tolowercase    ts
break
case hiveparser tok_local_dir
case hiveparser tok_dir
// this is a dfs file
string fname   stripquotes ast getchild 0  gettext
if    qb getparseinfo   getissubq
astnode  ast getchild 0   gettoken   gettype      hiveparser tok_tmp_file
if  qb isctas
qb setisquery false
ctx setresdir null
ctx setresfile null
// allocate a temporary output dir on the location of the table
string tablename   getunescapedname  astnode ast getchild 0
table newtable   db newtable tablename
path location
try
warehouse wh   new warehouse conf
location   wh getdatabasepath db getdatabase newtable getdbname
catch  metaexception e
throw new semanticexception e
try
fname   ctx getexternaltmpfileuri
fileutils makequalified location  conf  touri
catch  exception e
throw new semanticexception generateerrormessage ast
location tostring     e
if  hiveconf getboolvar conf  hiveconf confvars hivestatsautogather
tablespec ts   new tablespec db  conf  this ast
// set that variable to automatically collect stats during the mapreduce job
qb getparseinfo   setisinserttotable true
// add the table spec for the destination table.
qb getparseinfo   addtablespec ts tablename tolowercase    ts
else
qb setisquery true
fname   ctx getmrtmpfileuri
ctx setresdir new path fname
qb getmetadata   setdestforalias name  fname
ast gettoken   gettype      hiveparser tok_dir
break
default
throw new semanticexception generateerrormessage ast
ast gettoken   gettype
catch  hiveexception e
// has to use full name to make sure it does not conflict with
// org.apache.commons.lang.stringutils
log error org apache hadoop util stringutils stringifyexception e
throw new semanticexception e getmessage    e
private void replaceviewreferencewithdefinition qb qb  table tab
string tab_name  string alias  throws semanticexception
parsedriver pd   new parsedriver
astnode viewtree
final astnodeorigin vieworigin   new astnodeorigin    tab gettablename
tab getviewexpandedtext    alias  qb getparseinfo   getsrcforalias
alias
try
string viewtext   tab getviewexpandedtext
// reparse text, passing null for context to avoid clobbering
// the top-level token stream.
astnode tree   pd parse viewtext  null
tree   parseutils findrootnonnulltoken tree
viewtree   tree
dispatcher nodeorigindispatcher   new dispatcher
public object dispatch node nd  java util stack<node> stack
object    nodeoutputs
astnode  nd  setorigin vieworigin
return null
graphwalker nodeorigintagger   new defaultgraphwalker
nodeorigindispatcher
nodeorigintagger startwalking java util collections
<node> singleton viewtree   null
catch  parseexception e
// a user could encounter this if a stored view definition contains
// an old sql construct which has been eliminated in a later hive
// version, so we need to provide full debugging info to help
// with fixing the view definition.
log error org apache hadoop util stringutils stringifyexception e
stringbuilder sb   new stringbuilder
sb append e getmessage
errormsg renderorigin sb  vieworigin
throw new semanticexception sb tostring    e
qbexpr qbexpr   new qbexpr alias
dophase1qbexpr viewtree  qbexpr  qb getid    alias
qb rewriteviewtosubq alias  tab_name  qbexpr
private boolean ispresent string list  string elem
for  string s   list
if  s tolowercase   equals elem
return true
return false
@suppresswarnings
private void parsejoincondpopulatealias qbjointree jointree  astnode condn
arraylist<string> leftaliases  arraylist<string> rightaliases
arraylist<string> fields  throws semanticexception
// string[] allaliases = jointree.getallaliases();
switch  condn gettoken   gettype
case hiveparser tok_table_or_col
string tableorcol   unescapeidentifier condn getchild 0  gettext
tolowercase
unparsetranslator addidentifiertranslation  astnode  condn getchild 0
if  ispresent jointree getleftaliases    tableorcol
if   leftaliases contains tableorcol
leftaliases add tableorcol
else if  ispresent jointree getrightaliases    tableorcol
if   rightaliases contains tableorcol
rightaliases add tableorcol
else
// we don't support columns without table prefix in join condition right
// now.
// we need to pass metadata here to know which table the column belongs
// to.
throw new semanticexception errormsg invalid_table_alias getmsg condn
getchild 0
break
case hiveparser identifier
// it may be a field name, return the identifier and let the caller decide
// whether it is or not
if  fields    null
fields
add unescapeidentifier condn gettoken   gettext   tolowercase
unparsetranslator addidentifiertranslation condn
break
case hiveparser number
case hiveparser stringliteral
case hiveparser tok_stringliteralsequence
case hiveparser tok_charsetliteral
case hiveparser kw_true
case hiveparser kw_false
break
case hiveparser tok_function
// check all the arguments
for  int i   1  i < condn getchildcount    i
parsejoincondpopulatealias jointree   astnode  condn getchild i
leftaliases  rightaliases  null
break
default
// this is an operator - so check whether it is unary or binary operator
if  condn getchildcount      1
parsejoincondpopulatealias jointree   astnode  condn getchild 0
leftaliases  rightaliases  null
else if  condn getchildcount      2
arraylist<string> fields1   null
// if it is a dot operator, remember the field name of the rhs of the
// left semijoin
if  jointree getnosemijoin      false
condn gettoken   gettype      hiveparser dot
// get the semijoin rhs table name and field name
fields1   new arraylist<string>
int rhssize   rightaliases size
parsejoincondpopulatealias jointree   astnode  condn getchild 0
leftaliases  rightaliases  null
string rhsalias   null
if  rightaliases size   > rhssize       the new table is rhs table
rhsalias   rightaliases get rightaliases size     1
parsejoincondpopulatealias jointree   astnode  condn getchild 1
leftaliases  rightaliases  fields1
if  rhsalias    null    fields1 size   > 0
jointree addrhssemijoincolumns rhsalias  condn
else
parsejoincondpopulatealias jointree   astnode  condn getchild 0
leftaliases  rightaliases  null
parsejoincondpopulatealias jointree   astnode  condn getchild 1
leftaliases  rightaliases  fields1
else
throw new semanticexception condn tostringtree
condn getchildcount
break
private void populatealiases list<string> leftaliases
list<string> rightaliases  astnode condn  qbjointree jointree
list<string> leftsrc  throws semanticexception
if   leftaliases size      0      rightaliases size      0
throw new semanticexception errormsg invalid_join_condition_1
getmsg condn
if  rightaliases size      0
assert rightaliases size      1
jointree getexpressions   get 1  add condn
else if  leftaliases size      0
jointree getexpressions   get 0  add condn
for  string s   leftaliases
if   leftsrc contains s
leftsrc add s
else
throw new semanticexception errormsg invalid_join_condition_2
getmsg condn
private void parsejoincondition qbjointree jointree  astnode joincond  list<string> leftsrc
throws semanticexception
if  joincond    null
return
joincond cond   jointree getjoincond
jointype type   cond getjointype
parsejoincondition jointree  joincond  leftsrc  type
list<arraylist<astnode>> filters   jointree getfilters
if  type    jointype leftouter    type    jointype fullouter
jointree addfiltermapping cond getleft    cond getright    filters get 0  size
if  type    jointype rightouter    type    jointype fullouter
jointree addfiltermapping cond getright    cond getleft    filters get 1  size
/**
* parse the join condition. if the condition is a join condition, throw an
* error if it is not an equality. otherwise, break it into left and right
* expressions and store in the join tree. if the condition is a join filter,
* add it to the filter list of join tree. the join condition can contains
* conditions on both the left and tree trees and filters on either.
* currently, we only support equi-joins, so we throw an error if the
* condition involves both subtrees and is not a equality. also, we only
* support and i.e ors are not supported currently as their semantics are not
* very clear, may lead to data explosion and there is no usecase.
*
* @param jointree
*          jointree to be populated
* @param joincond
*          join condition
* @param leftsrc
*          left sources
* @throws semanticexception
*/
private void parsejoincondition qbjointree jointree  astnode joincond
list<string> leftsrc  jointype type  throws semanticexception
if  joincond    null
return
switch  joincond gettoken   gettype
case hiveparser kw_or
throw new semanticexception errormsg invalid_join_condition_3
getmsg joincond
case hiveparser kw_and
parsejoincondition jointree   astnode  joincond getchild 0   leftsrc  type
parsejoincondition jointree   astnode  joincond getchild 1   leftsrc  type
break
case hiveparser equal_ns
case hiveparser equal
astnode leftcondn    astnode  joincond getchild 0
arraylist<string> leftcondal1   new arraylist<string>
arraylist<string> leftcondal2   new arraylist<string>
parsejoincondpopulatealias jointree  leftcondn  leftcondal1  leftcondal2

astnode rightcondn    astnode  joincond getchild 1
arraylist<string> rightcondal1   new arraylist<string>
arraylist<string> rightcondal2   new arraylist<string>
parsejoincondpopulatealias jointree  rightcondn  rightcondal1
rightcondal2  null
// is it a filter or a join condition
// if it is filter see if it can be pushed above the join
// filter cannot be pushed if
// * join is full outer or
// * join is left outer and filter is on left alias or
// * join is right outer and filter is on right alias
if    leftcondal1 size      0      leftcondal2 size      0
rightcondal1 size      0      rightcondal2 size      0
throw new semanticexception errormsg invalid_join_condition_1
getmsg joincond
if  leftcondal1 size      0
if   rightcondal1 size      0
rightcondal1 size      0      rightcondal2 size      0
if  type equals jointype leftouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 0  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 0  add joincond
else
jointree getfiltersforpushing   get 0  add joincond
else if  rightcondal2 size      0
populatealiases leftcondal1  leftcondal2  leftcondn  jointree
leftsrc
populatealiases rightcondal1  rightcondal2  rightcondn  jointree
leftsrc
boolean nullsafe   joincond gettoken   gettype      hiveparser equal_ns
jointree getnullsafes   add nullsafe
else if  leftcondal2 size      0
if   rightcondal2 size      0
rightcondal1 size      0      rightcondal2 size      0
if  type equals jointype rightouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 1  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 1  add joincond
else
jointree getfiltersforpushing   get 1  add joincond
else if  rightcondal1 size      0
populatealiases leftcondal1  leftcondal2  leftcondn  jointree
leftsrc
populatealiases rightcondal1  rightcondal2  rightcondn  jointree
leftsrc
boolean nullsafe   joincond gettoken   gettype      hiveparser equal_ns
jointree getnullsafes   add nullsafe
else if  rightcondal1 size      0
if  type equals jointype leftouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 0  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 0  add joincond
else
jointree getfiltersforpushing   get 0  add joincond
else
if  type equals jointype rightouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 1  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 1  add joincond
else
jointree getfiltersforpushing   get 1  add joincond
break
default
boolean isfunction    joincond gettype      hiveparser tok_function
// create all children
int childrenbegin    isfunction ? 1   0
arraylist<arraylist<string>> leftalias   new arraylist<arraylist<string>>
joincond getchildcount     childrenbegin
arraylist<arraylist<string>> rightalias   new arraylist<arraylist<string>>
joincond getchildcount     childrenbegin
for  int ci   0  ci < joincond getchildcount     childrenbegin  ci
arraylist<string> left   new arraylist<string>
arraylist<string> right   new arraylist<string>
leftalias add left
rightalias add right
for  int ci   childrenbegin  ci < joincond getchildcount    ci
parsejoincondpopulatealias jointree   astnode  joincond getchild ci
leftalias get ci   childrenbegin   rightalias get ci
childrenbegin   null
boolean leftaliasnull   true
for  arraylist<string> left   leftalias
if  left size      0
leftaliasnull   false
break
boolean rightaliasnull   true
for  arraylist<string> right   rightalias
if  right size      0
rightaliasnull   false
break
if   leftaliasnull     rightaliasnull
throw new semanticexception errormsg invalid_join_condition_1
getmsg joincond
if   leftaliasnull
if  type equals jointype leftouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 0  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 0  add joincond
else
jointree getfiltersforpushing   get 0  add joincond
else
if  type equals jointype rightouter
type equals jointype fullouter
if  conf getboolvar hiveconf confvars hiveouterjoinsupportsfilters
jointree getfilters   get 1  add joincond
else
log warn errormsg outerjoin_uses_filters
jointree getfiltersforpushing   get 1  add joincond
else
jointree getfiltersforpushing   get 1  add joincond
break
@suppresswarnings
public <t extends operatordesc> operator<t> putopinsertmap operator<t> op
rowresolver rr
opparsecontext ctx   new opparsecontext rr
opparsectx put op  ctx
op augmentplan
return op
@suppresswarnings
private operator genhavingplan string dest  qb qb  operator input
throws semanticexception
astnode havingexpr   qb getparseinfo   gethavingforclause dest
opparsecontext inputctx   opparsectx get input
rowresolver inputrr   inputctx getrowresolver
map<astnode  string> exprtocolumnalias   qb getparseinfo   getallexprtocolumnalias
for  astnode astnode   exprtocolumnalias keyset
if  inputrr getexpression astnode     null
inputrr put    exprtocolumnalias get astnode   inputrr getexpression astnode
astnode condn    astnode  havingexpr getchild 0
operator output   putopinsertmap operatorfactory getandmakechild
new filterdesc genexprnodedesc condn  inputrr   false   new rowschema
inputrr getcolumninfos     input   inputrr
return output
@suppresswarnings
private operator genfilterplan string dest  qb qb  operator input
throws semanticexception
astnode whereexpr   qb getparseinfo   getwhrforclause dest
return genfilterplan qb   astnode  whereexpr getchild 0   input
/**
* create a filter plan. the condition and the inputs are specified.
*
* @param qb
*          current query block
* @param condn
*          the condition to be resolved
* @param input
*          the input operator
*/
@suppresswarnings
private operator genfilterplan qb qb  astnode condn  operator input
throws semanticexception
opparsecontext inputctx   opparsectx get input
rowresolver inputrr   inputctx getrowresolver
operator output   putopinsertmap operatorfactory getandmakechild
new filterdesc genexprnodedesc condn  inputrr   false   new rowschema
inputrr getcolumninfos     input   inputrr
if  log isdebugenabled
log debug     qb getid
inputrr tostring
return output
@suppresswarnings
private integer gencollistregex string colregex  string tabalias
astnode sel  arraylist<exprnodedesc> col_list
rowresolver input  integer pos  rowresolver output  list<string> aliases
throws semanticexception
// the table alias should exist
if  tabalias    null     input hastablealias tabalias
throw new semanticexception errormsg invalid_table_alias getmsg sel
// todo: have to put in the support for as clause
pattern regex   null
try
regex   pattern compile colregex  pattern case_insensitive
catch  patternsyntaxexception e
throw new semanticexception errormsg invalid_column getmsg sel  e
getmessage
stringbuilder replacementtext   new stringbuilder
int matched   0
// add empty string to the list of aliases. some operators (ex. groupby) add
// columninfos for table alias "".
if   aliases contains
aliases add
// for expr "*", aliases should be iterated in the order they are specified
// in the query.
for  string alias   aliases
hashmap<string  columninfo> fmap   input getfieldmap alias
if  fmap    null
continue
// for the tab.* case, add all the columns to the fieldlist
// from the input schema
for  map entry<string  columninfo> entry   fmap entryset
columninfo colinfo   entry getvalue
string name   colinfo getinternalname
string tmp   input reverselookup name
// skip the colinfos which are not for this particular alias
if  tabalias    null     tmp equalsignorecase tabalias
continue
if  colinfo getisvirtualcol      colinfo ishiddenvirtualcol
continue
// not matching the regex?
if   regex matcher tmp  matches
continue
exprnodecolumndesc expr   new exprnodecolumndesc colinfo gettype
name  colinfo gettabalias    colinfo getisvirtualcol    colinfo isskewedcol
col_list add expr
output put tmp  tmp
new columninfo getcolumninternalname pos   colinfo gettype
colinfo gettabalias    colinfo getisvirtualcol
colinfo ishiddenvirtualcol
pos   integer valueof pos intvalue     1
matched
if  unparsetranslator isenabled
if  replacementtext length   > 0
replacementtext append
replacementtext append hiveutils unparseidentifier tmp
replacementtext append
replacementtext append hiveutils unparseidentifier tmp
if  matched    0
throw new semanticexception errormsg invalid_column getmsg sel
if  unparsetranslator isenabled
unparsetranslator addtranslation sel  replacementtext tostring
return pos
public static string getcolumninternalname int pos
return hiveconf getcolumninternalname pos
private string getscriptprogname string cmd
int end   cmd indexof
return  end     1  ? cmd   cmd substring 0  end
private string getscriptargs string cmd
int end   cmd indexof
return  end     1  ?     cmd substring end  cmd length
private static int getpositionfrominternalname string internalname
return hiveconf getpositionfrominternalname internalname
private string fetchfilesnotinlocalfilesystem string cmd
sessionstate ss   sessionstate get
string progname   getscriptprogname cmd
if  sessionstate candownloadresource progname
string filepath   ss add_resource resourcetype file  progname  true
if  filepath    null
throw new runtimeexception     progname
path p   new path filepath
string filename   p getname
string scriptargs   getscriptargs cmd
string finalcmd   filename   scriptargs
return finalcmd
return cmd
private tabledesc gettabledescfromserde astnode child  string cols
string coltypes  boolean defaultcols  throws semanticexception
if  child gettype      hiveparser tok_serdename
string serdename   unescapesqlstring child getchild 0  gettext
class<? extends deserializer> serdeclass   null
try
serdeclass    class<? extends deserializer>  class forname serdename
true  javautils getclassloader
catch  classnotfoundexception e
throw new semanticexception e
tabledesc tbldesc   planutils gettabledesc serdeclass  integer
tostring utilities tabcode   cols  coltypes  defaultcols
// copy all the properties
if  child getchildcount      2
astnode prop    astnode    astnode  child getchild 1   getchild 0
for  int propchild   0  propchild < prop getchildcount    propchild
string key   unescapesqlstring prop getchild propchild  getchild 0
gettext
string value   unescapesqlstring prop getchild propchild  getchild 1
gettext
tbldesc getproperties   setproperty key  value
return tbldesc
else if  child gettype      hiveparser tok_serdeprops
tabledesc tbldesc   planutils getdefaulttabledesc integer
tostring utilities ctrlacode   cols  coltypes  defaultcols
int numchildrowformat   child getchildcount
for  int numc   0  numc < numchildrowformat  numc
astnode rowchild    astnode  child getchild numc
switch  rowchild gettoken   gettype
case hiveparser tok_tablerowformatfield
string fielddelim   unescapesqlstring rowchild getchild 0  gettext
tbldesc getproperties
setproperty serdeconstants field_delim  fielddelim
tbldesc getproperties   setproperty serdeconstants serialization_format
fielddelim
if  rowchild getchildcount   >  2
string fieldescape   unescapesqlstring rowchild getchild 1
gettext
tbldesc getproperties   setproperty serdeconstants escape_char
fieldescape
break
case hiveparser tok_tablerowformatcollitems
tbldesc getproperties   setproperty serdeconstants collection_delim
unescapesqlstring rowchild getchild 0  gettext
break
case hiveparser tok_tablerowformatmapkeys
tbldesc getproperties   setproperty serdeconstants mapkey_delim
unescapesqlstring rowchild getchild 0  gettext
break
case hiveparser tok_tablerowformatlines
string linedelim   unescapesqlstring rowchild getchild 0  gettext
tbldesc getproperties   setproperty serdeconstants line_delim  linedelim
if   linedelim equals        linedelim equals
throw new semanticexception generateerrormessage rowchild
errormsg lines_terminated_by_non_newline getmsg
break
default
assert false
return tbldesc
// should never come here
return null
private void failifcolaliasexists set<string> nameset  string name
throws semanticexception
if  nameset contains name
throw new semanticexception errormsg column_alias_already_exists
getmsg name
nameset add name
@suppresswarnings
private operator genscriptplan astnode trfm  qb qb  operator input
throws semanticexception
// if there is no "as" clause, the output schema will be "key,value"
arraylist<columninfo> outputcols   new arraylist<columninfo>
int inputserdenum   1  inputrecordwriternum   2
int outputserdenum   4  outputrecordreadernum   5
int outputcolsnum   6
boolean outputcolnames   false  outputcolschemas   false
int execpos   3
boolean defaultoutputcols   false
// go over all the children
if  trfm getchildcount   > outputcolsnum
astnode outcols    astnode  trfm getchild outputcolsnum
if  outcols gettype      hiveparser tok_aliaslist
outputcolnames   true
else if  outcols gettype      hiveparser tok_tabcollist
outputcolschemas   true
// if column type is not specified, use a string
if   outputcolnames     outputcolschemas
string intname   getcolumninternalname 0
columninfo colinfo   new columninfo intname
typeinfofactory stringtypeinfo  null  false
colinfo setalias
outputcols add colinfo
intname   getcolumninternalname 1
colinfo   new columninfo intname  typeinfofactory stringtypeinfo  null
false
colinfo setalias
outputcols add colinfo
defaultoutputcols   true
else
astnode collist    astnode  trfm getchild outputcolsnum
int ccount   collist getchildcount
set<string> colaliasnamesduplicatecheck   new hashset<string>
if  outputcolnames
for  int i   0  i < ccount    i
string colalias   unescapeidentifier   astnode  collist getchild i
gettext
failifcolaliasexists colaliasnamesduplicatecheck  colalias
string intname   getcolumninternalname i
columninfo colinfo   new columninfo intname
typeinfofactory stringtypeinfo  null  false
colinfo setalias colalias
outputcols add colinfo
else
for  int i   0  i < ccount    i
astnode child    astnode  collist getchild i
assert child gettype      hiveparser tok_tabcol
string colalias   unescapeidentifier   astnode  child getchild 0
gettext
failifcolaliasexists colaliasnamesduplicatecheck  colalias
string intname   getcolumninternalname i
columninfo colinfo   new columninfo intname  typeinfoutils
gettypeinfofromtypestring gettypestringfromast  astnode  child
getchild 1     null  false
colinfo setalias colalias
outputcols add colinfo
rowresolver out_rwsch   new rowresolver
stringbuilder columns   new stringbuilder
stringbuilder columntypes   new stringbuilder
for  int i   0  i < outputcols size      i
if  i    0
columns append
columntypes append
columns append outputcols get i  getinternalname
columntypes append outputcols get i  gettype   gettypename
out_rwsch put qb getparseinfo   getalias    outputcols get i  getalias
outputcols get i
stringbuilder inpcolumns   new stringbuilder
stringbuilder inpcolumntypes   new stringbuilder
arraylist<columninfo> inputschema   opparsectx get input  getrowresolver
getcolumninfos
for  int i   0  i < inputschema size      i
if  i    0
inpcolumns append
inpcolumntypes append
inpcolumns append inputschema get i  getinternalname
inpcolumntypes append inputschema get i  gettype   gettypename
tabledesc outinfo
tabledesc errinfo
tabledesc ininfo
string defaultserdename   conf getvar hiveconf confvars hivescriptserde
class<? extends deserializer> serde
try
serde    class<? extends deserializer>  class forname defaultserdename
true  javautils getclassloader
catch  classnotfoundexception e
throw new semanticexception e
int fieldseparator   utilities tabcode
if  hiveconf getboolvar conf  hiveconf confvars hivescriptescape
fieldseparator   utilities ctrlacode
// input and output serdes
if  trfm getchild inputserdenum  getchildcount   > 0
ininfo   gettabledescfromserde  astnode     astnode  trfm
getchild inputserdenum    getchild 0   inpcolumns tostring
inpcolumntypes tostring    false
else
ininfo   planutils gettabledesc serde  integer
tostring fieldseparator   inpcolumns tostring    inpcolumntypes
tostring    false  true
if  trfm getchild outputserdenum  getchildcount   > 0
outinfo   gettabledescfromserde  astnode     astnode  trfm
getchild outputserdenum    getchild 0   columns tostring
columntypes tostring    false
// this is for backward compatibility. if the user did not specify the
// output column list, we assume that there are 2 columns: key and value.
// however, if the script outputs: col1, col2, col3 seperated by tab, the
// requirement is: key is col and value is (col2 tab col3)
else
outinfo   planutils gettabledesc serde  integer
tostring fieldseparator   columns tostring    columntypes
tostring    defaultoutputcols
// error stream always uses the default serde with a single column
errinfo   planutils gettabledesc serde  integer tostring utilities tabcode
// output record readers
class<? extends recordreader> outrecordreader   getrecordreader  astnode  trfm
getchild outputrecordreadernum
class<? extends recordwriter> inrecordwriter   getrecordwriter  astnode  trfm
getchild inputrecordwriternum
class<? extends recordreader> errrecordreader   getdefaultrecordreader
operator output   putopinsertmap operatorfactory getandmakechild
new scriptdesc
fetchfilesnotinlocalfilesystem stripquotes trfm getchild execpos  gettext
ininfo  inrecordwriter  outinfo  outrecordreader  errrecordreader  errinfo
new rowschema out_rwsch getcolumninfos     input   out_rwsch
return output
private class<? extends recordreader> getrecordreader astnode node
throws semanticexception
string name
if  node getchildcount      0
name   conf getvar hiveconf confvars hivescriptrecordreader
else
name   unescapesqlstring node getchild 0  gettext
try
return  class<? extends recordreader>  class forname name  true
javautils getclassloader
catch  classnotfoundexception e
throw new semanticexception e
private class<? extends recordreader> getdefaultrecordreader
throws semanticexception
string name
name   conf getvar hiveconf confvars hivescriptrecordreader
try
return  class<? extends recordreader>  class forname name  true
javautils getclassloader
catch  classnotfoundexception e
throw new semanticexception e
private class<? extends recordwriter> getrecordwriter astnode node
throws semanticexception
string name
if  node getchildcount      0
name   conf getvar hiveconf confvars hivescriptrecordwriter
else
name   unescapesqlstring node getchild 0  gettext
try
return  class<? extends recordwriter>  class forname name  true
javautils getclassloader
catch  classnotfoundexception e
throw new semanticexception e
private list<integer> getgroupingsetsforrollup int size
list<integer> groupingsetkeys   new arraylist<integer>
for  int i   0  i <  size  i
groupingsetkeys add  1 << i    1
return groupingsetkeys
private list<integer> getgroupingsetsforcube int size
int count   1 << size
list<integer> results   new arraylist<integer> count
for int i   0  i < count    i
results add i
return results
// this function returns the grouping sets along with the grouping expressions
// even if rollups and cubes are present in the query, they are converted to
// grouping sets at this point
private objectpair<list<astnode>  list<integer>> getgroupbygroupingsetsforclause
qbparseinfo parseinfo  string dest  throws semanticexception
list<integer> groupingsets   new arraylist<integer>
list<astnode> groupbyexprs   getgroupbyforclause parseinfo  dest
if  parseinfo getdestrollups   contains dest
groupingsets   getgroupingsetsforrollup groupbyexprs size
else if  parseinfo getdestcubes   contains dest
groupingsets   getgroupingsetsforcube groupbyexprs size
else if  parseinfo getdestgroupingsets   contains dest
groupingsets   getgroupingsets groupbyexprs  parseinfo  dest
return new objectpair<list<astnode>  list<integer>> groupbyexprs  groupingsets
private list<integer> getgroupingsets list<astnode> groupbyexpr  qbparseinfo parseinfo
string dest  throws semanticexception
map<string  integer> exprpos   new hashmap<string  integer>
for  int i   0  i < groupbyexpr size      i
astnode node   groupbyexpr get i
exprpos put node tostringtree    i
astnode root   parseinfo getgroupbyforclause dest
list<integer> result   new arraylist<integer> root    null ? 0   root getchildcount
if  root    null
for  int i   0  i < root getchildcount      i
astnode child    astnode  root getchild i
if child gettype      hiveparser tok_grouping_sets_expression
continue
int bitmap   0
for  int j   0  j < child getchildcount      j
string treeasstring   child getchild j  tostringtree
integer pos   exprpos get treeasstring
if pos    null
throw new semanticexception
generateerrormessage  astnode child getchild j
errormsg hive_grouping_sets_expr_not_in_groupby geterrorcodedmsg
bitmap   setbit bitmap  pos
result add bitmap
if checkfornoaggr result
throw new semanticexception
errormsg hive_grouping_sets_aggr_nofunc getmsg
return result
private boolean checkfornoaggr list<integer> bitmaps
boolean ret   true
for int mask   bitmaps
ret    mask    0
return ret
private int setbit int bitmap  int bitidx
return bitmap    1 << bitidx
/**
* this function is a wrapper of parseinfo.getgroupbyforclause which
* automatically translates select distinct a,b,c to select a,b,c group by
* a,b,c.
*/
static list<astnode> getgroupbyforclause qbparseinfo parseinfo  string dest
if  parseinfo getselforclause dest  gettoken   gettype      hiveparser tok_selectdi
astnode selectexprs   parseinfo getselforclause dest
list<astnode> result   new arraylist<astnode> selectexprs    null ? 0
selectexprs getchildcount
if  selectexprs    null
for  int i   0  i < selectexprs getchildcount      i
if    astnode  selectexprs getchild i   gettoken   gettype      hiveparser tok_hintlist
continue
// table.column as alias
astnode grpbyexpr    astnode  selectexprs getchild i  getchild 0
result add grpbyexpr
return result
else
astnode grpbyexprs   parseinfo getgroupbyforclause dest
list<astnode> result   new arraylist<astnode> grpbyexprs    null ? 0
grpbyexprs getchildcount
if  grpbyexprs    null
for  int i   0  i < grpbyexprs getchildcount      i
astnode grpbyexpr    astnode  grpbyexprs getchild i
if  grpbyexpr gettype      hiveparser tok_grouping_sets_expression
result add grpbyexpr
return result
private static string getcolalias astnode selexpr  string defaultname
rowresolver inputrr  boolean includefuncname  int colnum
string colalias   null
string tabalias   null
string colref   new string
if  selexpr getchildcount      2
// return zz for "xx + yy as zz"
colalias   unescapeidentifier selexpr getchild 1  gettext
colref   tabalias
colref   colalias
return colref
astnode root    astnode  selexpr getchild 0
if  root gettype      hiveparser tok_table_or_col
colalias
basesemanticanalyzer unescapeidentifier root getchild 0  gettext
colref   tabalias
colref   colalias
return colref
if  root gettype      hiveparser dot
astnode tab    astnode  root getchild 0
if  tab gettype      hiveparser tok_table_or_col
string t   unescapeidentifier tab getchild 0  gettext
if  inputrr hastablealias t
tabalias   t
// return zz for "xx.zz" and "xx.yy.zz"
astnode col    astnode  root getchild 1
if  col gettype      hiveparser identifier
colalias   unescapeidentifier col gettext
//if specified generate alias using func name
if  includefuncname     root gettype      hiveparser tok_function
string expr_flattened   root tostringtree
//remove all tok tokens
string expr_no_tok   expr_flattened replaceall
//remove all non alphanumeric letters, replace whitespace spans with underscore
string  expr_formatted   expr_no_tok replaceall       trim   replaceall
//limit length to 20 chars
if expr_formatted length  >autogen_colalias_prfx_maxlength
expr_formatted   expr_formatted substring 0  autogen_colalias_prfx_maxlength
//append colnum to make it unique
colalias   expr_formatted concat     colnum
if  colalias    null
// return defaultname if selexpr is not a simple xx.yy.zz
colalias   defaultname   colnum
colref   tabalias
colref   colalias
return colref
/**
* returns whether the pattern is a regex expression (instead of a normal
* string). normal string is a string with all alphabets/digits and "_".
*/
private static boolean isregex string pattern
for  int i   0  i < pattern length    i
if   character isletterordigit pattern charat i
pattern charat i
return true
return false
private operator<?> genselectplan string dest  qb qb  operator<?> input
throws semanticexception
astnode selexprlist   qb getparseinfo   getselforclause dest
operator<?> op   genselectplan selexprlist  qb  input
if  log isdebugenabled
log debug     dest
return op
@suppresswarnings
private operator<?> genselectplan astnode selexprlist  qb qb
operator<?> input  throws semanticexception
if  log isdebugenabled
log debug     selexprlist tostringtree
arraylist<exprnodedesc> col_list   new arraylist<exprnodedesc>
rowresolver out_rwsch   new rowresolver
astnode trfm   null
string alias   qb getparseinfo   getalias
integer pos   integer valueof 0
rowresolver inputrr   opparsectx get input  getrowresolver
// select * or select transform(*)
boolean selectstar   false
int posn   0
boolean hintpresent    selexprlist getchild 0  gettype      hiveparser tok_hintlist
if  hintpresent
posn
boolean isintransform    selexprlist getchild posn  getchild 0  gettype
hiveparser tok_transform
if  isintransform
queryproperties setusesscript true
globallimitctx sethastransformorudtf true
trfm    astnode  selexprlist getchild posn  getchild 0
// detect queries of the form select udtf(col) as ...
// by looking for a function as the first child, and then checking to see
// if the function is a generic udtf. it's not as clean as transform due to
// the lack of a special token.
boolean isudtf   false
string udtftablealias   null
arraylist<string> udtfcolaliases   new arraylist<string>
astnode udtfexpr    astnode  selexprlist getchild posn  getchild 0
genericudtf genericudtf   null
int udtfexprtype   udtfexpr gettype
if  udtfexprtype    hiveparser tok_function
udtfexprtype    hiveparser tok_functionstar
string funcname   typecheckprocfactory defaultexprprocessor
getfunctiontext udtfexpr  true
functioninfo fi   functionregistry getfunctioninfo funcname
if  fi    null
genericudtf   fi getgenericudtf
isudtf    genericudtf    null
if  isudtf
globallimitctx sethastransformorudtf true
if  isudtf     fi isnative
unparsetranslator addidentifiertranslation  astnode  udtfexpr
getchild 0
if  isudtf
// only support a single expression when it's a udtf
if  selexprlist getchildcount   > 1
throw new semanticexception generateerrormessage
astnode  selexprlist getchild 1
errormsg udtf_multiple_expr getmsg
// require an as for udtfs for column aliases
astnode selexpr    astnode  selexprlist getchild posn
if  selexpr getchildcount   < 2
throw new semanticexception generateerrormessage udtfexpr
errormsg udtf_require_as getmsg
// get the column / table aliases from the expression. start from 1 as
// 0 is the tok_function
for  int i   1  i < selexpr getchildcount    i
astnode selexprchild    astnode  selexpr getchild i
switch  selexprchild gettype
case hiveparser identifier
udtfcolaliases add unescapeidentifier selexprchild gettext
unparsetranslator addidentifiertranslation selexprchild
break
case hiveparser tok_tabalias
assert  selexprchild getchildcount      1
udtftablealias   unescapeidentifier selexprchild getchild 0
gettext
qb addalias udtftablealias
unparsetranslator addidentifiertranslation  astnode  selexprchild
getchild 0
break
default
assert  false
if  log isdebugenabled
log debug     udtftablealias
log debug     udtfcolaliases
// the list of expressions after select or select transform.
astnode exprlist
if  isintransform
exprlist    astnode  trfm getchild 0
else if  isudtf
exprlist   udtfexpr
else
exprlist   selexprlist
if  log isdebugenabled
log debug     inputrr tostring
// for udtf's, skip the function name to get the expressions
int startposn   isudtf ? posn   1   posn
if  isintransform
startposn   0
// iterate over all expression (either after select, or in select transform)
for  int i   startposn  i < exprlist getchildcount      i
// child can be expr as alias, or expr.
astnode child    astnode  exprlist getchild i
boolean hasasclause     isintransform      child getchildcount      2
// expr as (alias,...) parses, but is only allowed for udtf's
// this check is not needed and invalid when there is a transform b/c the
// ast's are slightly different.
if   isintransform     isudtf    child getchildcount   > 2
throw new semanticexception generateerrormessage
astnode  child getchild 2
errormsg invalid_as getmsg
// the real expression
astnode expr
string tabalias
string colalias
if  isintransform    isudtf
tabalias   null
colalias   autogencolaliasprfxlbl   i
expr   child
else
// get rid of tok_selexpr
expr    astnode  child getchild 0
string colref   getcolalias child  autogencolaliasprfxlbl  inputrr
autogencolaliasprfxincludefuncname  i
tabalias   colref
colalias   colref
if  hasasclause
unparsetranslator addidentifiertranslation  astnode  child
getchild 1
if  expr gettype      hiveparser tok_allcolref
pos   gencollistregex    expr getchildcount      0 ? null
getunescapedname  astnode expr getchild 0   tolowercase
expr  col_list  inputrr  pos  out_rwsch  qb getaliases
selectstar   true
else if  expr gettype      hiveparser tok_table_or_col     hasasclause
inputrr getisexprresolver
isregex unescapeidentifier expr getchild 0  gettext
// in case the expression is a regex col.
// this can only happen without as clause
// we don't allow this for exprresolver - the group by case
pos   gencollistregex unescapeidentifier expr getchild 0  gettext
null  expr  col_list  inputrr  pos  out_rwsch  qb getaliases
else if  expr gettype      hiveparser dot
expr getchild 0  gettype      hiveparser tok_table_or_col
inputrr hastablealias unescapeidentifier expr getchild 0
getchild 0  gettext   tolowercase         hasasclause
inputrr getisexprresolver
isregex unescapeidentifier expr getchild 1  gettext
// in case the expression is table.col (col can be regex).
// this can only happen without as clause
// we don't allow this for exprresolver - the group by case
pos   gencollistregex unescapeidentifier expr getchild 1  gettext
unescapeidentifier expr getchild 0  getchild 0  gettext
tolowercase     expr  col_list  inputrr  pos  out_rwsch
qb getaliases
else
// case when this is an expression
typecheckctx tcctx   new typecheckctx inputrr
// we allow stateful functions in the select list (but nowhere else)
tcctx setallowstatefulfunctions true
exprnodedesc exp   genexprnodedesc expr  inputrr  tcctx
col_list add exp
if   stringutils isempty alias
out_rwsch get null  colalias     null
throw new semanticexception errormsg ambiguous_column getmsg colalias
columninfo colinfo   new columninfo getcolumninternalname pos
exp getwritableobjectinspector    tabalias  false
colinfo setskewedcol  exp instanceof exprnodecolumndesc  ?   exprnodecolumndesc  exp
isskewedcol     false
out_rwsch put tabalias  colalias  colinfo
pos   integer valueof pos intvalue     1
selectstar   selectstar    exprlist getchildcount      posn   1
arraylist<string> columnnames   new arraylist<string>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
for  int i   0  i < col_list size    i
// replace null with cast(null as string)
if  col_list get i  instanceof exprnodenulldesc
col_list set i  new exprnodeconstantdesc
typeinfofactory stringtypeinfo  null
string outputcol   getcolumninternalname i
colexprmap put outputcol  col_list get i
columnnames add outputcol
operator output   putopinsertmap operatorfactory getandmakechild
new selectdesc col_list  columnnames  selectstar   new rowschema
out_rwsch getcolumninfos     input   out_rwsch
output setcolumnexprmap colexprmap
if  isintransform
output   genscriptplan trfm  qb  output
if  isudtf
output   genudtfplan genericudtf  udtftablealias  udtfcolaliases  qb
output
if  log isdebugenabled
log debug     out_rwsch tostring
return output
/**
* class to store genericudaf related information.
*/
static class genericudafinfo
arraylist<exprnodedesc> convertedparameters
genericudafevaluator genericudafevaluator
typeinfo returntype
/**
* convert exprnodedesc array to typeinfo array.
*/
static arraylist<typeinfo> gettypeinfo arraylist<exprnodedesc> exprs
arraylist<typeinfo> result   new arraylist<typeinfo>
for  exprnodedesc expr   exprs
result add expr gettypeinfo
return result
/**
* convert exprnodedesc array to objectinspector array.
*/
static arraylist<objectinspector> getwritableobjectinspector arraylist<exprnodedesc> exprs
arraylist<objectinspector> result   new arraylist<objectinspector>
for  exprnodedesc expr   exprs
result add expr getwritableobjectinspector
return result
/**
* convert exprnodedesc array to typeinfo array.
*/
static objectinspector getstandardobjectinspector arraylist<typeinfo> exprs
objectinspector result   new objectinspector
for  int i   0  i < exprs size    i
result   typeinfoutils
getstandardwritableobjectinspectorfromtypeinfo exprs get i
return result
/**
* returns the genericudafevaluator for the aggregation. this is called once
* for each groupby aggregation.
*/
static genericudafevaluator getgenericudafevaluator string aggname
arraylist<exprnodedesc> aggparameters  astnode aggtree
boolean isdistinct  boolean isallcolumns
throws semanticexception
arraylist<objectinspector> originalparametertypeinfos
getwritableobjectinspector aggparameters
genericudafevaluator result   functionregistry getgenericudafevaluator
aggname  originalparametertypeinfos  isdistinct  isallcolumns
if  null    result
string reason    "   aggname
with parameters "   originalparametertypeinfos
throw new semanticexception errormsg invalid_function_signature getmsg
astnode  aggtree getchild 0   reason
return result
/**
* returns the genericudafinfo struct for the aggregation.
*
* @param aggname
*          the name of the udaf.
* @param aggparameters
*          the exprnodedesc of the original parameters
* @param aggtree
*          the astnode node of the udaf in the query.
* @return genericudafinfo
* @throws semanticexception
*           when the udaf is not found or has problems.
*/
static genericudafinfo getgenericudafinfo genericudafevaluator evaluator
genericudafevaluator mode emode  arraylist<exprnodedesc> aggparameters
throws semanticexception
genericudafinfo r   new genericudafinfo
// set r.genericudafevaluator
r genericudafevaluator   evaluator
// set r.returntype
objectinspector returnoi   null
try
arraylist<objectinspector> aggois   getwritableobjectinspector aggparameters
objectinspector aggoiarray   new objectinspector
for  int ii   0  ii < aggois size      ii
aggoiarray   aggois get ii
returnoi   r genericudafevaluator init emode  aggoiarray
r returntype   typeinfoutils gettypeinfofromobjectinspector returnoi
catch  hiveexception e
throw new semanticexception e
// set r.convertedparameters
// todo: type conversion
r convertedparameters   aggparameters
return r
private static genericudafevaluator mode groupbydescmodetoudafmode
groupbydesc mode mode  boolean isdistinct
switch  mode
case complete
return genericudafevaluator mode complete
case partial1
return genericudafevaluator mode partial1
case partial2
return genericudafevaluator mode partial2
case partials
return isdistinct ? genericudafevaluator mode partial1
genericudafevaluator mode partial2
case final
return genericudafevaluator mode final
case hash
return genericudafevaluator mode partial1
case mergepartial
return isdistinct ? genericudafevaluator mode complete
genericudafevaluator mode final
default
throw new runtimeexception
/**
* check if the given internalname represents a constant parameter in aggregation parameters
* of an aggregation tree.
* this method is only invoked when map-side aggregation is not involved. in this case,
* every parameter in every aggregation tree should already have a corresponding columninfo,
* which is generated when the corresponding reducesinkoperator of the groupbyoperator being
* generating is generated. if we find that this parameter is a constant parameter,
* we will return the corresponding exprnodedesc in reducevalues, and we will not need to
* use a new exprnodecolumndesc, which can not be treated as a constant parameter, for this
* parameter (since the writableobjectinspector of a exprnodecolumndesc will not be
* a instance of constantobjectinspector).
*
* @param reducevalues
*          value columns of the corresponding reducesinkoperator
* @param internalname
*          the internal name of this parameter
* @return the exprnodedesc of the constant parameter if the given internalname represents
*         a constant parameter; otherwise, return null
*/
private exprnodedesc isconstantparameterinaggregationparameters string internalname
list<exprnodedesc> reducevalues
// only the pattern of "value._col([0-9]+)" should be handled.
string terms   internalname split
if  terms length    2    reducevalues    null
return null
if  utilities reducefield value tostring   equals terms
int pos   getpositionfrominternalname terms
if  pos >  0    pos < reducevalues size
exprnodedesc reducevalue   reducevalues get pos
if  reducevalue    null
if  reducevalue getwritableobjectinspector   instanceof constantobjectinspector
// this internalname represents a constant parameter in aggregation parameters
return reducevalue
return null
/**
* generate the groupbyoperator for the query block (parseinfo.getxxx(dest)).
* the new groupbyoperator will be a child of the reducesinkoperatorinfo.
*
* @param mode
*          the mode of the aggregation (partial1 or complete)
* @param genericudafevaluators
*          if not null, this function will store the mapping from aggregation
*          stringtree to the genericudafevaluator in this parameter, so it
*          can be used in the next-stage groupby aggregations.
* @return the new groupbyoperator
*/
@suppresswarnings
private operator gengroupbyplangroupbyoperator qbparseinfo parseinfo
string dest  operator reducesinkoperatorinfo  groupbydesc mode mode
map<string  genericudafevaluator> genericudafevaluators
throws semanticexception
rowresolver groupbyinputrowresolver   opparsectx
get reducesinkoperatorinfo  getrowresolver
rowresolver groupbyoutputrowresolver   new rowresolver
groupbyoutputrowresolver setisexprresolver true
arraylist<exprnodedesc> groupbykeys   new arraylist<exprnodedesc>
arraylist<aggregationdesc> aggregations   new arraylist<aggregationdesc>
arraylist<string> outputcolumnnames   new arraylist<string>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
columninfo exprinfo   groupbyinputrowresolver getexpression grpbyexpr
if  exprinfo    null
throw new semanticexception errormsg invalid_column getmsg grpbyexpr
groupbykeys add new exprnodecolumndesc exprinfo gettype    exprinfo
getinternalname       false
string field   getcolumninternalname i
outputcolumnnames add field
groupbyoutputrowresolver putexpression grpbyexpr
new columninfo field  exprinfo gettype    null  false
colexprmap put field  groupbykeys get groupbykeys size     1
// for each aggregation
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
assert  aggregationtrees    null
// get the last colname for the reduce key
// it represents the column name corresponding to distinct aggr, if any
string lastkeycolname   null
list<exprnodedesc> reducevalues   null
if  reducesinkoperatorinfo getconf   instanceof reducesinkdesc
list<string> inputkeycols     reducesinkdesc
reducesinkoperatorinfo getconf    getoutputkeycolumnnames
if  inputkeycols size   > 0
lastkeycolname   inputkeycols get inputkeycols size   1
reducevalues     reducesinkdesc reducesinkoperatorinfo getconf    getvaluecols
int numdistinctudfs   0
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
// this is the genericudaf name
string aggname   unescapeidentifier value getchild 0  gettext
boolean isdistinct   value gettype      hiveparser tok_functiondi
boolean isallcolumns   value gettype      hiveparser tok_functionstar
// convert children to aggparameters
arraylist<exprnodedesc> aggparameters   new arraylist<exprnodedesc>
// 0 is the function name
for  int i   1  i < value getchildcount    i
astnode paraexpr    astnode  value getchild i
columninfo paraexprinfo
groupbyinputrowresolver getexpression paraexpr
if  paraexprinfo    null
throw new semanticexception errormsg invalid_column getmsg paraexpr
string paraexpression   paraexprinfo getinternalname
assert  paraexpression    null
if  isdistinct    lastkeycolname    null
// if aggr is distinct, the parameter is name is constructed as
// key.lastkeycolname:<tag>._colx
paraexpression   utilities reducefield key name
lastkeycolname       numdistinctudfs
getcolumninternalname i 1
exprnodedesc expr   new exprnodecolumndesc paraexprinfo gettype
paraexpression  paraexprinfo gettabalias
paraexprinfo getisvirtualcol
exprnodedesc reducevalue   isconstantparameterinaggregationparameters
paraexprinfo getinternalname    reducevalues
if  reducevalue    null
// this parameter is a constant
expr   reducevalue
aggparameters add expr
if  isdistinct
numdistinctudfs
mode amode   groupbydescmodetoudafmode mode  isdistinct
genericudafevaluator genericudafevaluator   getgenericudafevaluator
aggname  aggparameters  value  isdistinct  isallcolumns
assert  genericudafevaluator    null
genericudafinfo udaf   getgenericudafinfo genericudafevaluator  amode
aggparameters
aggregations add new aggregationdesc aggname tolowercase
udaf genericudafevaluator  udaf convertedparameters  isdistinct
amode
string field   getcolumninternalname groupbykeys size
aggregations size     1
outputcolumnnames add field
groupbyoutputrowresolver putexpression value  new columninfo
field  udaf returntype     false
// save the evaluator so that it can be used by the next-stage
// groupbyoperators
if  genericudafevaluators    null
genericudafevaluators put entry getkey    genericudafevaluator
float groupbymemoryusage   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrhashmemory
float memorythreshold   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrmemorythreshold
operator op   putopinsertmap operatorfactory getandmakechild
new groupbydesc mode  outputcolumnnames  groupbykeys  aggregations
false groupbymemoryusage memorythreshold  null  false  0
new rowschema groupbyoutputrowresolver getcolumninfos
reducesinkoperatorinfo   groupbyoutputrowresolver
op setcolumnexprmap colexprmap
return op
// add the grouping set key to the group by operator.
// this is not the first group by operator, but it is a subsequent group by operator
// which is forwarding the grouping keys introduced by the grouping sets.
// for eg: consider: select key, value, count(1) from t group by key, value with rollup.
// assuming map-side aggregation and no skew, the plan would look like:
//
//   tablescan --> select --> groupby1 --> reducesink --> groupby2 --> select --> filesink
//
// this function is called for groupby2 to pass the additional grouping keys introduced by
// groupby1 for the grouping set (corresponding to the rollup).
private void addgroupingsetkey list<exprnodedesc> groupbykeys
rowresolver groupbyinputrowresolver
rowresolver groupbyoutputrowresolver
list<string> outputcolumnnames
map<string  exprnodedesc> colexprmap  throws semanticexception
// for grouping sets, add a dummy grouping key
string groupingsetcolumnname
groupbyinputrowresolver get null  virtualcolumn groupingid getname    getinternalname
exprnodedesc inputexpr   new exprnodecolumndesc typeinfofactory stringtypeinfo
groupingsetcolumnname  null  false
groupbykeys add inputexpr
string field   getcolumninternalname groupbykeys size     1
outputcolumnnames add field
groupbyoutputrowresolver put null  virtualcolumn groupingid getname
new columninfo
field
typeinfofactory stringtypeinfo

true
colexprmap put field  groupbykeys get groupbykeys size     1
// process grouping set for the reduce sink operator
// for eg: consider: select key, value, count(1) from t group by key, value with rollup.
// assuming map-side aggregation and no skew, the plan would look like:
//
//   tablescan --> select --> groupby1 --> reducesink --> groupby2 --> select --> filesink
//
// this function is called for reducesink to add the additional grouping keys introduced by
// groupby1 into the reduce keys.
private void processgroupingsetreducesinkoperator rowresolver reducesinkinputrowresolver
rowresolver reducesinkoutputrowresolver
list<exprnodedesc> reducekeys
list<string> outputkeycolumnnames
map<string  exprnodedesc> colexprmap  throws semanticexception
// add a key for reduce sink
string groupingsetcolumnname
reducesinkinputrowresolver get null  virtualcolumn groupingid getname    getinternalname
exprnodedesc inputexpr   new exprnodecolumndesc typeinfofactory stringtypeinfo
groupingsetcolumnname  null  false
reducekeys add inputexpr
outputkeycolumnnames add getcolumninternalname reducekeys size     1
string field   utilities reducefield key tostring
getcolumninternalname reducekeys size     1
columninfo colinfo   new columninfo field  reducekeys get
reducekeys size     1  gettypeinfo    null  true
reducesinkoutputrowresolver put null  virtualcolumn groupingid getname    colinfo
colexprmap put colinfo getinternalname    inputexpr
/**
* generate the groupbyoperator for the query block (parseinfo.getxxx(dest)).
* the new groupbyoperator will be a child of the reducesinkoperatorinfo.
*
* @param mode
*          the mode of the aggregation (mergepartial, partial2)
* @param genericudafevaluators
*          the mapping from aggregation stringtree to the
*          genericudafevaluator.
* @param distpartaggr
*          partial aggregation for distincts
* @return the new groupbyoperator
*/
@suppresswarnings
private operator gengroupbyplangroupbyoperator1 qbparseinfo parseinfo
string dest  operator reducesinkoperatorinfo  groupbydesc mode mode
map<string  genericudafevaluator> genericudafevaluators
boolean distpartagg
boolean groupingsetspresent  throws semanticexception
arraylist<string> outputcolumnnames   new arraylist<string>
rowresolver groupbyinputrowresolver   opparsectx
get reducesinkoperatorinfo  getrowresolver
rowresolver groupbyoutputrowresolver   new rowresolver
groupbyoutputrowresolver setisexprresolver true
arraylist<exprnodedesc> groupbykeys   new arraylist<exprnodedesc>
arraylist<aggregationdesc> aggregations   new arraylist<aggregationdesc>
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
columninfo exprinfo   groupbyinputrowresolver getexpression grpbyexpr
if  exprinfo    null
throw new semanticexception errormsg invalid_column getmsg grpbyexpr
groupbykeys add new exprnodecolumndesc exprinfo gettype    exprinfo
getinternalname    exprinfo gettabalias    exprinfo
getisvirtualcol
string field   getcolumninternalname i
outputcolumnnames add field
groupbyoutputrowresolver putexpression grpbyexpr
new columninfo field  exprinfo gettype       false
colexprmap put field  groupbykeys get groupbykeys size     1
// for grouping sets, add a dummy grouping key
if  groupingsetspresent
addgroupingsetkey
groupbykeys
groupbyinputrowresolver
groupbyoutputrowresolver
outputcolumnnames
colexprmap
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
// get the last colname for the reduce key
// it represents the column name corresponding to distinct aggr, if any
string lastkeycolname   null
list<exprnodedesc> reducevalues   null
if  reducesinkoperatorinfo getconf   instanceof reducesinkdesc
list<string> inputkeycols     reducesinkdesc
reducesinkoperatorinfo getconf    getoutputkeycolumnnames
if  inputkeycols size   > 0
lastkeycolname   inputkeycols get inputkeycols size   1
reducevalues     reducesinkdesc reducesinkoperatorinfo getconf    getvaluecols
int numdistinctudfs   0
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
string aggname   unescapeidentifier value getchild 0  gettext
arraylist<exprnodedesc> aggparameters   new arraylist<exprnodedesc>
boolean isdistinct    value gettype      hiveparser tok_functiondi
// if the function is distinct, partial aggregation has not been done on
// the client side.
// if distpartagg is set, the client is letting us know that partial
// aggregation has not been done.
// for eg: select a, count(b+c), count(distinct d+e) group by a
// for count(b+c), if partial aggregation has been performed, then we
// directly look for count(b+c).
// otherwise, we look for b+c.
// for distincts, partial aggregation is never performed on the client
// side, so always look for the parameters: d+e
boolean partialaggdone     distpartagg    isdistinct
if   partialaggdone
// 0 is the function name
for  int i   1  i < value getchildcount    i
astnode paraexpr    astnode  value getchild i
columninfo paraexprinfo
groupbyinputrowresolver getexpression paraexpr
if  paraexprinfo    null
throw new semanticexception errormsg invalid_column
getmsg paraexpr
string paraexpression   paraexprinfo getinternalname
assert  paraexpression    null
if  isdistinct    lastkeycolname    null
// if aggr is distinct, the parameter is name is constructed as
// key.lastkeycolname:<tag>._colx
paraexpression   utilities reducefield key name
lastkeycolname       numdistinctudfs
getcolumninternalname i 1
exprnodedesc expr   new exprnodecolumndesc paraexprinfo gettype
paraexpression  paraexprinfo gettabalias
paraexprinfo getisvirtualcol
exprnodedesc reducevalue   isconstantparameterinaggregationparameters
paraexprinfo getinternalname    reducevalues
if  reducevalue    null
// this parameter is a constant
expr   reducevalue
aggparameters add expr
else
columninfo paraexprinfo   groupbyinputrowresolver getexpression value
if  paraexprinfo    null
throw new semanticexception errormsg invalid_column getmsg value
string paraexpression   paraexprinfo getinternalname
assert  paraexpression    null
aggparameters add new exprnodecolumndesc paraexprinfo gettype
paraexpression  paraexprinfo gettabalias    paraexprinfo
getisvirtualcol
if  isdistinct
numdistinctudfs
boolean isallcolumns   value gettype      hiveparser tok_functionstar
mode amode   groupbydescmodetoudafmode mode  isdistinct
genericudafevaluator genericudafevaluator   null
// for distincts, partial aggregations have not been done
if  distpartagg
genericudafevaluator   getgenericudafevaluator aggname  aggparameters
value  isdistinct  isallcolumns
assert  genericudafevaluator    null
genericudafevaluators put entry getkey    genericudafevaluator
else
genericudafevaluator   genericudafevaluators get entry getkey
assert  genericudafevaluator    null
genericudafinfo udaf   getgenericudafinfo genericudafevaluator  amode
aggparameters
aggregations add new aggregationdesc aggname tolowercase
udaf genericudafevaluator  udaf convertedparameters
mode    groupbydesc mode final    isdistinct   amode
string field   getcolumninternalname groupbykeys size
aggregations size     1
outputcolumnnames add field
groupbyoutputrowresolver putexpression value  new columninfo
field  udaf returntype     false
float groupbymemoryusage   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrhashmemory
float memorythreshold   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrmemorythreshold
// nothing special needs to be done for grouping sets.
// this is the final group by operator, so multiple rows corresponding to the
// grouping sets have been generated upstream.
operator op   putopinsertmap operatorfactory getandmakechild
new groupbydesc mode  outputcolumnnames  groupbykeys  aggregations
distpartagg groupbymemoryusage memorythreshold  null  false  0
new rowschema groupbyoutputrowresolver getcolumninfos     reducesinkoperatorinfo
groupbyoutputrowresolver
op setcolumnexprmap colexprmap
return op
/**
* generate the map-side groupbyoperator for the query block
* (qb.getparseinfo().getxxx(dest)). the new groupbyoperator will be a child
* of the inputoperatorinfo.
*
* @param mode
*          the mode of the aggregation (hash)
* @param genericudafevaluators
*          if not null, this function will store the mapping from aggregation
*          stringtree to the genericudafevaluator in this parameter, so it
*          can be used in the next-stage groupby aggregations.
* @return the new groupbyoperator
*/
@suppresswarnings
private operator gengroupbyplanmapgroupbyoperator qb qb
string dest
list<astnode> grpbyexprs
operator inputoperatorinfo
groupbydesc mode mode
map<string  genericudafevaluator> genericudafevaluators
list<integer> groupingsetkeys
boolean groupingsetspresent  throws semanticexception
rowresolver groupbyinputrowresolver   opparsectx get inputoperatorinfo
getrowresolver
qbparseinfo parseinfo   qb getparseinfo
rowresolver groupbyoutputrowresolver   new rowresolver
groupbyoutputrowresolver setisexprresolver true
arraylist<exprnodedesc> groupbykeys   new arraylist<exprnodedesc>
arraylist<string> outputcolumnnames   new arraylist<string>
arraylist<aggregationdesc> aggregations   new arraylist<aggregationdesc>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
exprnodedesc grpbyexprnode   genexprnodedesc grpbyexpr
groupbyinputrowresolver
groupbykeys add grpbyexprnode
string field   getcolumninternalname i
outputcolumnnames add field
groupbyoutputrowresolver putexpression grpbyexpr
new columninfo field  grpbyexprnode gettypeinfo       false
colexprmap put field  groupbykeys get groupbykeys size     1
// the grouping set key is present after the grouping keys, before the distinct keys
int groupingsetsposition   groupbykeys size
// for grouping sets, add a dummy grouping key
// this dummy key needs to be added as a reduce key
// for eg: consider: select key, value, count(1) from t group by key, value with rollup.
// assuming map-side aggregation and no skew, the plan would look like:
//
//   tablescan --> select --> groupby1 --> reducesink --> groupby2 --> select --> filesink
//
// this function is called for groupby1 to create an additional grouping key
// for the grouping set (corresponding to the rollup).
if  groupingsetspresent
// the value for the constant does not matter. it is replaced by the grouping set
// value for the actual implementation
exprnodeconstantdesc constant   new exprnodeconstantdesc
groupbykeys add constant
string field   getcolumninternalname groupbykeys size     1
outputcolumnnames add field
groupbyoutputrowresolver put null  virtualcolumn groupingid getname
new columninfo
field
typeinfofactory stringtypeinfo

true
colexprmap put field  constant
// if there is a distinctfuncexp, add all parameters to the reducekeys.
if   parseinfo getdistinctfuncexprsforclause dest  isempty
list<astnode> list   parseinfo getdistinctfuncexprsforclause dest
for astnode value  list
// 0 is function name
for  int i   1  i < value getchildcount    i
astnode parameter    astnode  value getchild i
if  groupbyoutputrowresolver getexpression parameter     null
exprnodedesc distexprnode   genexprnodedesc parameter
groupbyinputrowresolver
groupbykeys add distexprnode
string field   getcolumninternalname groupbykeys size   1
outputcolumnnames add field
groupbyoutputrowresolver putexpression parameter  new columninfo
field  distexprnode gettypeinfo       false
colexprmap put field  groupbykeys get groupbykeys size     1
// for each aggregation
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
assert  aggregationtrees    null
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
string aggname   unescapeidentifier value getchild 0  gettext
arraylist<exprnodedesc> aggparameters   new arraylist<exprnodedesc>
new arraylist<class<?>>
// 0 is the function name
for  int i   1  i < value getchildcount    i
astnode paraexpr    astnode  value getchild i
exprnodedesc paraexprnode   genexprnodedesc paraexpr
groupbyinputrowresolver
aggparameters add paraexprnode
boolean isdistinct   value gettype      hiveparser tok_functiondi
boolean isallcolumns   value gettype      hiveparser tok_functionstar
mode amode   groupbydescmodetoudafmode mode  isdistinct
genericudafevaluator genericudafevaluator   getgenericudafevaluator
aggname  aggparameters  value  isdistinct  isallcolumns
assert  genericudafevaluator    null
genericudafinfo udaf   getgenericudafinfo genericudafevaluator  amode
aggparameters
aggregations add new aggregationdesc aggname tolowercase
udaf genericudafevaluator  udaf convertedparameters  isdistinct
amode
string field   getcolumninternalname groupbykeys size
aggregations size     1
outputcolumnnames add field
groupbyoutputrowresolver putexpression value  new columninfo
field  udaf returntype     false
// save the evaluator so that it can be used by the next-stage
// groupbyoperators
if  genericudafevaluators    null
genericudafevaluators put entry getkey    genericudafevaluator
float groupbymemoryusage   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrhashmemory
float memorythreshold   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrmemorythreshold
operator op   putopinsertmap operatorfactory getandmakechild
new groupbydesc mode  outputcolumnnames  groupbykeys  aggregations
false groupbymemoryusage memorythreshold
groupingsetkeys  groupingsetspresent  groupingsetsposition
new rowschema groupbyoutputrowresolver getcolumninfos
inputoperatorinfo   groupbyoutputrowresolver
op setcolumnexprmap colexprmap
return op
/**
* generate the reducesinkoperator for the group by query block
* (qb.getpartinfo().getxxx(dest)). the new reducesinkoperator will be a child
* of inputoperatorinfo.
*
* it will put all group by keys and the distinct field (if any) in the
* map-reduce sort key, and all other fields in the map-reduce value.
*
* @param numpartitionfields
*          the number of fields for map-reduce partitioning. this is usually
*          the number of fields in the group by keys.
* @return the new reducesinkoperator.
* @throws semanticexception
*/
@suppresswarnings
private operator gengroupbyplanreducesinkoperator qb qb
string dest
operator inputoperatorinfo
list<astnode> grpbyexprs
int numpartitionfields
boolean changenumpartitionfields
int numreducers
boolean mapaggrdone
boolean groupingsetspresent  throws semanticexception
rowresolver reducesinkinputrowresolver   opparsectx get inputoperatorinfo
getrowresolver
qbparseinfo parseinfo   qb getparseinfo
rowresolver reducesinkoutputrowresolver   new rowresolver
reducesinkoutputrowresolver setisexprresolver true
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
// pre-compute group-by keys and store in reducekeys
list<string> outputkeycolumnnames   new arraylist<string>
list<string> outputvaluecolumnnames   new arraylist<string>
arraylist<exprnodedesc> reducekeys   getreducekeysforreducesink grpbyexprs  dest
reducesinkinputrowresolver  reducesinkoutputrowresolver  outputkeycolumnnames
colexprmap
// add a key for reduce sink
if  groupingsetspresent
// process grouping set for the reduce sink operator
processgroupingsetreducesinkoperator
reducesinkinputrowresolver
reducesinkoutputrowresolver
reducekeys
outputkeycolumnnames
colexprmap
if  changenumpartitionfields
numpartitionfields
list<list<integer>> distinctcolindices   getdistinctcolindicesforreducesink parseinfo  dest
reducekeys  reducesinkinputrowresolver  reducesinkoutputrowresolver  outputkeycolumnnames
arraylist<exprnodedesc> reducevalues   new arraylist<exprnodedesc>
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
if   mapaggrdone
getreducevaluesforreducesinknomapagg parseinfo  dest  reducesinkinputrowresolver
reducesinkoutputrowresolver  outputvaluecolumnnames  reducevalues
else
// put partial aggregation results in reducevalues
int inputfield   reducekeys size
for  map entry<string  astnode> entry   aggregationtrees entryset
typeinfo type   reducesinkinputrowresolver getcolumninfos   get
inputfield  gettype
reducevalues add new exprnodecolumndesc type
getcolumninternalname inputfield      false
inputfield
outputvaluecolumnnames add getcolumninternalname reducevalues size     1
string field   utilities reducefield value tostring
getcolumninternalname reducevalues size     1
reducesinkoutputrowresolver putexpression entry getvalue
new columninfo field  type  null  false
reducesinkoperator rsop    reducesinkoperator  putopinsertmap
operatorfactory getandmakechild
planutils getreducesinkdesc reducekeys
groupingsetspresent ? grpbyexprs size     1   grpbyexprs size
reducevalues  distinctcolindices
outputkeycolumnnames  outputvaluecolumnnames  true   1  numpartitionfields
numreducers
new rowschema reducesinkoutputrowresolver getcolumninfos     inputoperatorinfo
reducesinkoutputrowresolver
rsop setcolumnexprmap colexprmap
return rsop
private arraylist<exprnodedesc> getreducekeysforreducesink list<astnode> grpbyexprs  string dest
rowresolver reducesinkinputrowresolver  rowresolver reducesinkoutputrowresolver
list<string> outputkeycolumnnames  map<string  exprnodedesc> colexprmap
throws semanticexception
arraylist<exprnodedesc> reducekeys   new arraylist<exprnodedesc>
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
exprnodedesc inputexpr   genexprnodedesc grpbyexpr
reducesinkinputrowresolver
reducekeys add inputexpr
if  reducesinkoutputrowresolver getexpression grpbyexpr     null
outputkeycolumnnames add getcolumninternalname reducekeys size     1
string field   utilities reducefield key tostring
getcolumninternalname reducekeys size     1
columninfo colinfo   new columninfo field  reducekeys get
reducekeys size     1  gettypeinfo    null  false
reducesinkoutputrowresolver putexpression grpbyexpr  colinfo
colexprmap put colinfo getinternalname    inputexpr
else
throw new semanticexception errormsg duplicate_groupby_key
getmsg grpbyexpr
return reducekeys
private list<list<integer>> getdistinctcolindicesforreducesink qbparseinfo parseinfo  string dest
list<exprnodedesc> reducekeys  rowresolver reducesinkinputrowresolver
rowresolver reducesinkoutputrowresolver  list<string> outputkeycolumnnames
throws semanticexception
list<list<integer>> distinctcolindices   new arraylist<list<integer>>
// if there is a distinctfuncexp, add all parameters to the reducekeys.
if   parseinfo getdistinctfuncexprsforclause dest  isempty
list<astnode> distfuncs   parseinfo getdistinctfuncexprsforclause dest
string colname   getcolumninternalname reducekeys size
outputkeycolumnnames add colname
for  int i   0  i < distfuncs size    i
astnode value   distfuncs get i
int numexprs   0
list<integer> distinctindices   new arraylist<integer>
// 0 is function name
for  int j   1  j < value getchildcount    j
astnode parameter    astnode  value getchild j
exprnodedesc expr   genexprnodedesc parameter  reducesinkinputrowresolver
// see if expr is already present in reducekeys.
// get index of expr in reducekeys
int ri
for  ri   0  ri < reducekeys size    ri
if  reducekeys get ri  getexprstring   equals expr getexprstring
break
// add the expr to reducekeys if it is not present
if  ri    reducekeys size
reducekeys add expr
// add the index of expr in reducekeys to distinctindices
distinctindices add ri
string name   getcolumninternalname numexprs
string field   utilities reducefield key tostring         colname
i
name
columninfo colinfo   new columninfo field  expr gettypeinfo    null  false
reducesinkoutputrowresolver putexpression parameter  colinfo
numexprs
distinctcolindices add distinctindices
return distinctcolindices
private void getreducevaluesforreducesinknomapagg qbparseinfo parseinfo  string dest
rowresolver reducesinkinputrowresolver  rowresolver reducesinkoutputrowresolver
list<string> outputvaluecolumnnames  arraylist<exprnodedesc> reducevalues
throws semanticexception
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
// put parameters to aggregations in reducevalues
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
// 0 is function name
for  int i   1  i < value getchildcount    i
astnode parameter    astnode  value getchild i
if  reducesinkoutputrowresolver getexpression parameter     null
reducevalues add genexprnodedesc parameter
reducesinkinputrowresolver
outputvaluecolumnnames
add getcolumninternalname reducevalues size     1
string field   utilities reducefield value tostring
getcolumninternalname reducevalues size     1
reducesinkoutputrowresolver putexpression parameter  new columninfo field
reducevalues get reducevalues size     1  gettypeinfo    null
false
@suppresswarnings
private operator gencommongroupbyplanreducesinkoperator qb qb  list<string> dests
operator inputoperatorinfo  throws semanticexception
rowresolver reducesinkinputrowresolver   opparsectx get inputoperatorinfo
getrowresolver
qbparseinfo parseinfo   qb getparseinfo
rowresolver reducesinkoutputrowresolver   new rowresolver
reducesinkoutputrowresolver setisexprresolver true
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
// the group by keys and distinct keys should be the same for all dests, so using the first
// one to produce these will be the same as using any other.
string dest   dests get 0
// pre-compute group-by keys and store in reducekeys
list<string> outputkeycolumnnames   new arraylist<string>
list<string> outputvaluecolumnnames   new arraylist<string>
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
arraylist<exprnodedesc> reducekeys   getreducekeysforreducesink grpbyexprs  dest
reducesinkinputrowresolver  reducesinkoutputrowresolver  outputkeycolumnnames
colexprmap
list<list<integer>> distinctcolindices   getdistinctcolindicesforreducesink parseinfo  dest
reducekeys  reducesinkinputrowresolver  reducesinkoutputrowresolver  outputkeycolumnnames
arraylist<exprnodedesc> reducevalues   new arraylist<exprnodedesc>
// the dests can have different non-distinct aggregations, so we have to iterate over all of
// them
for  string destination   dests
getreducevaluesforreducesinknomapagg parseinfo  destination  reducesinkinputrowresolver
reducesinkoutputrowresolver  outputvaluecolumnnames  reducevalues
// need to pass all of the columns used in the where clauses as reduce values
astnode whereclause   parseinfo getwhrforclause destination
if  whereclause    null
list<astnode> columnexprs
getcolumnexprsfromastnode whereclause  reducesinkinputrowresolver
for  int i   0  i < columnexprs size    i
astnode parameter   columnexprs get i
if  reducesinkoutputrowresolver getexpression parameter     null
reducevalues add genexprnodedesc parameter
reducesinkinputrowresolver
outputvaluecolumnnames
add getcolumninternalname reducevalues size     1
string field   utilities reducefield value tostring
getcolumninternalname reducevalues size     1
reducesinkoutputrowresolver putexpression parameter  new columninfo field
reducevalues get reducevalues size     1  gettypeinfo    null
false
reducesinkoperator rsop    reducesinkoperator  putopinsertmap
operatorfactory getandmakechild planutils getreducesinkdesc reducekeys
grpbyexprs size    reducevalues  distinctcolindices
outputkeycolumnnames  outputvaluecolumnnames  true   1  grpbyexprs size
1   new rowschema reducesinkoutputrowresolver
getcolumninfos     inputoperatorinfo   reducesinkoutputrowresolver
rsop setcolumnexprmap colexprmap
return rsop
/**
* given an astnode, it returns all of the descendant astnodes which represent column expressions
*
* @param node
* @param inputrr
* @return
* @throws semanticexception
*/
private list<astnode> getcolumnexprsfromastnode astnode node  rowresolver inputrr
throws semanticexception
list<astnode> nodes   new arraylist<astnode>
if  node getchildcount      0
return nodes
for  int i   0  i < node getchildcount    i
astnode child    astnode node getchild i
if  child gettype      hiveparser tok_table_or_col    child getchild 0     null
inputrr get null
basesemanticanalyzer unescapeidentifier child getchild 0  gettext        null
nodes add child
else
nodes addall getcolumnexprsfromastnode child  inputrr
return nodes
/**
* generate the second reducesinkoperator for the group by plan
* (parseinfo.getxxx(dest)). the new reducesinkoperator will be a child of
* groupbyoperatorinfo.
*
* the second reducesinkoperator will put the group by keys in the map-reduce
* sort key, and put the partial aggregation results in the map-reduce value.
*
* @param numpartitionfields
*          the number of fields in the map-reduce partition key. this should
*          always be the same as the number of group by keys. we should be
*          able to remove this parameter since in this phase there is no
*          distinct any more.
* @return the new reducesinkoperator.
* @throws semanticexception
*/
@suppresswarnings
private operator gengroupbyplanreducesinkoperator2mr qbparseinfo parseinfo
string dest
operator groupbyoperatorinfo
int numpartitionfields
int numreducers
boolean groupingsetspresent  throws semanticexception
rowresolver reducesinkinputrowresolver2   opparsectx get
groupbyoperatorinfo  getrowresolver
rowresolver reducesinkoutputrowresolver2   new rowresolver
reducesinkoutputrowresolver2 setisexprresolver true
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
arraylist<exprnodedesc> reducekeys   new arraylist<exprnodedesc>
arraylist<string> outputcolumnnames   new arraylist<string>
// get group-by keys and store in reducekeys
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
string field   getcolumninternalname i
outputcolumnnames add field
typeinfo typeinfo   reducesinkinputrowresolver2 getexpression
grpbyexpr  gettype
exprnodecolumndesc inputexpr   new exprnodecolumndesc typeinfo  field
false
reducekeys add inputexpr
columninfo colinfo   new columninfo utilities reducefield key tostring
field  typeinfo     false
reducesinkoutputrowresolver2 putexpression grpbyexpr  colinfo
colexprmap put colinfo getinternalname    inputexpr
// add a key for reduce sink
if  groupingsetspresent
// note that partitioning fields dont need to change, since it is either
// partitioned randomly, or by all grouping keys + distinct keys
processgroupingsetreducesinkoperator
reducesinkinputrowresolver2
reducesinkoutputrowresolver2
reducekeys
outputcolumnnames
colexprmap
// get partial aggregation results and store in reducevalues
arraylist<exprnodedesc> reducevalues   new arraylist<exprnodedesc>
int inputfield   reducekeys size
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
for  map entry<string  astnode> entry   aggregationtrees entryset
string field   getcolumninternalname inputfield
astnode t   entry getvalue
typeinfo typeinfo   reducesinkinputrowresolver2 getexpression t
gettype
reducevalues add new exprnodecolumndesc typeinfo  field     false
inputfield
string col   getcolumninternalname reducevalues size     1
outputcolumnnames add col
reducesinkoutputrowresolver2 putexpression t  new columninfo
utilities reducefield value tostring         col  typeinfo
false
reducesinkoperator rsop    reducesinkoperator  putopinsertmap
operatorfactory getandmakechild planutils getreducesinkdesc reducekeys
reducevalues  outputcolumnnames  true   1  numpartitionfields
numreducers   new rowschema reducesinkoutputrowresolver2
getcolumninfos     groupbyoperatorinfo
reducesinkoutputrowresolver2
rsop setcolumnexprmap colexprmap
return rsop
/**
* generate the second groupbyoperator for the group by plan
* (parseinfo.getxxx(dest)). the new groupbyoperator will do the second
* aggregation based on the partial aggregation results.
*
* @param mode
*          the mode of aggregation (final)
* @param genericudafevaluators
*          the mapping from aggregation stringtree to the
*          genericudafevaluator.
* @return the new groupbyoperator
* @throws semanticexception
*/
@suppresswarnings
private operator gengroupbyplangroupbyoperator2mr qbparseinfo parseinfo
string dest
operator reducesinkoperatorinfo2
groupbydesc mode mode
map<string  genericudafevaluator> genericudafevaluators
boolean groupingsetspresent  throws semanticexception
rowresolver groupbyinputrowresolver2   opparsectx get
reducesinkoperatorinfo2  getrowresolver
rowresolver groupbyoutputrowresolver2   new rowresolver
groupbyoutputrowresolver2 setisexprresolver true
arraylist<exprnodedesc> groupbykeys   new arraylist<exprnodedesc>
arraylist<aggregationdesc> aggregations   new arraylist<aggregationdesc>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
arraylist<string> outputcolumnnames   new arraylist<string>
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
columninfo exprinfo   groupbyinputrowresolver2 getexpression grpbyexpr
if  exprinfo    null
throw new semanticexception errormsg invalid_column getmsg grpbyexpr
string expression   exprinfo getinternalname
groupbykeys add new exprnodecolumndesc exprinfo gettype    expression
exprinfo gettabalias    exprinfo getisvirtualcol
string field   getcolumninternalname i
outputcolumnnames add field
groupbyoutputrowresolver2 putexpression grpbyexpr
new columninfo field  exprinfo gettype       false
colexprmap put field  groupbykeys get groupbykeys size     1
// for grouping sets, add a dummy grouping key
if  groupingsetspresent
addgroupingsetkey
groupbykeys
groupbyinputrowresolver2
groupbyoutputrowresolver2
outputcolumnnames
colexprmap
hashmap<string  astnode> aggregationtrees   parseinfo
getaggregationexprsforclause dest
for  map entry<string  astnode> entry   aggregationtrees entryset
arraylist<exprnodedesc> aggparameters   new arraylist<exprnodedesc>
astnode value   entry getvalue
columninfo paraexprinfo   groupbyinputrowresolver2 getexpression value
if  paraexprinfo    null
throw new semanticexception errormsg invalid_column getmsg value
string paraexpression   paraexprinfo getinternalname
assert  paraexpression    null
aggparameters add new exprnodecolumndesc paraexprinfo gettype
paraexpression  paraexprinfo gettabalias    paraexprinfo
getisvirtualcol
string aggname   unescapeidentifier value getchild 0  gettext
boolean isdistinct   value gettype      hiveparser tok_functiondi
boolean isstar   value gettype      hiveparser tok_functionstar
mode amode   groupbydescmodetoudafmode mode  isdistinct
genericudafevaluator genericudafevaluator   genericudafevaluators
get entry getkey
assert  genericudafevaluator    null
genericudafinfo udaf   getgenericudafinfo genericudafevaluator  amode
aggparameters
aggregations
add new aggregationdesc
aggname tolowercase
udaf genericudafevaluator
udaf convertedparameters
mode    groupbydesc mode final    value gettoken   gettype
hiveparser tok_functiondi
amode
string field   getcolumninternalname groupbykeys size
aggregations size     1
outputcolumnnames add field
groupbyoutputrowresolver2 putexpression value  new columninfo
field  udaf returntype     false
float groupbymemoryusage   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrhashmemory
float memorythreshold   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrmemorythreshold
operator op   putopinsertmap operatorfactory getandmakechild
new groupbydesc mode  outputcolumnnames  groupbykeys  aggregations
false groupbymemoryusage memorythreshold  null  false  0
new rowschema groupbyoutputrowresolver2 getcolumninfos
reducesinkoperatorinfo2   groupbyoutputrowresolver2
op setcolumnexprmap colexprmap
return op
/**
* generate a group-by plan using a single map-reduce job (3 operators will be
* inserted):
*
* reducesink ( keys = (k1_exp, k2_exp, distinct_exp), values = (a1_exp,
* a2_exp) ) sortgroupby (keys = (key.0,key.1), aggregations =
* (count_distinct(key.2), sum(value.0), count(value.1))) select (final
* selects).
*
* @param dest
* @param qb
* @param input
* @return
* @throws semanticexception
*
*           generate a group-by plan using 1 map-reduce job. spray by the
*           group by key, and sort by the distinct key (if any), and compute
*           aggregates * the aggregation evaluation functions are as
*           follows: partitioning key: grouping key
*
*           sorting key: grouping key if no distinct grouping + distinct key
*           if distinct
*
*           reducer: iterate/merge (mode = complete)
**/
@suppresswarnings
private operator gengroupbyplan1mr string dest  qb qb  operator input
throws semanticexception
qbparseinfo parseinfo   qb getparseinfo
int numreducers    1
objectpair<list<astnode>  list<integer>> grpbyexprsgroupingsets
getgroupbygroupingsetsforclause parseinfo  dest
list<astnode> grpbyexprs   grpbyexprsgroupingsets getfirst
list<integer> groupingsets   grpbyexprsgroupingsets getsecond
if  grpbyexprs isempty
numreducers   1
// grouping sets are not allowed
if   groupingsets isempty
throw new semanticexception errormsg hive_grouping_sets_aggr_nomapaggr getmsg
// ////// 1. generate reducesinkoperator
operator reducesinkoperatorinfo
gengroupbyplanreducesinkoperator qb
dest
input
grpbyexprs
grpbyexprs size
false
numreducers
false
false
// ////// 2. generate groupbyoperator
operator groupbyoperatorinfo   gengroupbyplangroupbyoperator parseinfo
dest  reducesinkoperatorinfo  groupbydesc mode complete  null
return groupbyoperatorinfo
@suppresswarnings
private operator gengroupbyplan1mrmultireducegb list<string> dests  qb qb  operator input
throws semanticexception
qbparseinfo parseinfo   qb getparseinfo
exprnodedesc previous   null
operator selectinput   input
// in order to facilitate partition pruning, or the where clauses together and put them at the
// top of the operator tree, this could also reduce the amount of data going to the reducer
list<exprnodedesc exprnodedescequalitywrapper> whereexpressions
new arraylist<exprnodedesc exprnodedescequalitywrapper>
for  string dest   dests
astnode whereexpr   parseinfo getwhrforclause dest
if  whereexpr    null
opparsecontext inputctx   opparsectx get input
rowresolver inputrr   inputctx getrowresolver
exprnodedesc current   genexprnodedesc  astnode whereexpr getchild 0   inputrr
// check the list of where expressions already added so they aren't duplicated
exprnodedesc exprnodedescequalitywrapper currentwrapped
new exprnodedesc exprnodedescequalitywrapper current
if   whereexpressions contains currentwrapped
whereexpressions add currentwrapped
else
continue
if  previous    null
// if this is the first expression
previous   current
continue
genericudfopor or   new genericudfopor
list<exprnodedesc> expressions   new arraylist<exprnodedesc> 2
expressions add previous
expressions add current
exprnodedesc orexpr
new exprnodegenericfuncdesc typeinfofactory booleantypeinfo  or  expressions
previous   orexpr
else
// if an expression does not have a where clause, there can be no common filter
previous   null
break
if  previous    null
opparsecontext inputctx   opparsectx get input
rowresolver inputrr   inputctx getrowresolver
filterdesc orfilterdesc   new filterdesc previous  false
selectinput   putopinsertmap operatorfactory getandmakechild
orfilterdesc  new rowschema
inputrr getcolumninfos     input   inputrr
// insert a select operator here used by the columnpruner to reduce
// the data to shuffle
operator select   insertselectallplanforgroupby selectinput
// generate reducesinkoperator
operator reducesinkoperatorinfo   gencommongroupbyplanreducesinkoperator qb  dests  select
// it is assumed throughout the code that a reducer has a single child, add a
// forwardoperator so that we can add multiple filter/group by operators as children
rowresolver reducesinkoperatorinforr   opparsectx get reducesinkoperatorinfo  getrowresolver
operator forwardop   putopinsertmap operatorfactory getandmakechild new forwarddesc
new rowschema reducesinkoperatorinforr getcolumninfos     reducesinkoperatorinfo
reducesinkoperatorinforr
operator curr   forwardop
for  string dest   dests
curr   forwardop
if  parseinfo getwhrforclause dest     null
curr   genfilterplan dest  qb  forwardop
// generate groupbyoperator
operator groupbyoperatorinfo   gengroupbyplangroupbyoperator parseinfo
dest  curr  groupbydesc mode complete  null
curr   genpostgroupbybodyplan groupbyoperatorinfo  dest  qb
return curr
static arraylist<genericudafevaluator> getudafevaluators
arraylist<aggregationdesc> aggs
arraylist<genericudafevaluator> result   new arraylist<genericudafevaluator>
for  int i   0  i < aggs size    i
result add aggs get i  getgenericudafevaluator
return result
/**
* generate a multi group-by plan using a 2 map-reduce jobs.
*
* @param dest
* @param qb
* @param input
* @return
* @throws semanticexception
*
*           generate a group-by plan using a 2 map-reduce jobs. spray by the
*           distinct key in hope of getting a uniform distribution, and
*           compute partial aggregates by the grouping key. evaluate partial
*           aggregates first, and spray by the grouping key to compute actual
*           aggregates in the second phase. the agggregation evaluation
*           functions are as follows: partitioning key: distinct key
*
*           sorting key: distinct key
*
*           reducer: iterate/terminatepartial (mode = partial1)
*
*           stage 2
*
*           partitioning key: grouping key
*
*           sorting key: grouping key
*
*           reducer: merge/terminate (mode = final)
*/
@suppresswarnings
private operator gengroupbyplan2mrmultigroupby string dest  qb qb
operator input  throws semanticexception
// ////// generate groupbyoperator for a map-side partial aggregation
map<string  genericudafevaluator> genericudafevaluators
new linkedhashmap<string  genericudafevaluator>
qbparseinfo parseinfo   qb getparseinfo
// ////// 2. generate groupbyoperator
operator groupbyoperatorinfo   gengroupbyplangroupbyoperator1 parseinfo
dest  input  groupbydesc mode hash  genericudafevaluators  true  false
int numreducers    1
list<astnode> grpbyexprs   getgroupbyforclause parseinfo  dest
// ////// 3. generate reducesinkoperator2
operator reducesinkoperatorinfo2   gengroupbyplanreducesinkoperator2mr
parseinfo  dest  groupbyoperatorinfo  grpbyexprs size    numreducers  false
// ////// 4. generate groupbyoperator2
operator groupbyoperatorinfo2   gengroupbyplangroupbyoperator2mr parseinfo
dest  reducesinkoperatorinfo2  groupbydesc mode final
genericudafevaluators  false
return groupbyoperatorinfo2
/**
* generate a group-by plan using a 2 map-reduce jobs (5 operators will be
* inserted):
*
* reducesink ( keys = (k1_exp, k2_exp, distinct_exp), values = (a1_exp,
* a2_exp) ) note: if distinct_exp is null, partition by rand() sortgroupby
* (keys = (key.0,key.1), aggregations = (count_distinct(key.2), sum(value.0),
* count(value.1))) reducesink ( keys = (0,1), values=(2,3,4)) sortgroupby
* (keys = (key.0,key.1), aggregations = (sum(value.0), sum(value.1),
* sum(value.2))) select (final selects).
*
* @param dest
* @param qb
* @param input
* @return
* @throws semanticexception
*
*           generate a group-by plan using a 2 map-reduce jobs. spray by the
*           grouping key and distinct key (or a random number, if no distinct
*           is present) in hope of getting a uniform distribution, and
*           compute partial aggregates grouped by the reduction key (grouping
*           key + distinct key). evaluate partial aggregates first, and spray
*           by the grouping key to compute actual aggregates in the second
*           phase. the agggregation evaluation functions are as follows:
*           partitioning key: random() if no distinct grouping + distinct key
*           if distinct
*
*           sorting key: grouping key if no distinct grouping + distinct key
*           if distinct
*
*           reducer: iterate/terminatepartial (mode = partial1)
*
*           stage 2
*
*           partitioning key: grouping key
*
*           sorting key: grouping key if no distinct grouping + distinct key
*           if distinct
*
*           reducer: merge/terminate (mode = final)
*/
@suppresswarnings
private operator gengroupbyplan2mr string dest  qb qb  operator input
throws semanticexception
qbparseinfo parseinfo   qb getparseinfo
objectpair<list<astnode>  list<integer>> grpbyexprsgroupingsets
getgroupbygroupingsetsforclause parseinfo  dest
list<astnode> grpbyexprs   grpbyexprsgroupingsets getfirst
list<integer> groupingsets   grpbyexprsgroupingsets getsecond
// grouping sets are not allowed
// this restriction can be lifted in future.
// hive-3508 has been filed for this
if   groupingsets isempty
throw new semanticexception errormsg hive_grouping_sets_aggr_nomapaggr getmsg
// ////// 1. generate reducesinkoperator
// there is a special case when we want the rows to be randomly distributed
// to
// reducers for load balancing problem. that happens when there is no
// distinct
// operator. we set the numpartitioncolumns to -1 for this purpose. this is
// captured by writablecomparablehiveobject.hashcode() function.
operator reducesinkoperatorinfo
gengroupbyplanreducesinkoperator qb
dest
input
grpbyexprs
parseinfo getdistinctfuncexprsforclause dest  isempty   ?  1   integer max_value
false
1
false
false
// ////// 2. generate groupbyoperator
map<string  genericudafevaluator> genericudafevaluators
new linkedhashmap<string  genericudafevaluator>
groupbyoperator groupbyoperatorinfo    groupbyoperator  gengroupbyplangroupbyoperator
parseinfo  dest  reducesinkoperatorinfo  groupbydesc mode partial1
genericudafevaluators
int numreducers    1
if  grpbyexprs isempty
numreducers   1
// ////// 3. generate reducesinkoperator2
operator reducesinkoperatorinfo2   gengroupbyplanreducesinkoperator2mr
parseinfo  dest  groupbyoperatorinfo  grpbyexprs size    numreducers  false
// ////// 4. generate groupbyoperator2
operator groupbyoperatorinfo2   gengroupbyplangroupbyoperator2mr parseinfo
dest  reducesinkoperatorinfo2  groupbydesc mode final
genericudafevaluators  false
return groupbyoperatorinfo2
private boolean optimizemapaggrgroupby string dest  qb qb
list<astnode> grpbyexprs   getgroupbyforclause qb getparseinfo    dest
if   grpbyexprs    null      grpbyexprs isempty
return false
if   qb getparseinfo   getdistinctfuncexprsforclause dest  isempty
return false
return true
static private void extractcolumns set<string> colnamesexprs
exprnodedesc exprnode  throws semanticexception
if  exprnode instanceof exprnodecolumndesc
colnamesexprs add   exprnodecolumndesc exprnode  getcolumn
return
if  exprnode instanceof exprnodegenericfuncdesc
exprnodegenericfuncdesc funcdesc    exprnodegenericfuncdesc exprnode
for  exprnodedesc childexpr  funcdesc getchildexprs
extractcolumns colnamesexprs  childexpr
static private boolean hascommonelement set<string> set1  set<string> set2
for  string elem1   set1
if  set2 contains elem1
return true
return false
private void checkexpressionsforgroupingset list<astnode> grpbyexprs
list<astnode> distinctgrpbyexprs
map<string  astnode> aggregationtrees
rowresolver   inputrowresolver  throws semanticexception
set<string> colnamesgroupbyexprs   new hashset<string>
set<string> colnamesgroupbydistinctexprs   new hashset<string>
set<string> colnamesaggregateparameters   new hashset<string>
// the columns in the group by expressions should not intersect with the columns in the
// distinct expressions
for  astnode grpbyexpr   grpbyexprs
extractcolumns colnamesgroupbyexprs  genexprnodedesc grpbyexpr  inputrowresolver
// if there is a distinctfuncexp, add all parameters to the reducekeys.
if   distinctgrpbyexprs isempty
for astnode value  distinctgrpbyexprs
// 0 is function name
for  int i   1  i < value getchildcount    i
astnode parameter    astnode  value getchild i
exprnodedesc distexprnode   genexprnodedesc parameter  inputrowresolver
// extract all the columns
extractcolumns colnamesgroupbydistinctexprs  distexprnode
if  hascommonelement colnamesgroupbyexprs  colnamesgroupbydistinctexprs
throw
new semanticexception errormsg hive_grouping_sets_aggr_expression_invalid getmsg
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
arraylist<exprnodedesc> aggparameters   new arraylist<exprnodedesc>
// 0 is the function name
for  int i   1  i < value getchildcount    i
astnode paraexpr    astnode  value getchild i
exprnodedesc paraexprnode   genexprnodedesc paraexpr  inputrowresolver
// extract all the columns
extractcolumns colnamesaggregateparameters  paraexprnode
if  hascommonelement colnamesgroupbyexprs  colnamesaggregateparameters
throw
new semanticexception errormsg hive_grouping_sets_aggr_expression_invalid getmsg
/**
* generate a group-by plan using 1 map-reduce job. first perform a map-side
* partial aggregation (to reduce the amount of data), at this point of time,
* we may turn off map-side partial aggregation based on its performance. then
* spray by the group by key, and sort by the distinct key (if any), and
* compute aggregates based on actual aggregates
*
* the agggregation evaluation functions are as follows: mapper:
* iterate/terminatepartial (mode = hash)
*
* partitioning key: grouping key
*
* sorting key: grouping key if no distinct grouping + distinct key if
* distinct
*
* reducer: iterate/terminate if distinct merge/terminate if no distinct (mode
* = mergepartial)
*/
@suppresswarnings
private operator gengroupbyplanmapaggr1mr string dest  qb qb
operator inputoperatorinfo  throws semanticexception
qbparseinfo parseinfo   qb getparseinfo
objectpair<list<astnode>  list<integer>> grpbyexprsgroupingsets
getgroupbygroupingsetsforclause parseinfo  dest
list<astnode> grpbyexprs   grpbyexprsgroupingsets getfirst
list<integer> groupingsets   grpbyexprsgroupingsets getsecond
boolean groupingsetspresent    groupingsets isempty
if  groupingsetspresent
checkexpressionsforgroupingset grpbyexprs
parseinfo getdistinctfuncexprsforclause dest
parseinfo getaggregationexprsforclause dest
opparsectx get inputoperatorinfo  getrowresolver
// ////// generate groupbyoperator for a map-side partial aggregation
map<string  genericudafevaluator> genericudafevaluators
new linkedhashmap<string  genericudafevaluator>
groupbyoperator groupbyoperatorinfo
groupbyoperator  gengroupbyplanmapgroupbyoperator
qb
dest
grpbyexprs
inputoperatorinfo
groupbydesc mode hash
genericudafevaluators
groupingsets
groupingsetspresent
groupoptoinputtables put groupbyoperatorinfo  opparsectx get
inputoperatorinfo  getrowresolver   gettablenames
int numreducers    1
// optimize the scenario when there are no grouping keys - only 1 reducer is
// needed
if  grpbyexprs isempty
numreducers   1
// ////// generate reducesink operator
operator reducesinkoperatorinfo
gengroupbyplanreducesinkoperator qb
dest
groupbyoperatorinfo
grpbyexprs
grpbyexprs size
true
numreducers
true
groupingsetspresent
// this is a 1-stage map-reduce processing of the groupby. tha map-side
// aggregates was just used to
// reduce output data. in case of distincts, partial results are not used,
// and so iterate is again
// invoked on the reducer. in case of non-distincts, partial results are
// used, and merge is invoked
// on the reducer.
return gengroupbyplangroupbyoperator1 parseinfo  dest
reducesinkoperatorinfo  groupbydesc mode mergepartial
genericudafevaluators  false  groupingsetspresent
/**
* generate a group-by plan using a 2 map-reduce jobs. however, only 1
* group-by plan is generated if the query involves no grouping key and no
* distincts. in that case, the plan is same as generated by
* gengroupbyplanmapaggr1mr. otherwise, the following plan is generated: first
* perform a map side partial aggregation (to reduce the amount of data). then
* spray by the grouping key and distinct key (or a random number, if no
* distinct is present) in hope of getting a uniform distribution, and compute
* partial aggregates grouped by the reduction key (grouping key + distinct
* key). evaluate partial aggregates first, and spray by the grouping key to
* compute actual aggregates in the second phase. the agggregation evaluation
* functions are as follows: mapper: iterate/terminatepartial (mode = hash)
*
* partitioning key: random() if no distinct grouping + distinct key if
* distinct
*
* sorting key: grouping key if no distinct grouping + distinct key if
* distinct
*
* reducer: iterate/terminatepartial if distinct merge/terminatepartial if no
* distinct (mode = mergepartial)
*
* stage 2
*
* partitioining key: grouping key
*
* sorting key: grouping key if no distinct grouping + distinct key if
* distinct
*
* reducer: merge/terminate (mode = final)
*/
@suppresswarnings
private operator gengroupbyplanmapaggr2mr string dest  qb qb
operator inputoperatorinfo  throws semanticexception
qbparseinfo parseinfo   qb getparseinfo
objectpair<list<astnode>  list<integer>> grpbyexprsgroupingsets
getgroupbygroupingsetsforclause parseinfo  dest
list<astnode> grpbyexprs   grpbyexprsgroupingsets getfirst
list<integer> groupingsets   grpbyexprsgroupingsets getsecond
boolean groupingsetspresent    groupingsets isempty
if  groupingsetspresent
checkexpressionsforgroupingset grpbyexprs
parseinfo getdistinctfuncexprsforclause dest
parseinfo getaggregationexprsforclause dest
opparsectx get inputoperatorinfo  getrowresolver
// ////// generate groupbyoperator for a map-side partial aggregation
map<string  genericudafevaluator> genericudafevaluators
new linkedhashmap<string  genericudafevaluator>
groupbyoperator groupbyoperatorinfo
groupbyoperator  gengroupbyplanmapgroupbyoperator
qb  dest  grpbyexprs  inputoperatorinfo  groupbydesc mode hash
genericudafevaluators  groupingsets  groupingsetspresent
groupoptoinputtables put groupbyoperatorinfo  opparsectx get
inputoperatorinfo  getrowresolver   gettablenames
// optimize the scenario when there are no grouping keys and no distinct - 2
// map-reduce jobs are not needed
// for eg: select count(1) from t where t.ds = ....
if   optimizemapaggrgroupby dest  qb
list<astnode> distinctfuncexprs   parseinfo getdistinctfuncexprsforclause dest
// ////// generate reducesink operator
operator reducesinkoperatorinfo
gengroupbyplanreducesinkoperator qb
dest
groupbyoperatorinfo
grpbyexprs
distinctfuncexprs isempty   ?  1   integer max_value
false
1
true
groupingsetspresent
// ////// generate groupbyoperator for a partial aggregation
operator groupbyoperatorinfo2   gengroupbyplangroupbyoperator1 parseinfo
dest  reducesinkoperatorinfo  groupbydesc mode partials
genericudafevaluators  false  groupingsetspresent
int numreducers    1
if  grpbyexprs isempty
numreducers   1
// ////// generate reducesinkoperator2
operator reducesinkoperatorinfo2   gengroupbyplanreducesinkoperator2mr
parseinfo  dest  groupbyoperatorinfo2  grpbyexprs size    numreducers
groupingsetspresent
// ////// generate groupbyoperator3
return gengroupbyplangroupbyoperator2mr parseinfo  dest
reducesinkoperatorinfo2  groupbydesc mode final
genericudafevaluators  groupingsetspresent
else
// if there are no grouping keys, grouping sets cannot be present
assert  groupingsetspresent
// ////// generate reducesink operator
operator reducesinkoperatorinfo
gengroupbyplanreducesinkoperator qb
dest
groupbyoperatorinfo
grpbyexprs
grpbyexprs size
false
1
true
groupingsetspresent
return gengroupbyplangroupbyoperator2mr parseinfo  dest
reducesinkoperatorinfo  groupbydesc mode final  genericudafevaluators  false
@suppresswarnings
private operator genconversionops string dest  qb qb  operator input
throws semanticexception
integer dest_type   qb getmetadata   getdesttypeforalias dest
switch  dest_type intvalue
case qbmetadata dest_table
qb getmetadata   getdesttableforalias dest
break
case qbmetadata dest_partition
qb getmetadata   getdestpartitionforalias dest  gettable
break
default
return input
return input
private int getreducersbucketing int totalfiles  int maxreducers
int numfiles   totalfiles maxreducers
while  true
if  totalfiles%numfiles    0
return totalfiles numfiles
numfiles
private static class sortbucketrsctx
arraylist<exprnodedesc> partncols
boolean multifilespray
int     numfiles
int     totalfiles
public sortbucketrsctx
partncols   null
multifilespray   false
numfiles   1
totalfiles   1
/**
* @return the partncols
*/
public arraylist<exprnodedesc> getpartncols
return partncols
/**
* @param partncols the partncols to set
*/
public void setpartncols arraylist<exprnodedesc> partncols
this partncols   partncols
/**
* @return the multifilespray
*/
public boolean ismultifilespray
return multifilespray
/**
* @param multifilespray the multifilespray to set
*/
public void setmultifilespray boolean multifilespray
this multifilespray   multifilespray
/**
* @return the numfiles
*/
public int getnumfiles
return numfiles
/**
* @param numfiles the numfiles to set
*/
public void setnumfiles int numfiles
this numfiles   numfiles
/**
* @return the totalfiles
*/
public int gettotalfiles
return totalfiles
/**
* @param totalfiles the totalfiles to set
*/
public void settotalfiles int totalfiles
this totalfiles   totalfiles
@suppresswarnings
private operator genbucketingsortingdest string dest  operator input  qb qb  tabledesc table_desc
table dest_tab  sortbucketrsctx ctx
throws semanticexception
// if the table is bucketed, and bucketing is enforced, do the following:
// if the number of buckets is smaller than the number of maximum reducers,
// create those many reducers.
// if not, create a multifilesink instead of filesink - the multifilesink will
// spray the data into multiple buckets. that way, we can support a very large
// number of buckets without needing a very large number of reducers.
boolean enforcebucketing   false
boolean enforcesorting     false
arraylist<exprnodedesc> partncols   new arraylist<exprnodedesc>
arraylist<exprnodedesc> partncolsnoconvert   new arraylist<exprnodedesc>
arraylist<exprnodedesc> sortcols    new arraylist<exprnodedesc>
arraylist<integer> sortorders   new arraylist<integer>
boolean multifilespray   false
int     numfiles   1
int     totalfiles   1
if   dest_tab getnumbuckets   > 0
conf getboolvar hiveconf confvars hiveenforcebucketing
enforcebucketing   true
partncols   getparitioncolsfrombucketcols dest  qb  dest_tab  table_desc  input  true
partncolsnoconvert   getparitioncolsfrombucketcols dest  qb  dest_tab  table_desc  input  false
if   dest_tab getsortcols      null
dest_tab getsortcols   size   > 0
conf getboolvar hiveconf confvars hiveenforcesorting
enforcesorting   true
sortcols   getsortcols dest  qb  dest_tab  table_desc  input  true
sortorders   getsortorders dest  qb  dest_tab  input
if   enforcebucketing
partncols   sortcols
partncolsnoconvert   getsortcols dest  qb  dest_tab  table_desc  input  false
if  enforcebucketing    enforcesorting
int maxreducers   conf getintvar hiveconf confvars maxreducers
if  conf getintvar hiveconf confvars hadoopnumreducers  > 0
maxreducers   conf getintvar hiveconf confvars hadoopnumreducers
int numbuckets    dest_tab getnumbuckets
if  numbuckets > maxreducers
multifilespray   true
totalfiles   numbuckets
if  totalfiles % maxreducers    0
numfiles   totalfiles   maxreducers
else
// find the number of reducers such that it is a divisor of totalfiles
maxreducers   getreducersbucketing totalfiles  maxreducers
numfiles   totalfiles maxreducers
else
maxreducers   numbuckets
input   genreducesinkplanforsortingbucketing dest_tab  input
sortcols  sortorders  partncols  maxreducers
ctx setmultifilespray multifilespray
ctx setnumfiles numfiles
ctx setpartncols partncolsnoconvert
ctx settotalfiles totalfiles
//disable "merge mapfiles" and "merge mapred files".
hiveconf setboolvar conf  hiveconf confvars hivemergemapfiles  false
hiveconf setboolvar conf  hiveconf confvars hivemergemapredfiles  false
return input
/**
* check for hold_ddltime hint.
* @param qb
* @return true if hold_ddltime is set, false otherwise.
*/
private boolean checkholdddltime qb qb
astnode hints   qb getparseinfo   gethints
if  hints    null
return false
for  int pos   0  pos < hints getchildcount    pos
astnode hint    astnode  hints getchild pos
if    astnode  hint getchild 0   gettoken   gettype      hiveparser tok_hold_ddltime
return true
return false
@suppresswarnings
private operator genfilesinkplan string dest  qb qb  operator input
throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
qbmetadata qbm   qb getmetadata
integer dest_type   qbm getdesttypeforalias dest
table dest_tab   null         destination table if any
partition dest_part   null    destination partition if any
string querytmpdir   null     the intermediate destination directory
path dest_path   null     the final destination directory
tabledesc table_desc   null
int currenttableid   0
boolean islocal   false
sortbucketrsctx rsctx   new sortbucketrsctx
dynamicpartitionctx dpctx   null
loadtabledesc ltd   null
boolean holdddltime   checkholdddltime qb
switch  dest_type intvalue
case qbmetadata dest_table
dest_tab   qbm getdesttableforalias dest
// is the user trying to insert into a external tables
if    conf getboolvar hiveconf confvars hive_insert_into_external_tables
dest_tab gettabletype   equals tabletype external_table
throw new semanticexception
errormsg insert_external_table getmsg dest_tab gettablename
map<string  string> partspec   qbm getpartspecforalias dest
dest_path   dest_tab getpath
// check for partition
list<fieldschema> parts   dest_tab getpartitionkeys
if  parts    null    parts size   > 0       table is partitioned
if  partspec   null    partspec size      0       user did not specify partition
throw new semanticexception generateerrormessage
qb getparseinfo   getdestforclause dest
errormsg need_partition_error getmsg
// the hold_ddltiime hint should not be used with dynamic partition since the
// newly generated partitions should always update their ddltime
if  holdddltime
throw new semanticexception generateerrormessage
qb getparseinfo   getdestforclause dest
errormsg hold_ddltime_on_nonexist_partitions getmsg
dpctx   qbm getdpctx dest
if  dpctx    null
utilities validatepartspec dest_tab  partspec
dpctx   new dynamicpartitionctx dest_tab  partspec
conf getvar hiveconf confvars defaultpartitionname
conf getintvar hiveconf confvars dynamicpartitionmaxpartspernode
qbm setdpctx dest  dpctx
if  hiveconf getboolvar conf  hiveconf confvars dynamicpartitioning        allow dp
if  dpctx getnumdpcols   > 0
hiveconf getboolvar conf  hiveconf confvars hivemergemapfiles
hiveconf getboolvar conf  hiveconf confvars hivemergemapredfiles
utilities supportcombinefileinputformat      false
// do not support merge for hadoop versions (pre-0.20) that do not
// support combinehiveinputformat
hiveconf setboolvar conf  hiveconf confvars hivemergemapfiles  false
hiveconf setboolvar conf  hiveconf confvars hivemergemapredfiles  false
// turn on hive.task.progress to update # of partitions created to the jt
hiveconf setboolvar conf  hiveconf confvars hivejobprogress  true
else      qbmetadata dest_partition capture the all sp case
throw new semanticexception generateerrormessage
qb getparseinfo   getdestforclause dest
errormsg dynamic_partition_disabled getmsg
if  dpctx getsppath      null
dest_path   new path dest_tab getpath    dpctx getsppath
if   dest_tab getnumbuckets   > 0
conf getboolvar hiveconf confvars hiveenforcebucketing
dpctx setnumbuckets dest_tab getnumbuckets
boolean isnonnativetable   dest_tab isnonnative
if  isnonnativetable
querytmpdir   dest_path touri   getpath
else
querytmpdir   ctx getexternaltmpfileuri dest_path touri
if  dpctx    null
// set the root of the temporay path where dynamic partition columns will populate
dpctx setrootpath querytmpdir
// this table_desc does not contain the partitioning columns
table_desc   utilities gettabledesc dest_tab
// add sorting/bucketing if needed
input   genbucketingsortingdest dest  input  qb  table_desc  dest_tab  rsctx
idtotablenamemap put string valueof desttableid   dest_tab gettablename
currenttableid   desttableid
desttableid
// create the work for moving the table
// note: specify dynamic partitions in dest_tab for writeentity
if   isnonnativetable
ltd   new loadtabledesc querytmpdir  ctx getexternaltmpfileuri dest_path touri
table_desc  dpctx
ltd setreplace  qb getparseinfo   isinsertintotable dest_tab getdbname
dest_tab gettablename
if  holdddltime
log info
ltd setholdddltime true
loadtablework add ltd
// here only register the whole table for post-exec hook if no dp present
// in the case of dp, we will register writeentity in movetask when the
// list of dynamically created partitions are known.
if   dpctx    null    dpctx getnumdpcols      0
outputs add new writeentity dest_tab
throw new semanticexception errormsg output_specified_multiple_times
getmsg dest_tab gettablename
if   dpctx    null      dpctx getnumdpcols   >  0
// no static partition specified
if  dpctx getnumspcols      0
outputs add new writeentity dest_tab  false
// part of the partition specified
// create a dummypartition in this case. since, the metastore does not store partial
// partitions currently, we need to store dummy partitions
else
try
string ppath   dpctx getsppath
ppath   ppath substring 0  ppath length   1
dummypartition p
new dummypartition dest_tab  dest_tab getdbname
dest_tab gettablename         ppath
partspec
outputs add new writeentity p  false
catch  hiveexception e
throw new semanticexception e getmessage    e
break
case qbmetadata dest_partition
dest_part   qbm getdestpartitionforalias dest
dest_tab   dest_part gettable
if    conf getboolvar hiveconf confvars hive_insert_into_external_tables
dest_tab gettabletype   equals tabletype external_table
throw new semanticexception
errormsg insert_external_table getmsg dest_tab gettablename
path tabpath   dest_tab getpath
path partpath   dest_part getpartitionpath
// if the table is in a different dfs than the partition,
// replace the partition's dfs with the table's dfs.
dest_path   new path tabpath touri   getscheme    tabpath touri
getauthority    partpath touri   getpath
querytmpdir   ctx getexternaltmpfileuri dest_path touri
table_desc   utilities gettabledesc dest_tab
// add sorting/bucketing if needed
input   genbucketingsortingdest dest  input  qb  table_desc  dest_tab  rsctx
idtotablenamemap put string valueof desttableid   dest_tab gettablename
currenttableid   desttableid
desttableid
ltd   new loadtabledesc querytmpdir  ctx getexternaltmpfileuri dest_path touri
table_desc  dest_part getspec
ltd setreplace  qb getparseinfo   isinsertintotable dest_tab getdbname
dest_tab gettablename
if  holdddltime
try
partition part   db getpartition dest_tab  dest_part getspec    false
if  part    null
throw new semanticexception generateerrormessage
qb getparseinfo   getdestforclause dest
errormsg hold_ddltime_on_nonexist_partitions getmsg
catch  hiveexception e
throw new semanticexception e
log info
ltd setholdddltime true
loadtablework add ltd
if   outputs add new writeentity dest_part
throw new semanticexception errormsg output_specified_multiple_times
getmsg dest_tab gettablename         dest_part getname
break
case qbmetadata dest_local_file
islocal   true
// fall through
case qbmetadata dest_dfs_file
dest_path   new path qbm getdestfileforalias dest
string deststr   dest_path tostring
if  islocal
// for local directory - we always write to map-red intermediate
// store and then copy to local fs
querytmpdir   ctx getmrtmpfileuri
else
// otherwise write to the file system implied by the directory
// no copy is required. we may want to revisit this policy in future
try
path qpath   fileutils makequalified dest_path  conf
querytmpdir   ctx getexternaltmpfileuri qpath touri
catch  exception e
throw new semanticexception
dest_path  e
string cols
string coltypes
arraylist<columninfo> colinfos   inputrr getcolumninfos
// ctas case: the file output format and serde are defined by the create
// table command
// rather than taking the default value
list<fieldschema> field_schemas   null
createtabledesc tbldesc   qb gettabledesc
if  tbldesc    null
field_schemas   new arraylist<fieldschema>
boolean first   true
for  columninfo colinfo   colinfos
string nm   inputrr reverselookup colinfo getinternalname
if  nm    null       non null column alias
colinfo setalias nm
if  field_schemas    null
fieldschema col   new fieldschema
if  nm    null
col setname unescapeidentifier colinfo getalias    tolowercase        remove ``
else
col setname colinfo getinternalname
col settype colinfo gettype   gettypename
field_schemas add col
if   first
cols   cols concat
coltypes   coltypes concat
first   false
cols   cols concat colinfo getinternalname
// replace void type with string when the output is a temp table or
// local files.
// a void type can be generated under the query:
//
// select null from tt;
// or
// insert overwrite local directory "abc" select null from tt;
//
// where there is no column type to which the null value should be
// converted.
//
string tname   colinfo gettype   gettypename
if  tname equals serdeconstants void_type_name
coltypes   coltypes concat serdeconstants string_type_name
else
coltypes   coltypes concat tname
// update the create table descriptor with the resulting schema.
if  tbldesc    null
tbldesc setcols new arraylist<fieldschema> field_schemas
if   ctx ismrtmpfileuri deststr
idtotablenamemap put string valueof desttableid   deststr
currenttableid   desttableid
desttableid
boolean isdfsdir    dest_type intvalue      qbmetadata dest_dfs_file
loadfilework add new loadfiledesc tbldesc  querytmpdir  deststr  isdfsdir  cols
coltypes
if  tbldesc    null
if  qb getisquery
string fileformat   hiveconf getvar conf  hiveconf confvars hivequeryresultfileformat
table_desc   planutils getdefaultqueryoutputtabledesc cols  coltypes  fileformat
else
table_desc   planutils getdefaulttabledesc integer
tostring utilities ctrlacode   cols  coltypes  false
else
table_desc   planutils gettabledesc tbldesc  cols  coltypes
if   outputs add new writeentity deststr   isdfsdir
throw new semanticexception errormsg output_specified_multiple_times
getmsg deststr
break
default
throw new semanticexception     dest_type
input   genconversionselectoperator dest  qb  input  table_desc  dpctx
inputrr   opparsectx get input  getrowresolver
arraylist<columninfo> veccol   new arraylist<columninfo>
try
structobjectinspector rowobjectinspector    structobjectinspector  table_desc
getdeserializer   getobjectinspector
list<? extends structfield> fields   rowobjectinspector
getallstructfieldrefs
for  int i   0  i < fields size    i
veccol add new columninfo fields get i  getfieldname    typeinfoutils
gettypeinfofromobjectinspector fields get i
getfieldobjectinspector        false
catch  exception e
throw new semanticexception e getmessage    e
rowschema fsrs   new rowschema veccol
filesinkdesc filesinkdesc   new filesinkdesc
querytmpdir
table_desc
conf getboolvar hiveconf confvars compressresult
currenttableid
rsctx ismultifilespray
rsctx getnumfiles
rsctx gettotalfiles
rsctx getpartncols
dpctx
// set the stats publishing/aggregating key prefix
// the same as directory name. the directory name
// can be changed in the optimizer  but the key should not be changed
// it should be the same as the movework's sourcedir.
filesinkdesc setstatsaggprefix filesinkdesc getdirname
if  dest_part    null
try
string staticspec   warehouse makepartpath dest_part getspec
filesinkdesc setstaticspec staticspec
catch  metaexception e
throw new semanticexception e
else if  dpctx    null
filesinkdesc setstaticspec dpctx getsppath
operator output   putopinsertmap operatorfactory getandmakechild filesinkdesc
fsrs  input   inputrr
if  ltd    null    sessionstate get      null
sessionstate get   getlineagestate
mapdirtofop ltd getsourcedir     filesinkoperator output
if  log isdebugenabled
log debug     dest
dest_path       inputrr tostring
return output
/**
* generate the conversion selectoperator that converts the columns into the
* types that are expected by the table_desc.
*/
operator genconversionselectoperator string dest  qb qb  operator input
tabledesc table_desc  dynamicpartitionctx dpctx  throws semanticexception
structobjectinspector oi   null
try
deserializer deserializer   table_desc getdeserializerclass
newinstance
deserializer initialize conf  table_desc getproperties
oi    structobjectinspector  deserializer getobjectinspector
catch  exception e
throw new semanticexception e
// check column number
list<? extends structfield> tablefields   oi getallstructfieldrefs
boolean dynpart   hiveconf getboolvar conf  hiveconf confvars dynamicpartitioning
arraylist<columninfo> rowfields   opparsectx get input  getrowresolver
getcolumninfos
int incolumncnt   rowfields size
int outcolumncnt   tablefields size
if  dynpart    dpctx    null
outcolumncnt    dpctx getnumdpcols
if  incolumncnt    outcolumncnt
string reason       dest       outcolumncnt
incolumncnt
throw new semanticexception errormsg target_table_column_mismatch getmsg
qb getparseinfo   getdestforclause dest   reason
else if  dynpart    dpctx    null
// create the mapping from input exprnode to dest table dp column
dpctx mapinputtodp rowfields sublist tablefields size    rowfields size
// check column types
boolean converted   false
int columnnumber   tablefields size
arraylist<exprnodedesc> expressions   new arraylist<exprnodedesc>
columnnumber
// metadatatypedcolumnsetserde does not need type conversions because it
// does the conversion to string by itself.
boolean ismetadataserde   table_desc getdeserializerclass   equals
metadatatypedcolumnsetserde class
boolean islazysimpleserde   table_desc getdeserializerclass   equals
lazysimpleserde class
if   ismetadataserde
// here only deals with non-partition columns. we deal with partition columns next
for  int i   0  i < columnnumber  i
objectinspector tablefieldoi   tablefields get i
getfieldobjectinspector
typeinfo tablefieldtypeinfo   typeinfoutils
gettypeinfofromobjectinspector tablefieldoi
typeinfo rowfieldtypeinfo   rowfields get i  gettype
exprnodedesc column   new exprnodecolumndesc rowfieldtypeinfo
rowfields get i  getinternalname       false  rowfields get i  isskewedcol
// lazysimpleserde can convert any types to string type using
// json-format.
if   tablefieldtypeinfo equals rowfieldtypeinfo
islazysimpleserde
tablefieldtypeinfo getcategory   equals category primitive     tablefieldtypeinfo
equals typeinfofactory stringtypeinfo
// need to do some conversions here
converted   true
if  tablefieldtypeinfo getcategory      category primitive
// cannot convert to complex types
column   null
else
column   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc tablefieldtypeinfo gettypename
column
if  column    null
string reason       i
rowfieldtypeinfo       tablefieldtypeinfo
throw new semanticexception errormsg target_table_column_mismatch
getmsg qb getparseinfo   getdestforclause dest   reason
expressions add column
// deal with dynamic partition columns: convert exprnodedesc type to string??
if  dynpart    dpctx    null    dpctx getnumdpcols   > 0
// dp columns starts with tablefields.size()
for  int i   tablefields size    i < rowfields size      i
typeinfo rowfieldtypeinfo   rowfields get i  gettype
exprnodedesc column   new exprnodecolumndesc
rowfieldtypeinfo  rowfields get i  getinternalname       false
expressions add column
// converted = true; // [todo]: should we check & convert type to string and set it to true?
if  converted
// add the select operator
rowresolver rowresolver   new rowresolver
arraylist<string> colname   new arraylist<string>
for  int i   0  i < expressions size    i
string name   getcolumninternalname i
rowresolver put    name  new columninfo name  expressions get i
gettypeinfo       false
colname add name
operator output   putopinsertmap operatorfactory getandmakechild
new selectdesc expressions  colname   new rowschema rowresolver
getcolumninfos     input   rowresolver
return output
else
// not converted
return input
@suppresswarnings
private operator genlimitplan string dest  qb qb  operator input  int limit
throws semanticexception
// a map-only job can be optimized - instead of converting it to a
// map-reduce job, we can have another map
// job to do the same to avoid the cost of sorting in the map-reduce phase.
// a better approach would be to
// write into a local file and then have a map-only job.
// add the limit operator to get the value fields
rowresolver inputrr   opparsectx get input  getrowresolver
limitdesc limitdesc   new limitdesc limit
globallimitctx setlastreducelimitdesc limitdesc
operator limitmap   putopinsertmap operatorfactory getandmakechild
limitdesc  new rowschema inputrr getcolumninfos     input
inputrr
if  log isdebugenabled
log debug     dest
inputrr tostring
return limitmap
private operator genudtfplan genericudtf genericudtf
string outputtablealias  arraylist<string> colaliases  qb qb
operator input  throws semanticexception
// no group by / distribute by / sort by / cluster by
qbparseinfo qbp   qb getparseinfo
if   qbp getdesttogroupby   isempty
throw new semanticexception errormsg udtf_no_group_by getmsg
if   qbp getdesttodistributeby   isempty
throw new semanticexception errormsg udtf_no_distribute_by getmsg
if   qbp getdesttosortby   isempty
throw new semanticexception errormsg udtf_no_sort_by getmsg
if   qbp getdesttoclusterby   isempty
throw new semanticexception errormsg udtf_no_cluster_by getmsg
if   qbp getaliastolateralviews   isempty
throw new semanticexception errormsg udtf_lateral_view getmsg
if  log isdebugenabled
log debug     outputtablealias
colaliases
// use the rowresolver from the input operator to generate a input
// objectinspector that can be used to initialize the udtf. then, the
// resulting output object inspector can be used to make the rowresolver
// for the udtf operator
rowresolver selectrr   opparsectx get input  getrowresolver
arraylist<columninfo> inputcols   selectrr getcolumninfos
// create the object inspector for the input columns and initialize the udtf
arraylist<string> colnames   new arraylist<string>
objectinspector colois   new objectinspector
for  int i   0  i < inputcols size    i
colnames add inputcols get i  getinternalname
colois   inputcols get i  getobjectinspector
structobjectinspector outputoi   genericudtf initialize colois
// make sure that the number of column aliases in the as clause matches
// the number of columns output by the udtf
int numudtfcols   outputoi getallstructfieldrefs   size
int numsuppliedaliases   colaliases size
if  numudtfcols    numsuppliedaliases
throw new semanticexception errormsg udtf_alias_mismatch
getmsg     numudtfcols
numsuppliedaliases
// generate the output column info's / row resolver using internal names.
arraylist<columninfo> udtfcols   new arraylist<columninfo>
iterator<string> colaliasesiter   colaliases iterator
for  structfield sf   outputoi getallstructfieldrefs
string colalias   colaliasesiter next
assert  colalias    null
// since the udtf operator feeds into a lvj operator that will rename
// all the internal names, we can just use field name from the udtf's oi
// as the internal name
columninfo col   new columninfo sf getfieldname    typeinfoutils
gettypeinfofromobjectinspector sf getfieldobjectinspector
outputtablealias  false
udtfcols add col
// create the row resolver for this operator from the output columns
rowresolver out_rwsch   new rowresolver
for  int i   0  i < udtfcols size    i
out_rwsch put outputtablealias  colaliases get i   udtfcols get i
// add the udtfoperator to the operator dag
operator<?> udtf   putopinsertmap operatorfactory getandmakechild
new udtfdesc genericudtf   new rowschema out_rwsch getcolumninfos
input   out_rwsch
return udtf
@suppresswarnings
private operator genlimitmapredplan string dest  qb qb  operator input
int limit  boolean extramrstep  throws semanticexception
// a map-only job can be optimized - instead of converting it to a
// map-reduce job, we can have another map
// job to do the same to avoid the cost of sorting in the map-reduce phase.
// a better approach would be to
// write into a local file and then have a map-only job.
// add the limit operator to get the value fields
operator curr   genlimitplan dest  qb  input  limit
// the client requested that an extra map-reduce step be performed
if   extramrstep
return curr
// create a reducesink operator followed by another limit
curr   genreducesinkplan dest  qb  curr  1
return genlimitplan dest  qb  curr  limit
private arraylist<exprnodedesc> getparitioncolsfrombucketcols string dest  qb qb  table tab
tabledesc table_desc  operator input  boolean convert
throws semanticexception
list<string> tabbucketcols   tab getbucketcols
list<fieldschema> tabcols    tab getcols
// partition by the bucketing column
list<integer> posns   new arraylist<integer>
for  string bucketcol   tabbucketcols
int pos   0
for  fieldschema tabcol   tabcols
if  bucketcol equals tabcol getname
posns add pos
break
pos
return genconvertcol dest  qb  tab  table_desc  input  posns  convert
private arraylist<exprnodedesc> genconvertcol string dest  qb qb  table tab  tabledesc table_desc  operator input
list<integer> posns  boolean convert  throws semanticexception
structobjectinspector oi   null
try
deserializer deserializer   table_desc getdeserializerclass
newinstance
deserializer initialize conf  table_desc getproperties
oi    structobjectinspector  deserializer getobjectinspector
catch  exception e
throw new semanticexception e
list<? extends structfield> tablefields   oi getallstructfieldrefs
arraylist<columninfo> rowfields   opparsectx get input  getrowresolver
getcolumninfos
// check column type
int columnnumber   posns size
arraylist<exprnodedesc> expressions   new arraylist<exprnodedesc> columnnumber
for  integer posn  posns
objectinspector tablefieldoi   tablefields get posn  getfieldobjectinspector
typeinfo tablefieldtypeinfo   typeinfoutils gettypeinfofromobjectinspector tablefieldoi
typeinfo rowfieldtypeinfo   rowfields get posn  gettype
exprnodedesc column   new exprnodecolumndesc rowfieldtypeinfo  rowfields get posn  getinternalname
rowfields get posn  gettabalias    rowfields get posn  getisvirtualcol
if  convert     tablefieldtypeinfo equals rowfieldtypeinfo
// need to do some conversions here
if  tablefieldtypeinfo getcategory      category primitive
// cannot convert to complex types
column   null
else
column   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc tablefieldtypeinfo gettypename
column
if  column    null
string reason       posn
rowfieldtypeinfo       tablefieldtypeinfo
throw new semanticexception errormsg target_table_column_mismatch
getmsg qb getparseinfo   getdestforclause dest   reason
expressions add column
return expressions
private arraylist<exprnodedesc> getsortcols string dest  qb qb  table tab  tabledesc table_desc  operator input  boolean convert
throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
list<order> tabsortcols   tab getsortcols
list<fieldschema> tabcols    tab getcols
// partition by the bucketing column
list<integer> posns   new arraylist<integer>
for  order sortcol   tabsortcols
int pos   0
for  fieldschema tabcol   tabcols
if  sortcol getcol   equals tabcol getname
columninfo colinfo   inputrr getcolumninfos   get pos
posns add pos
break
pos
return genconvertcol dest  qb  tab  table_desc  input  posns  convert
private arraylist<integer> getsortorders string dest  qb qb  table tab  operator input
throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
list<order> tabsortcols   tab getsortcols
list<fieldschema> tabcols    tab getcols
arraylist<integer> orders   new arraylist<integer>
for  order sortcol   tabsortcols
for  fieldschema tabcol   tabcols
if  sortcol getcol   equals tabcol getname
orders add sortcol getorder
break
return orders
@suppresswarnings
private operator genreducesinkplanforsortingbucketing table tab  operator input
arraylist<exprnodedesc> sortcols
list<integer> sortorders
arraylist<exprnodedesc> partitioncols
int numreducers
throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
// for the generation of the values expression just get the inputs
// signature and generate field expressions for those
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
arraylist<exprnodedesc> valuecols   new arraylist<exprnodedesc>
for  columninfo colinfo   inputrr getcolumninfos
valuecols add new exprnodecolumndesc colinfo gettype    colinfo
getinternalname    colinfo gettabalias    colinfo
getisvirtualcol
colexprmap put colinfo getinternalname    valuecols
get valuecols size     1
arraylist<string> outputcolumns   new arraylist<string>
for  int i   0  i < valuecols size    i
outputcolumns add getcolumninternalname i
stringbuilder order   new stringbuilder
for  int sortorder   sortorders
order append sortorder    basesemanticanalyzer hive_column_order_asc ?
operator interim   putopinsertmap operatorfactory getandmakechild planutils
getreducesinkdesc sortcols  valuecols  outputcolumns  false   1
partitioncols  order tostring    numreducers
new rowschema inputrr getcolumninfos     input   inputrr
interim setcolumnexprmap colexprmap
// add the extract operator to get the value fields
rowresolver out_rwsch   new rowresolver
rowresolver interim_rwsch   inputrr
integer pos   integer valueof 0
for  columninfo colinfo   interim_rwsch getcolumninfos
string info   interim_rwsch reverselookup colinfo getinternalname
out_rwsch put info  info  new columninfo
getcolumninternalname pos   colinfo gettype    info
colinfo getisvirtualcol    colinfo ishiddenvirtualcol
pos   integer valueof pos intvalue     1
operator output   putopinsertmap operatorfactory getandmakechild
new extractdesc new exprnodecolumndesc typeinfofactory stringtypeinfo
utilities reducefield value tostring       false    new rowschema
out_rwsch getcolumninfos     interim   out_rwsch
if  log isdebugenabled
log debug     tab gettablename
out_rwsch tostring
return output
@suppresswarnings
private operator genreducesinkplan string dest  qb qb  operator input
int numreducers  throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
// first generate the expression for the partition and sort keys
// the cluster by clause / distribute by clause has the aliases for
// partition function
astnode partitionexprs   qb getparseinfo   getclusterbyforclause dest
if  partitionexprs    null
partitionexprs   qb getparseinfo   getdistributebyforclause dest
arraylist<exprnodedesc> partitioncols   new arraylist<exprnodedesc>
if  partitionexprs    null
int ccount   partitionexprs getchildcount
for  int i   0  i < ccount    i
astnode cl    astnode  partitionexprs getchild i
partitioncols add genexprnodedesc cl  inputrr
astnode sortexprs   qb getparseinfo   getclusterbyforclause dest
if  sortexprs    null
sortexprs   qb getparseinfo   getsortbyforclause dest
if  sortexprs    null
sortexprs   qb getparseinfo   getorderbyforclause dest
if  sortexprs    null
assert numreducers    1
// in strict mode, in the presence of order by, limit must be specified
integer limit   qb getparseinfo   getdestlimit dest
if  conf getvar hiveconf confvars hivemapredmode  equalsignorecase
limit    null
throw new semanticexception generateerrormessage sortexprs
errormsg no_limit_with_orderby getmsg
arraylist<exprnodedesc> sortcols   new arraylist<exprnodedesc>
stringbuilder order   new stringbuilder
if  sortexprs    null
int ccount   sortexprs getchildcount
for  int i   0  i < ccount    i
astnode cl    astnode  sortexprs getchild i
if  cl gettype      hiveparser tok_tabsortcolnameasc
// sortby asc
order append
cl    astnode  cl getchild 0
else if  cl gettype      hiveparser tok_tabsortcolnamedesc
// sortby desc
order append
cl    astnode  cl getchild 0
else
// clusterby
order append
exprnodedesc exprnode   genexprnodedesc cl  inputrr
sortcols add exprnode
// for the generation of the values expression just get the inputs
// signature and generate field expressions for those
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
arraylist<exprnodedesc> valuecols   new arraylist<exprnodedesc>
for  columninfo colinfo   inputrr getcolumninfos
valuecols add new exprnodecolumndesc colinfo gettype    colinfo
getinternalname    colinfo gettabalias    colinfo
getisvirtualcol
colexprmap put colinfo getinternalname    valuecols
get valuecols size     1
arraylist<string> outputcolumns   new arraylist<string>
for  int i   0  i < valuecols size    i
outputcolumns add getcolumninternalname i
operator interim   putopinsertmap operatorfactory getandmakechild planutils
getreducesinkdesc sortcols  valuecols  outputcolumns  false   1
partitioncols  order tostring    numreducers
new rowschema inputrr getcolumninfos     input   inputrr
interim setcolumnexprmap colexprmap
// add the extract operator to get the value fields
rowresolver out_rwsch   new rowresolver
rowresolver interim_rwsch   inputrr
integer pos   integer valueof 0
for  columninfo colinfo   interim_rwsch getcolumninfos
string info   interim_rwsch reverselookup colinfo getinternalname
out_rwsch put info  info  new columninfo
getcolumninternalname pos   colinfo gettype    info
colinfo getisvirtualcol    colinfo ishiddenvirtualcol
pos   integer valueof pos intvalue     1
operator output   putopinsertmap operatorfactory getandmakechild
new extractdesc new exprnodecolumndesc typeinfofactory stringtypeinfo
utilities reducefield value tostring       false    new rowschema
out_rwsch getcolumninfos     interim   out_rwsch
if  log isdebugenabled
log debug     dest
out_rwsch tostring
return output
private operator genjoinoperatorchildren qbjointree join  operator left
operator right  hashset<integer> omitopts  throws semanticexception
rowresolver outputrs   new rowresolver
arraylist<string> outputcolumnnames   new arraylist<string>
// all children are base classes
operator<?> rightops   new operator
int outputpos   0
map<string  byte> reversedexprs   new hashmap<string  byte>
hashmap<byte  list<exprnodedesc>> exprmap   new hashmap<byte  list<exprnodedesc>>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
hashmap<integer  set<string>> postoaliasmap   new hashmap<integer  set<string>>
hashmap<byte  list<exprnodedesc>> filtermap
new hashmap<byte  list<exprnodedesc>>
for  int pos   0  pos < right length    pos
operator input   right
if  input    null
input   left
arraylist<exprnodedesc> keydesc   new arraylist<exprnodedesc>
arraylist<exprnodedesc> filterdesc   new arraylist<exprnodedesc>
byte tag   byte valueof  byte     reducesinkdesc   input getconf
gettag
// check whether this input operator produces output
if  omitopts    null     omitopts contains pos
// prepare output descriptors for the input opt
rowresolver inputrs   opparsectx get input  getrowresolver
iterator<string> keysiter   inputrs gettablenames   iterator
set<string> aliases   postoaliasmap get pos
if  aliases    null
aliases   new hashset<string>
postoaliasmap put pos  aliases
while  keysiter hasnext
string key   keysiter next
aliases add key
hashmap<string  columninfo> map   inputrs getfieldmap key
iterator<string> fnamesiter   map keyset   iterator
while  fnamesiter hasnext
string field   fnamesiter next
columninfo valueinfo   inputrs get key  field
keydesc add new exprnodecolumndesc valueinfo gettype    valueinfo
getinternalname    valueinfo gettabalias    valueinfo
getisvirtualcol
if  outputrs get key  field     null
string colname   getcolumninternalname outputpos
outputpos
outputcolumnnames add colname
colexprmap put colname  keydesc get keydesc size     1
outputrs put key  field  new columninfo colname  valueinfo
gettype    key  valueinfo getisvirtualcol    valueinfo
ishiddenvirtualcol
reversedexprs put colname  tag
for  astnode cond   join getfilters   get tag
filterdesc add genexprnodedesc cond  inputrs
exprmap put tag  keydesc
filtermap put tag  filterdesc
rightops   input
joinconddesc joincondns   new joinconddesc
for  int i   0  i < join getjoincond   length  i
joincond condn   join getjoincond
joincondns   new joinconddesc condn
joindesc desc   new joindesc exprmap  outputcolumnnames
join getnoouterjoin    joincondns  filtermap
desc setreversedexprs reversedexprs
desc setfiltermap join getfiltermap
joinoperator joinop    joinoperator  operatorfactory getandmakechild desc
new rowschema outputrs getcolumninfos     rightops
joinop setcolumnexprmap colexprmap
joinop setpostoaliasmap postoaliasmap
if  join getnullsafes      null
boolean nullsafes   new boolean
for  int i   0  i < nullsafes length  i
nullsafes   join getnullsafes   get i
desc setnullsafes nullsafes
return putopinsertmap joinop  outputrs
@suppresswarnings
private operator genjoinreducesinkchild qb qb  qbjointree jointree
operator child  string srcname  int pos  throws semanticexception
rowresolver inputrs   opparsectx get child  getrowresolver
rowresolver outputrs   new rowresolver
arraylist<string> outputcolumns   new arraylist<string>
arraylist<exprnodedesc> reducekeys   new arraylist<exprnodedesc>
// compute join keys and store in reducekeys
arraylist<astnode> exprs   jointree getexpressions   get pos
for  int i   0  i < exprs size    i
astnode expr   exprs get i
reducekeys add genexprnodedesc expr  inputrs
// walk over the input row resolver and copy in the output
arraylist<exprnodedesc> reducevalues   new arraylist<exprnodedesc>
iterator<string> tblnamesiter   inputrs gettablenames   iterator
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
while  tblnamesiter hasnext
string src   tblnamesiter next
hashmap<string  columninfo> fmap   inputrs getfieldmap src
for  map entry<string  columninfo> entry   fmap entryset
string field   entry getkey
columninfo valueinfo   entry getvalue
exprnodecolumndesc inputexpr   new exprnodecolumndesc valueinfo
gettype    valueinfo getinternalname    valueinfo gettabalias
valueinfo getisvirtualcol
reducevalues add inputexpr
if  outputrs get src  field     null
string col   getcolumninternalname reducevalues size     1
outputcolumns add col
columninfo newcolinfo   new columninfo utilities reducefield value
tostring
col  valueinfo gettype    src  valueinfo
getisvirtualcol    valueinfo ishiddenvirtualcol
colexprmap put newcolinfo getinternalname    inputexpr
outputrs put src  field  newcolinfo
int numreds    1
// use only 1 reducer in case of cartesian product
if  reducekeys size      0
numreds   1
// cartesian product is not supported in strict mode
if  conf getvar hiveconf confvars hivemapredmode  equalsignorecase
throw new semanticexception errormsg no_cartesian_product getmsg
reducesinkoperator rsop    reducesinkoperator  putopinsertmap
operatorfactory getandmakechild planutils getreducesinkdesc reducekeys
reducevalues  outputcolumns  false  jointree getnexttag
reducekeys size    numreds   new rowschema outputrs
getcolumninfos     child   outputrs
rsop setcolumnexprmap colexprmap
return rsop
private operator genjoinoperator qb qb  qbjointree jointree
hashmap<string  operator> map  throws semanticexception
qbjointree leftchild   jointree getjoinsrc
operator joinsrcop   null
if  leftchild    null
operator joinop   genjoinoperator qb  leftchild  map
arraylist<astnode> filter   jointree getfiltersforpushing   get 0
for  astnode cond   filter
joinop   genfilterplan qb  cond  joinop
joinsrcop   genjoinreducesinkchild qb  jointree  joinop  null  0
operator srcops   new operator
hashset<integer> omitopts   null     set of input to the join that should be
// omitted by the output
int pos   0
for  string src   jointree getbasesrc
if  src    null
operator srcop   map get src tolowercase
// for left-semi join, generate an additional selection & group-by
// operator before reducesink
arraylist<astnode> fields   jointree getrhssemijoincolumns src
if  fields    null
// the rhs table columns should be not be output from the join
if  omitopts    null
omitopts   new hashset<integer>
omitopts add pos
// generate a selection operator for group-by keys only
srcop   insertselectforsemijoin fields  srcop
// generate a groupby operator (hash mode) for a map-side partial
// aggregation for semijoin
srcop   genmapgroupbyforsemijoin qb  fields  srcop
groupbydesc mode hash
// generate a reducesink operator for the join
srcops   genjoinreducesinkchild qb  jointree  srcop  src  pos
pos
else
assert pos    0
srcops   null
// type checking and implicit type conversion for join keys
genjoinoperatortypecheck joinsrcop  srcops
joinoperator joinop    joinoperator  genjoinoperatorchildren jointree
joinsrcop  srcops  omitopts
joincontext put joinop  jointree
return joinop
/**
* construct a selection operator for semijoin that filter out all fields
* other than the group by keys.
*
* @param fields
*          list of fields need to be output
* @param input
*          input operator
* @return the selection operator.
* @throws semanticexception
*/
private operator insertselectforsemijoin arraylist<astnode> fields
operator input  throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
arraylist<exprnodedesc> collist   new arraylist<exprnodedesc>
arraylist<string> columnnames   new arraylist<string>
// construct the list of columns that need to be projected
for  astnode field   fields
exprnodecolumndesc exprnode    exprnodecolumndesc  genexprnodedesc field
inputrr
collist add exprnode
columnnames add exprnode getcolumn
// create selection operator
operator output   putopinsertmap operatorfactory getandmakechild
new selectdesc collist  columnnames  false   new rowschema inputrr
getcolumninfos     input   inputrr
output setcolumnexprmap input getcolumnexprmap
return output
private operator genmapgroupbyforsemijoin qb qb  arraylist<astnode> fields     the
// astnode
// of
// the
// join
// key
// "tab.col"
operator inputoperatorinfo  groupbydesc mode mode
throws semanticexception
rowresolver groupbyinputrowresolver   opparsectx get inputoperatorinfo
getrowresolver
rowresolver groupbyoutputrowresolver   new rowresolver
arraylist<exprnodedesc> groupbykeys   new arraylist<exprnodedesc>
arraylist<string> outputcolumnnames   new arraylist<string>
arraylist<aggregationdesc> aggregations   new arraylist<aggregationdesc>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
qb getparseinfo
groupbyoutputrowresolver setisexprresolver true      join keys should only
// be columns but not be
// expressions
for  int i   0  i < fields size      i
// get the group by keys to columninfo
astnode colname   fields get i
exprnodedesc grpbyexprnode   genexprnodedesc colname
groupbyinputrowresolver
groupbykeys add grpbyexprnode
// generate output column names
string field   getcolumninternalname i
outputcolumnnames add field
columninfo colinfo2   new columninfo field  grpbyexprnode gettypeinfo
false
groupbyoutputrowresolver putexpression colname  colinfo2
// establish mapping from the output column to the input column
colexprmap put field  grpbyexprnode
// generate group-by operator
float groupbymemoryusage   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrhashmemory
float memorythreshold   hiveconf getfloatvar conf  hiveconf confvars hivemapaggrmemorythreshold
operator op   putopinsertmap operatorfactory getandmakechild
new groupbydesc mode  outputcolumnnames  groupbykeys  aggregations
false groupbymemoryusage memorythreshold  null  false  0
new rowschema groupbyoutputrowresolver getcolumninfos
inputoperatorinfo   groupbyoutputrowresolver
op setcolumnexprmap colexprmap
return op
private void genjoinoperatortypecheck operator left  operator right
throws semanticexception
// keys[i] -> arraylist<exprnodedesc> for the i-th join operator key list
arraylist<arraylist<exprnodedesc>> keys   new arraylist<arraylist<exprnodedesc>>
int keylength   0
for  int i   0  i < right length  i
operator oi    i    0    right    null ? left   right
reducesinkdesc now     reducesinkoperator   oi   getconf
if  i    0
keylength   now getkeycols   size
else
assert  keylength    now getkeycols   size
keys add now getkeycols
// implicit type conversion hierarchy
for  int k   0  k < keylength  k
// find the common class for type conversion
typeinfo commontype   keys get 0  get k  gettypeinfo
for  int i   1  i < right length  i
typeinfo a   commontype
typeinfo b   keys get i  get k  gettypeinfo
commontype   functionregistry getcommonclassforcomparison a  b
if  commontype    null
throw new semanticexception
a gettypename
b gettypename
// add implicit type conversion if necessary
for  int i   0  i < right length  i
if   commontype equals keys get i  get k  gettypeinfo
keys get i  set
k
typecheckprocfactory defaultexprprocessor getfuncexprnodedesc
commontype gettypename    keys get i  get k
// regenerate keyserializationinfo because the reducesinkoperator's
// output key types might have changed.
for  int i   0  i < right length  i
operator oi    i    0    right    null ? left   right
reducesinkdesc now     reducesinkoperator   oi   getconf
now setkeyserializeinfo planutils getreducekeytabledesc planutils
getfieldschemasfromcolumnlist now getkeycols        now
getorder
private operator genjoinplan qb qb  hashmap<string  operator> map
throws semanticexception
qbjointree jointree   qb getqbjointree
operator joinop   genjoinoperator qb  jointree  map
return joinop
/**
* extract the filters from the join condition and push them on top of the
* source operators. this procedure traverses the query tree recursively,
*/
private void pushjoinfilters qb qb  qbjointree jointree
hashmap<string  operator> map  throws semanticexception
if  jointree getjoinsrc      null
pushjoinfilters qb  jointree getjoinsrc    map
arraylist<arraylist<astnode>> filters   jointree getfiltersforpushing
int pos   0
for  string src   jointree getbasesrc
if  src    null
operator srcop   map get src
arraylist<astnode> filter   filters get pos
for  astnode cond   filter
srcop   genfilterplan qb  cond  srcop
map put src  srcop
pos
private list<string> getmapsidejointables qb qb
list<string> cols   new arraylist<string>
astnode hints   qb getparseinfo   gethints
for  int pos   0  pos < hints getchildcount    pos
astnode hint    astnode  hints getchild pos
if    astnode  hint getchild 0   gettoken   gettype      hiveparser tok_mapjoin
astnode hinttblnames    astnode  hint getchild 1
int numch   hinttblnames getchildcount
for  int tblpos   0  tblpos < numch  tblpos
string tblname     astnode  hinttblnames getchild tblpos   gettext
tolowercase
if   cols contains tblname
cols add tblname
return cols
private qbjointree genuniquejointree qb qb  astnode joinparsetree
throws semanticexception
qbjointree jointree   new qbjointree
jointree setnoouterjoin false
jointree setexpressions new arraylist<arraylist<astnode>>
jointree setfilters new arraylist<arraylist<astnode>>
jointree setfiltersforpushing new arraylist<arraylist<astnode>>
// create jointree structures to fill them up later
arraylist<string> rightaliases   new arraylist<string>
arraylist<string> leftaliases   new arraylist<string>
arraylist<string> basesrc   new arraylist<string>
arraylist<boolean> preserved   new arraylist<boolean>
boolean lastpreserved   false
int cols    1
for  int i   0  i < joinparsetree getchildcount    i
astnode child    astnode  joinparsetree getchild i
switch  child gettoken   gettype
case hiveparser tok_tabref
// handle a table - populate aliases appropriately:
// leftaliases should contain the first table, rightaliases should
// contain all other tables and basesrc should contain all tables
string tablename   getunescapedunqualifiedtablename  astnode  child getchild 0
string alias   child getchildcount      1 ? tablename
unescapeidentifier child getchild child getchildcount     1
gettext   tolowercase
if  i    0
leftaliases add alias
jointree setleftalias alias
else
rightaliases add alias
basesrc add alias
preserved add lastpreserved
lastpreserved   false
break
case hiveparser tok_explist
if  cols     1    child getchildcount      0
cols   child getchildcount
else if  child getchildcount      cols
throw new semanticexception
arraylist<astnode> expressions   new arraylist<astnode>
arraylist<astnode> filt   new arraylist<astnode>
arraylist<astnode> filters   new arraylist<astnode>
for  node exp   child getchildren
expressions add  astnode  exp
jointree getexpressions   add expressions
jointree getfilters   add filt
jointree getfiltersforpushing   add filters
break
case hiveparser kw_preserve
lastpreserved   true
break
case hiveparser tok_subquery
throw new semanticexception
default
throw new semanticexception
jointree setbasesrc basesrc toarray new string
jointree setleftaliases leftaliases toarray new string
jointree setrightaliases rightaliases toarray new string
joincond condn   new joincond
for  int i   0  i < condn length  i
condn   new joincond preserved get i
jointree setjoincond condn
if  qb getparseinfo   gethints      null
parsestreamtables jointree  qb
return jointree
private qbjointree genjointree qb qb  astnode joinparsetree
throws semanticexception
qbjointree jointree   new qbjointree
joincond condn   new joincond
switch  joinparsetree gettoken   gettype
case hiveparser tok_leftouterjoin
jointree setnoouterjoin false
condn   new joincond 0  1  jointype leftouter
break
case hiveparser tok_rightouterjoin
jointree setnoouterjoin false
condn   new joincond 0  1  jointype rightouter
break
case hiveparser tok_fullouterjoin
jointree setnoouterjoin false
condn   new joincond 0  1  jointype fullouter
break
case hiveparser tok_leftsemijoin
jointree setnosemijoin false
condn   new joincond 0  1  jointype leftsemi
break
default
condn   new joincond 0  1  jointype inner
jointree setnoouterjoin true
break
jointree setjoincond condn
astnode left    astnode  joinparsetree getchild 0
astnode right    astnode  joinparsetree getchild 1
if   left gettoken   gettype      hiveparser tok_tabref
left gettoken   gettype      hiveparser tok_subquery
string tablename   getunescapedunqualifiedtablename  astnode  left getchild 0
tolowercase
string alias   left getchildcount      1 ? tablename
unescapeidentifier left getchild left getchildcount     1
gettext   tolowercase
jointree setleftalias alias
string leftaliases   new string
leftaliases   alias
jointree setleftaliases leftaliases
string children   new string
children   alias
jointree setbasesrc children
else if  isjointoken left
qbjointree lefttree   genjointree qb  left
jointree setjoinsrc lefttree
string leftchildaliases   lefttree getleftaliases
string leftaliases   new string
for  int i   0  i < leftchildaliases length  i
leftaliases   leftchildaliases
leftaliases   lefttree getrightaliases
jointree setleftaliases leftaliases
else
assert  false
if   right gettoken   gettype      hiveparser tok_tabref
right gettoken   gettype      hiveparser tok_subquery
string tablename   getunescapedunqualifiedtablename  astnode  right getchild 0
tolowercase
string alias   right getchildcount      1 ? tablename
unescapeidentifier right getchild right getchildcount     1
gettext   tolowercase
string rightaliases   new string
rightaliases   alias
jointree setrightaliases rightaliases
string children   jointree getbasesrc
if  children    null
children   new string
children   alias
jointree setbasesrc children
// remember rhs table for semijoin
if  jointree getnosemijoin      false
jointree addrhssemijoin alias
else
assert false
arraylist<arraylist<astnode>> expressions   new arraylist<arraylist<astnode>>
expressions add new arraylist<astnode>
expressions add new arraylist<astnode>
jointree setexpressions expressions
arraylist<boolean> nullsafes   new arraylist<boolean>
jointree setnullsafes nullsafes
arraylist<arraylist<astnode>> filters   new arraylist<arraylist<astnode>>
filters add new arraylist<astnode>
filters add new arraylist<astnode>
jointree setfilters filters
jointree setfiltermap new int
arraylist<arraylist<astnode>> filtersforpushing
new arraylist<arraylist<astnode>>
filtersforpushing add new arraylist<astnode>
filtersforpushing add new arraylist<astnode>
jointree setfiltersforpushing filtersforpushing
astnode joincond    astnode  joinparsetree getchild 2
arraylist<string> leftsrc   new arraylist<string>
parsejoincondition jointree  joincond  leftsrc
if  leftsrc size      1
jointree setleftalias leftsrc get 0
// check the hints to see if the user has specified a map-side join. this
// will be removed later on, once the cost-based
// infrastructure is in place
if  qb getparseinfo   gethints      null
list<string> mapsidetables   getmapsidejointables qb
list<string> mapaliases   jointree getmapaliases
for  string maptbl   mapsidetables
boolean maptable   false
for  string leftalias   jointree getleftaliases
if  maptbl equalsignorecase leftalias
maptable   true
for  string rightalias   jointree getrightaliases
if  maptbl equalsignorecase rightalias
maptable   true
if  maptable
if  mapaliases    null
mapaliases   new arraylist<string>
mapaliases add maptbl
jointree setmapsidejoin true
jointree setmapaliases mapaliases
parsestreamtables jointree  qb
return jointree
private void parsestreamtables qbjointree jointree  qb qb
list<string> streamaliases   jointree getstreamaliases
for  node hintnode   qb getparseinfo   gethints   getchildren
astnode hint    astnode  hintnode
if  hint getchild 0  gettype      hiveparser tok_streamtable
for  int i   0  i < hint getchild 1  getchildcount    i
if  streamaliases    null
streamaliases   new arraylist<string>
streamaliases add hint getchild 1  getchild i  gettext
jointree setstreamaliases streamaliases
private void mergejoins qb qb  qbjointree parent  qbjointree node
qbjointree target  int pos
string noderightaliases   node getrightaliases
string trgtrightaliases   target getrightaliases
string rightaliases   new string[noderightaliases length
trgtrightaliases length]
for  int i   0  i < trgtrightaliases length  i
rightaliases   trgtrightaliases
for  int i   0  i < noderightaliases length  i
rightaliases   noderightaliases
target setrightaliases rightaliases
string nodebasesrc   node getbasesrc
string trgtbasesrc   target getbasesrc
string basesrc   new string
for  int i   0  i < trgtbasesrc length  i
basesrc   trgtbasesrc
for  int i   1  i < nodebasesrc length  i
basesrc   nodebasesrc
target setbasesrc basesrc
arraylist<arraylist<astnode>> expr   target getexpressions
for  int i   0  i < noderightaliases length  i
expr add node getexpressions   get i   1
arraylist<boolean> nns   node getnullsafes
arraylist<boolean> tns   target getnullsafes
for  int i   0  i < tns size    i
tns set i  tns get i    nns get i         any of condition contains non ns  non ns
arraylist<arraylist<astnode>> filters   target getfilters
for  int i   0  i < noderightaliases length  i
filters add node getfilters   get i   1
if  node getfilters   get 0  size      0
arraylist<astnode> filterpos   filters get pos
filterpos addall node getfilters   get 0
int nmap   node getfiltermap
int tmap   target getfiltermap
int newmap   new int
for  int mapping   nmap
if  mapping    null
for  int i   0  i < mapping length  i  2
if  pos > 0    mapping > 0
mapping    trgtrightaliases length
if  nmap    null
if  tmap    null
tmap   nmap
else
int appended   new int length   nmap length]
system arraycopy tmap  0  appended  0  tmap length
system arraycopy nmap  0  appended  tmap length  nmap length
tmap   appended
system arraycopy tmap  0  newmap  0  tmap length
system arraycopy nmap  1  newmap  tmap length  nmap length   1
target setfiltermap newmap
arraylist<arraylist<astnode>> filter   target getfiltersforpushing
for  int i   0  i < noderightaliases length  i
filter add node getfiltersforpushing   get i   1
if  node getfiltersforpushing   get 0  size      0
arraylist<astnode> filterpos   filter get pos
filterpos addall node getfiltersforpushing   get 0
if  qb getqbjointree      node
qb setqbjointree node getjoinsrc
else
parent setjoinsrc node getjoinsrc
if  node getnoouterjoin      target getnoouterjoin
target setnoouterjoin true
else
target setnoouterjoin false
if  node getnosemijoin      target getnosemijoin
target setnosemijoin true
else
target setnosemijoin false
target mergerhssemijoin node
joincond nodecondns   node getjoincond
int nodecondnssize   nodecondns length
joincond targetcondns   target getjoincond
int targetcondnssize   targetcondns length
joincond newcondns   new joincond
for  int i   0  i < targetcondnssize  i
newcondns   targetcondns
for  int i   0  i < nodecondnssize  i
joincond nodecondn   nodecondns
if  nodecondn getleft      0
nodecondn setleft pos
else
nodecondn setleft nodecondn getleft     targetcondnssize
nodecondn setright nodecondn getright     targetcondnssize
newcondns   nodecondn
target setjoincond newcondns
if  target ismapsidejoin
assert node ismapsidejoin
list<string> mapaliases   target getmapaliases
for  string maptbl   node getmapaliases
if   mapaliases contains maptbl
mapaliases add maptbl
target setmapaliases mapaliases
private int findmergepos qbjointree node  qbjointree target
int res    1
string leftalias   node getleftalias
if  leftalias    null
return  1
arraylist<astnode> nodecondn   node getexpressions   get 0
arraylist<astnode> targetcondn   null
if  leftalias equals target getleftalias
targetcondn   target getexpressions   get 0
res   0
else
for  int i   0  i < target getrightaliases   length  i
if  leftalias equals target getrightaliases
targetcondn   target getexpressions   get i   1
res   i   1
break
if   targetcondn    null      nodecondn size      targetcondn size
return  1
for  int i   0  i < nodecondn size    i
if   nodecondn get i  tostringtree   equals
targetcondn get i  tostringtree
return  1
return res
private boolean mergejoinnodes qb qb  qbjointree parent  qbjointree node
qbjointree target
if  target    null
return false
if   node getnoouterjoin       target getnoouterjoin
// todo 8 way could be not enough number
if  node getrightaliases   length   node getrightaliases   length   1 >  8
log info errormsg joinnode_outerjoin_morethan_8
return false
int res   findmergepos node  target
if  res     1
mergejoins qb  parent  node  target  res
return true
return mergejoinnodes qb  parent  node  target getjoinsrc
private void mergejointree qb qb
qbjointree root   qb getqbjointree
qbjointree parent   null
while  root    null
boolean merged   mergejoinnodes qb  parent  root  root getjoinsrc
if  parent    null
if  merged
root   qb getqbjointree
else
parent   root
root   root getjoinsrc
else
if  merged
root   root getjoinsrc
else
parent   parent getjoinsrc
root   parent getjoinsrc
private operator insertselectallplanforgroupby operator input
throws semanticexception
opparsecontext inputctx   opparsectx get input
rowresolver inputrr   inputctx getrowresolver
arraylist<columninfo> columns   inputrr getcolumninfos
arraylist<exprnodedesc> collist   new arraylist<exprnodedesc>
arraylist<string> columnnames   new arraylist<string>
map<string  exprnodedesc> columnexprmap
new hashmap<string  exprnodedesc>
for  int i   0  i < columns size    i
columninfo col   columns get i
collist add new exprnodecolumndesc col gettype    col getinternalname
col gettabalias    col getisvirtualcol
columnnames add col getinternalname
columnexprmap put col getinternalname
new exprnodecolumndesc col gettype    col getinternalname
col gettabalias    col getisvirtualcol
operator output   putopinsertmap operatorfactory getandmakechild
new selectdesc collist  columnnames  true   new rowschema inputrr
getcolumninfos     input   inputrr
output setcolumnexprmap columnexprmap
return output
// return the common distinct expression
// there should be more than 1 destination, with group bys in all of them.
private list<astnode> getcommondistinctexprs qb qb  operator input
rowresolver inputrr   opparsectx get input  getrowresolver
qbparseinfo qbp   qb getparseinfo
treeset<string> ks   new treeset<string>
ks addall qbp getclausenames
// go over all the destination tables
if  ks size   <  1
return null
list<exprnodedesc exprnodedescequalitywrapper> oldlist   null
list<astnode> oldastlist   null
for  string dest   ks
// if a grouping set aggregation is present, common processing is not possible
if   qbp getdestcubes   isempty       qbp getdestrollups   isempty
return null
// if a filter is present, common processing is not possible
if  qbp getwhrforclause dest     null
return null
if  qbp getaggregationexprsforclause dest  size      0
getgroupbyforclause qbp  dest  size      0
return null
// all distinct expressions must be the same
list<astnode> list   qbp getdistinctfuncexprsforclause dest
if  list isempty
return null
list<exprnodedesc exprnodedescequalitywrapper> currdestlist
try
currdestlist   getdistinctexprs qbp  dest  inputrr
catch  semanticexception e
return null
list<astnode> currastlist   new arraylist<astnode>
for  astnode value  list
// 0 is function name
for  int i   1  i < value getchildcount    i
astnode parameter    astnode  value getchild i
currastlist add parameter
if  oldlist    null
oldlist   currdestlist
oldastlist   currastlist
else
if   matchexprlists oldlist  currdestlist
return null
return oldastlist
private operator createcommonreducesink qb qb  operator input
throws semanticexception
// go over all the tables and extract the common distinct key
list<astnode> distexprs   getcommondistinctexprs qb  input
qbparseinfo qbp   qb getparseinfo
treeset<string> ks   new treeset<string>
ks addall qbp getclausenames
// pass the entire row
rowresolver inputrr   opparsectx get input  getrowresolver
rowresolver reducesinkoutputrowresolver   new rowresolver
reducesinkoutputrowresolver setisexprresolver true
arraylist<exprnodedesc> reducekeys   new arraylist<exprnodedesc>
arraylist<exprnodedesc> reducevalues   new arraylist<exprnodedesc>
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
// pre-compute distinct group-by keys and store in reducekeys
list<string> outputcolumnnames   new arraylist<string>
for  astnode distn   distexprs
exprnodedesc distexpr   genexprnodedesc distn  inputrr
reducekeys add distexpr
if  reducesinkoutputrowresolver getexpression distn     null
outputcolumnnames add getcolumninternalname reducekeys size     1
string field   utilities reducefield key tostring
getcolumninternalname reducekeys size     1
columninfo colinfo   new columninfo field  reducekeys get
reducekeys size     1  gettypeinfo       false
reducesinkoutputrowresolver putexpression distn  colinfo
colexprmap put colinfo getinternalname    distexpr
// go over all the grouping keys and aggregations
for  string dest   ks
list<astnode> grpbyexprs   getgroupbyforclause qbp  dest
for  int i   0  i < grpbyexprs size      i
astnode grpbyexpr   grpbyexprs get i
if  reducesinkoutputrowresolver getexpression grpbyexpr     null
exprnodedesc grpbyexprnode   genexprnodedesc grpbyexpr  inputrr
reducevalues add grpbyexprnode
string field   utilities reducefield value tostring
getcolumninternalname reducevalues size     1
columninfo colinfo   new columninfo field  reducevalues get
reducevalues size     1  gettypeinfo       false
reducesinkoutputrowresolver putexpression grpbyexpr  colinfo
outputcolumnnames add getcolumninternalname reducevalues size     1
// for each aggregation
hashmap<string  astnode> aggregationtrees   qbp
getaggregationexprsforclause dest
assert  aggregationtrees    null
for  map entry<string  astnode> entry   aggregationtrees entryset
astnode value   entry getvalue
// 0 is the function name
for  int i   1  i < value getchildcount    i
astnode paraexpr    astnode  value getchild i
if  reducesinkoutputrowresolver getexpression paraexpr     null
exprnodedesc paraexprnode   genexprnodedesc paraexpr  inputrr
reducevalues add paraexprnode
string field   utilities reducefield value tostring
getcolumninternalname reducevalues size     1
columninfo colinfo   new columninfo field  reducevalues get
reducevalues size     1  gettypeinfo       false
reducesinkoutputrowresolver putexpression paraexpr  colinfo
outputcolumnnames
add getcolumninternalname reducevalues size     1
reducesinkoperator rsop    reducesinkoperator  putopinsertmap
operatorfactory getandmakechild planutils getreducesinkdesc reducekeys
reducevalues  outputcolumnnames  true   1  reducekeys size     1
new rowschema reducesinkoutputrowresolver getcolumninfos     input
reducesinkoutputrowresolver
rsop setcolumnexprmap colexprmap
return rsop
// groups the clause names into lists so that any two clauses in the same list has the same
// group by and distinct keys and no clause appears in more than one list.  returns a list of the
// lists of clauses.
private list<list<string>> getcommongroupbydestgroups qb qb  operator input
throws semanticexception
rowresolver inputrr   opparsectx get input  getrowresolver
qbparseinfo qbp   qb getparseinfo
treeset<string> ks   new treeset<string>
ks addall qbp getclausenames
list<list<string>> commongroupbydestgroups   new arraylist<list<string>>
// if this is a trivial query block return
if  ks size   <  1
list<string> onelist    new arraylist<string> 1
if  ks size      1
onelist add ks first
commongroupbydestgroups add onelist
return commongroupbydestgroups
list<list<exprnodedesc exprnodedescequalitywrapper>> spraykeylists
new arraylist<list<exprnodedesc exprnodedescequalitywrapper>> ks size
// iterate over each clause
for  string dest   ks
list<exprnodedesc exprnodedescequalitywrapper> spraykeys
getdistinctexprs qbp  dest  inputrr
// add the group by expressions
list<astnode> grpbyexprs   getgroupbyforclause qbp  dest
for  astnode grpbyexpr  grpbyexprs
exprnodedesc exprnodedescequalitywrapper grpbyexprwrapper
new exprnodedesc exprnodedescequalitywrapper genexprnodedesc grpbyexpr  inputrr
if   spraykeys contains grpbyexprwrapper
spraykeys add grpbyexprwrapper
// loop through each of the lists of exprs, looking for a match
boolean found   false
for  int i   0  i < spraykeylists size    i
if   matchexprlists spraykeylists get i   spraykeys
continue
// a match was found, so add the clause to the corresponding list
commongroupbydestgroups get i  add dest
found   true
break
// no match was found, so create new entries
if   found
spraykeylists add spraykeys
list<string> destgroup   new arraylist<string>
destgroup add dest
commongroupbydestgroups add destgroup
return commongroupbydestgroups
// returns whether or not two lists contain the same elements independent of order
private boolean matchexprlists list<exprnodedesc exprnodedescequalitywrapper> list1
list<exprnodedesc exprnodedescequalitywrapper> list2
if  list1 size      list2 size
return false
for  exprnodedesc exprnodedescequalitywrapper exprnodedesc   list1
if   list2 contains exprnodedesc
return false
return true
// returns a list of the distinct exprs for a given clause name as
// exprnodedesc.exprnodedescequalitywrapper without duplicates
private list<exprnodedesc exprnodedescequalitywrapper>
getdistinctexprs qbparseinfo qbp  string dest  rowresolver inputrr  throws semanticexception
list<astnode> distinctaggexprs   qbp getdistinctfuncexprsforclause dest
list<exprnodedesc exprnodedescequalitywrapper> distinctexprs
new arraylist<exprnodedesc exprnodedescequalitywrapper>
for  astnode distinctaggexpr  distinctaggexprs
// 0 is function name
for  int i   1  i < distinctaggexpr getchildcount    i
astnode parameter    astnode  distinctaggexpr getchild i
exprnodedesc exprnodedescequalitywrapper distinctexpr
new exprnodedesc exprnodedescequalitywrapper genexprnodedesc parameter  inputrr
if   distinctexprs contains distinctexpr
distinctexprs add distinctexpr
return distinctexprs
// see if there are any distinct expressions
private boolean distinctexprsexists qb qb
qbparseinfo qbp   qb getparseinfo
treeset<string> ks   new treeset<string>
ks addall qbp getclausenames
for  string dest   ks
list<astnode> list   qbp getdistinctfuncexprsforclause dest
if   list isempty
return true
return false
@suppresswarnings
private operator genbodyplan qb qb  operator input  throws semanticexception
qbparseinfo qbp   qb getparseinfo
treeset<string> ks   new treeset<string> qbp getclausenames
// for multi-group by with the same distinct, we ignore all user hints
// currently. it doesnt matter whether he has asked to do
// map-side aggregation or not. map side aggregation is turned off
list<astnode> commondistinctexprs   getcommondistinctexprs qb  input
boolean optimizemultigroupby   commondistinctexprs    null
operator curr   input
// if there is a single distinct, optimize that. spray initially by the
// distinct key,
// no computation at the mapper. have multiple group by operators at the
// reducer - and then
// proceed
if  optimizemultigroupby
curr   createcommonreducesink qb  input
rowresolver currrr   opparsectx get curr  getrowresolver
// create a forward operator
input   putopinsertmap operatorfactory getandmakechild new forwarddesc
new rowschema currrr getcolumninfos     curr   currrr
for  string dest   ks
curr   input
curr   gengroupbyplan2mrmultigroupby dest  qb  curr
curr   genselectplan dest  qb  curr
integer limit   qbp getdestlimit dest
if  limit    null
curr   genlimitmapredplan dest  qb  curr  limit intvalue    true
qb getparseinfo   setouterquerylimit limit intvalue
curr   genfilesinkplan dest  qb  curr
else
list<list<string>> commongroupbydestgroups   null
// if we can put multiple group bys in a single reducer, determine suitable groups of
// expressions, otherwise treat all the expressions as a single group
if  conf getboolvar hiveconf confvars hivemultigroupbysinglereducer
try
commongroupbydestgroups   getcommongroupbydestgroups qb  curr
catch  semanticexception e
log error    e
if  commongroupbydestgroups    null
commongroupbydestgroups   new arraylist<list<string>>
commongroupbydestgroups add new arraylist<string> ks
if   commongroupbydestgroups isempty
// iterate over each group of subqueries with the same group by/distinct keys
for  list<string> commongroupbydestgroup   commongroupbydestgroups
if  commongroupbydestgroup isempty
continue
string firstdest   commongroupbydestgroup get 0
// constructs a standard group by plan if:
// there is no other subquery with the same group by/distinct keys or
// (there are no aggregations in a representative query for the group and
//  there is no group by in that representative query) or
// the data is skewed or
// the conf variable used to control combining group bys into a signle reducer is false
if  commongroupbydestgroup size      1
qbp getaggregationexprsforclause firstdest  size      0
getgroupbyforclause qbp  firstdest  size      0
conf getboolvar hiveconf confvars hivegroupbyskew
conf getboolvar hiveconf confvars hivemultigroupbysinglereducer
// go over all the destination tables
for  string dest   commongroupbydestgroup
curr   input
if  qbp getwhrforclause dest     null
curr   genfilterplan dest  qb  curr
if  qbp getaggregationexprsforclause dest  size      0
getgroupbyforclause qbp  dest  size   > 0
//multiple distincts is not supported with skew in data
if  conf getboolvar hiveconf confvars hivegroupbyskew
qbp getdistinctfuncexprsforclause dest  size   > 1
throw new semanticexception errormsg unsupported_multiple_distincts
getmsg
// insert a select operator here used by the columnpruner to reduce
// the data to shuffle
curr   insertselectallplanforgroupby curr
if  conf getboolvar hiveconf confvars hivemapsideaggregate
if   conf getboolvar hiveconf confvars hivegroupbyskew
curr   gengroupbyplanmapaggr1mr dest  qb  curr
else
curr   gengroupbyplanmapaggr2mr dest  qb  curr
else if  conf getboolvar hiveconf confvars hivegroupbyskew
curr   gengroupbyplan2mr dest  qb  curr
else
curr   gengroupbyplan1mr dest  qb  curr
curr   genpostgroupbybodyplan curr  dest  qb
else
curr   gengroupbyplan1mrmultireducegb commongroupbydestgroup  qb  input
if  log isdebugenabled
log debug     qb getid
return curr
private operator genpostgroupbybodyplan operator curr  string dest  qb qb
throws semanticexception
qbparseinfo qbp   qb getparseinfo
// insert having plan here
if  qbp gethavingforclause dest     null
if  getgroupbyforclause qbp  dest  size      0
throw new semanticexception
curr   genhavingplan dest  qb  curr
curr   genselectplan dest  qb  curr
integer limit   qbp getdestlimit dest
// expressions are not supported currently without a alias.
// reduce sink is needed if the query contains a cluster by, distribute by,
// order by or a sort by clause.
boolean genreducesink   false
// currently, expressions are not allowed in cluster by, distribute by,
// order by or a sort by clause. for each of the above clause types, check
// if the clause contains any expression.
if  qbp getclusterbyforclause dest     null
genreducesink   true
if  qbp getdistributebyforclause dest     null
genreducesink   true
if  qbp getorderbyforclause dest     null
genreducesink   true
if  qbp getsortbyforclause dest     null
genreducesink   true
if  genreducesink
int numreducers    1
// use only 1 reducer if order by is present
if  qbp getorderbyforclause dest     null
numreducers   1
curr   genreducesinkplan dest  qb  curr  numreducers
if  qbp getissubq
if  limit    null
// in case of order by, only 1 reducer is used, so no need of
// another shuffle
curr   genlimitmapredplan dest  qb  curr  limit intvalue    qbp
getorderbyforclause dest     null ? false   true
else
curr   genconversionops dest  qb  curr
// exact limit can be taken care of by the fetch operator
if  limit    null
boolean extramrstep   true
if  qb getisquery      qbp getclusterbyforclause dest     null
qbp getsortbyforclause dest     null
extramrstep   false
curr   genlimitmapredplan dest  qb  curr  limit intvalue
extramrstep
qb getparseinfo   setouterquerylimit limit intvalue
curr   genfilesinkplan dest  qb  curr
// change curr ops row resolver's tab aliases to query alias if it
// exists
if  qb getparseinfo   getalias      null
rowresolver rr   opparsectx get curr  getrowresolver
rowresolver newrr   new rowresolver
string alias   qb getparseinfo   getalias
for  columninfo colinfo   rr getcolumninfos
string name   colinfo getinternalname
string tmp   rr reverselookup name
newrr put alias  tmp  colinfo
opparsectx get curr  setrowresolver newrr
return curr
@suppresswarnings
private operator genunionplan string unionalias  string leftalias
operator leftop  string rightalias  operator rightop
throws semanticexception
// currently, the unions are not merged - each union has only 2 parents. so,
// a n-way union will lead to (n-1) union operators.
// this can be easily merged into 1 union
rowresolver leftrr   opparsectx get leftop  getrowresolver
rowresolver rightrr   opparsectx get rightop  getrowresolver
hashmap<string  columninfo> leftmap   leftrr getfieldmap leftalias
hashmap<string  columninfo> rightmap   rightrr getfieldmap rightalias
// make sure the schemas of both sides are the same
astnode tabref   qb getaliases   isempty   ? null
qb getparseinfo   getsrcforalias qb getaliases   get 0
if  leftmap size      rightmap size
throw new semanticexception
for  map entry<string  columninfo> lentry   leftmap entryset
string field   lentry getkey
columninfo linfo   lentry getvalue
columninfo rinfo   rightmap get field
if  rinfo    null
throw new semanticexception generateerrormessage tabref
rightalias
field
if  linfo    null
throw new semanticexception generateerrormessage tabref
leftalias
field
if   linfo getinternalname   equals rinfo getinternalname
throw new semanticexception generateerrormessage tabref
field
getpositionfrominternalname linfo getinternalname
getpositionfrominternalname rinfo getinternalname
//try widening coversion, otherwise fail union
typeinfo commontypeinfo   functionregistry getcommonclassforunionall linfo gettype
rinfo gettype
if  commontypeinfo    null
throw new semanticexception generateerrormessage tabref
field
linfo gettype   gettypename
rinfo gettype   gettypename
// construct the forward operator
rowresolver unionoutrr   new rowresolver
for  map entry<string  columninfo> lentry   leftmap entryset
string field   lentry getkey
columninfo linfo   lentry getvalue
columninfo rinfo   rightmap get field
columninfo unioncolinfo   new columninfo linfo
unioncolinfo settype functionregistry getcommonclassforunionall linfo gettype
rinfo gettype
unionoutrr put unionalias  field  unioncolinfo
if    leftop instanceof unionoperator
leftop   geninputselectforunion leftop  leftmap  leftalias  unionoutrr  unionalias
if    rightop instanceof unionoperator
rightop   geninputselectforunion rightop  rightmap  rightalias  unionoutrr  unionalias
// if one of the children is a union, merge with it
// else create a new one
if   leftop instanceof unionoperator      rightop instanceof unionoperator
if  leftop instanceof unionoperator
// make left a child of right
list<operator<? extends operatordesc>> child
new arraylist<operator<? extends operatordesc>>
child add leftop
rightop setchildoperators child
list<operator<? extends operatordesc>> parent   leftop
getparentoperators
parent add rightop
uniondesc udesc     unionoperator  leftop  getconf
udesc setnuminputs udesc getnuminputs     1
return putopinsertmap leftop  unionoutrr
else
// make right a child of left
list<operator<? extends operatordesc>> child
new arraylist<operator<? extends operatordesc>>
child add rightop
leftop setchildoperators child
list<operator<? extends operatordesc>> parent   rightop
getparentoperators
parent add leftop
uniondesc udesc     unionoperator  rightop  getconf
udesc setnuminputs udesc getnuminputs     1
return putopinsertmap rightop  unionoutrr
// create a new union operator
operator<? extends operatordesc> unionforward   operatorfactory
getandmakechild new uniondesc    new rowschema unionoutrr
getcolumninfos
// set union operator as child of each of leftop and rightop
list<operator<? extends operatordesc>> child
new arraylist<operator<? extends operatordesc>>
child add unionforward
rightop setchildoperators child
child   new arraylist<operator<? extends operatordesc>>
child add unionforward
leftop setchildoperators child
list<operator<? extends operatordesc>> parent
new arraylist<operator<? extends operatordesc>>
parent add leftop
parent add rightop
unionforward setparentoperators parent
// create operator info list to return
return putopinsertmap unionforward  unionoutrr
/**
* generates a select operator which can go between the original input operator and the union
* operator.  this select casts columns to match the type of the associated column in the union,
* other columns pass through unchanged.  the new operator's only parent is the original input
* operator to the union, and it's only child is the union.  if the input does not need to be
* cast, the original operator is returned, and no new select operator is added.
*
* @param originputop
*          the original input operator to the union.
* @param originputfieldmap
*          a map from field name to columninfo for the original input operator.
* @param originputalias
*          the alias associated with the original input operator.
* @param unionoutrr
*          the union's output row resolver.
* @param unionalias
*          the alias of the union.
* @return
* @throws udfargumentexception
*/
private operator<? extends operatordesc> geninputselectforunion
operator<? extends operatordesc> originputop  map<string  columninfo> originputfieldmap
string originputalias  rowresolver unionoutrr  string unionalias
throws udfargumentexception
list<exprnodedesc> columns   new arraylist<exprnodedesc>
boolean needscast   false
for  map entry<string  columninfo> unionentry  unionoutrr getfieldmap unionalias  entryset
string field   unionentry getkey
columninfo linfo   originputfieldmap get field
exprnodedesc column   new exprnodecolumndesc linfo gettype    linfo getinternalname
linfo gettabalias    linfo getisvirtualcol    linfo isskewedcol
if   linfo gettype   equals unionentry getvalue   gettype
needscast   true
column   typecheckprocfactory defaultexprprocessor getfuncexprnodedesc
unionentry getvalue   gettype   gettypename    column
columns add column
// if none of the columns need to be cast there's no need for an additional select operator
if   needscast
return originputop
rowresolver rowresolver   new rowresolver
list<string> colname   new arraylist<string>
for  int i   0  i < columns size    i
string name   getcolumninternalname i
rowresolver put originputalias  name  new columninfo name  columns get i
gettypeinfo       false
colname add name
operator<selectdesc> newinputop   operatorfactory getandmakechild
new selectdesc columns  colname   new rowschema rowresolver getcolumninfos
originputop
return putopinsertmap newinputop  rowresolver
/**
* generates the sampling predicate from the tablesample clause information.
* this function uses the bucket column list to decide the expression inputs
* to the predicate hash function in case usebucketcols is set to true,
* otherwise the expression list stored in the tablesample is used. the bucket
* columns of the table are used to generate this predicate in case no
* expressions are provided on the tablesample clause and the table has
* clustering columns defined in it's metadata. the predicate created has the
* following structure:
*
* ((hash(expressions) & integer.max_value) % denominator) == numerator
*
* @param ts
*          tablesample clause information
* @param bucketcols
*          the clustering columns of the table
* @param usebucketcols
*          flag to indicate whether the bucketcols should be used as input to
*          the hash function
* @param alias
*          the alias used for the table in the row resolver
* @param rwsch
*          the row resolver used to resolve column references
* @param qbm
*          the metadata information for the query block which is used to
*          resolve unaliased columns
* @param planexpr
*          the plan tree for the expression. if the user specified this, the
*          parse expressions are not used
* @return exprnodedesc
* @exception semanticexception
*/
private exprnodedesc gensamplepredicate tablesample ts
list<string> bucketcols  boolean usebucketcols  string alias
rowresolver rwsch  qbmetadata qbm  exprnodedesc planexpr
throws semanticexception
exprnodedesc numeratorexpr   new exprnodeconstantdesc
typeinfofactory inttypeinfo  integer valueof ts getnumerator     1
exprnodedesc denominatorexpr   new exprnodeconstantdesc
typeinfofactory inttypeinfo  integer valueof ts getdenominator
exprnodedesc intmaxexpr   new exprnodeconstantdesc
typeinfofactory inttypeinfo  integer valueof integer max_value
arraylist<exprnodedesc> args   new arraylist<exprnodedesc>
if  planexpr    null
args add planexpr
else if  usebucketcols
for  string col   bucketcols
columninfo ci   rwsch get alias  col
// todo: change type to the one in the table schema
args add new exprnodecolumndesc ci gettype    ci getinternalname    ci
gettabalias    ci getisvirtualcol
else
for  astnode expr   ts getexprs
args add genexprnodedesc expr  rwsch
exprnodedesc equalsexpr   null
exprnodedesc hashfnexpr   new exprnodegenericfuncdesc
typeinfofactory inttypeinfo  new genericudfhash    args
assert  hashfnexpr    null
log info     hashfnexpr
exprnodedesc andexpr   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc    hashfnexpr  intmaxexpr
assert  andexpr    null
log info     andexpr
exprnodedesc modexpr   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc    andexpr  denominatorexpr
assert  modexpr    null
log info     modexpr
log info     numeratorexpr
equalsexpr   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc    modexpr  numeratorexpr
log info     equalsexpr
assert  equalsexpr    null
return equalsexpr
private string getaliasid string alias  qb qb
return  qb getid      null ? alias   qb getid         alias
@suppresswarnings
private operator gentableplan string alias  qb qb  throws semanticexception
string alias_id   getaliasid alias  qb
table tab   qb getmetadata   getsrcforalias alias
rowresolver rwsch
// is the table already present
operator<? extends operatordesc> top   topops get alias_id
operator<? extends operatordesc> dummysel   topselops get alias_id
if  dummysel    null
top   dummysel
if  top    null
rwsch   new rowresolver
try
structobjectinspector rowobjectinspector    structobjectinspector  tab
getdeserializer   getobjectinspector
list<? extends structfield> fields   rowobjectinspector
getallstructfieldrefs
for  int i   0  i < fields size    i
/**
* if the column is a skewed column, use columninfo accordingly
*/
columninfo colinfo   new columninfo fields get i  getfieldname
typeinfoutils gettypeinfofromobjectinspector fields get i
getfieldobjectinspector     alias  false
colinfo setskewedcol  isskewedcol alias  qb  fields get i
getfieldname     ? true   false
rwsch put alias  fields get i  getfieldname    colinfo
catch  serdeexception e
throw new runtimeexception e
// hack!! - refactor once the metadata apis with types are ready
// finally add the partitioning columns
for  fieldschema part_col   tab getpartcols
log trace     part_col
// todo: use the right type by calling part_col.gettype() instead of
// string.class
rwsch put alias  part_col getname    new columninfo part_col getname
typeinfofactory stringtypeinfo  alias  true
//put all virutal columns in rowresolver.
iterator<virtualcolumn> vcs   virtualcolumn getregistry conf  iterator
//use a list for easy cumtomize
list<virtualcolumn> vclist   new arraylist<virtualcolumn>
while  vcs hasnext
virtualcolumn vc   vcs next
rwsch put alias  vc getname    new columninfo vc getname
vc gettypeinfo    alias  true  vc getishidden
vclist add vc
// create the root of the operator tree
tablescandesc tsdesc   new tablescandesc alias  vclist
setupstats tsdesc  qb getparseinfo    tab  alias  rwsch
top   putopinsertmap operatorfactory get tsdesc
new rowschema rwsch getcolumninfos      rwsch
// add this to the list of top operators - we always start from a table
// scan
topops put alias_id  top
// add a mapping from the table scan operator to table
toptotable put  tablescanoperator  top  tab
else
rwsch   opparsectx get top  getrowresolver
top setchildoperators null
// check if this table is sampled and needs more than input pruning
operator<? extends operatordesc> tableop   top
tablesample ts   qb getparseinfo   gettabsample alias
if  ts    null
int num   ts getnumerator
int den   ts getdenominator
arraylist<astnode> sampleexprs   ts getexprs
// todo: do the type checking of the expressions
list<string> tabbucketcols   tab getbucketcols
int numbuckets   tab getnumbuckets
// if there are no sample cols and no bucket cols then throw an error
if  tabbucketcols size      0    sampleexprs size      0
throw new semanticexception errormsg non_bucketed_table getmsg
tab gettablename
if  num > den
throw new semanticexception
errormsg bucketed_numerator_bigger_denominator getmsg
tab gettablename
// check if a predicate is needed
// predicate is needed if either input pruning is not enough
// or if input pruning is not possible
// check if the sample columns are the same as the table bucket columns
boolean colsequal   true
if   sampleexprs size      tabbucketcols size
sampleexprs size      0
colsequal   false
for  int i   0  i < sampleexprs size      colsequal  i
boolean colfound   false
for  int j   0  j < tabbucketcols size       colfound  j
if  sampleexprs get i  gettoken   gettype      hiveparser tok_table_or_col
break
if    astnode  sampleexprs get i  getchild 0   gettext
equalsignorecase tabbucketcols get j
colfound   true
colsequal    colsequal    colfound
// check if input can be pruned
ts setinputpruning  sampleexprs    null    sampleexprs size      0    colsequal
// check if input pruning is enough
if   sampleexprs    null    sampleexprs size      0    colsequal
num    den     den % numbuckets    0    numbuckets % den    0
// input pruning is enough; add the filter for the optimizer to use it
// later
log info
exprnodedesc samplepredicate   gensamplepredicate ts  tabbucketcols
colsequal  alias  rwsch  qb getmetadata    null
tableop   operatorfactory getandmakechild new filterdesc
samplepredicate  true  new sampledesc ts getnumerator    ts
getdenominator    tabbucketcols  true
new rowschema rwsch getcolumninfos     top
else
// need to add filter
// create tableop to be filterdesc and set as child to 'top'
log info
exprnodedesc samplepredicate   gensamplepredicate ts  tabbucketcols
colsequal  alias  rwsch  qb getmetadata    null
tableop   operatorfactory getandmakechild new filterdesc
samplepredicate  true
new rowschema rwsch getcolumninfos     top
else
boolean testmode   conf getboolvar hiveconf confvars hivetestmode
if  testmode
string tabname   tab gettablename
// has the user explicitly asked not to sample this table
string unsampletbllist   conf
getvar hiveconf confvars hivetestmodenosample
string unsampletbls   unsampletbllist split
boolean unsample   false
for  string unsampletbl   unsampletbls
if  tabname equalsignorecase unsampletbl
unsample   true
if   unsample
int numbuckets   tab getnumbuckets
// if the input table is bucketed, choose the first bucket
if  numbuckets > 0
tablesample tssample   new tablesample 1  numbuckets
tssample setinputpruning true
qb getparseinfo   settabsample alias  tssample
exprnodedesc samplepred   gensamplepredicate tssample  tab
getbucketcols    true  alias  rwsch  qb getmetadata    null
tableop   operatorfactory
getandmakechild new filterdesc samplepred  true
new sampledesc tssample getnumerator    tssample
getdenominator    tab getbucketcols    true
new rowschema rwsch getcolumninfos     top
log info
else
// the table is not bucketed, add a dummy filter :: rand()
int freq   conf getintvar hiveconf confvars hivetestmodesamplefreq
tablesample tssample   new tablesample 1  freq
tssample setinputpruning false
qb getparseinfo   settabsample alias  tssample
log info
exprnodedesc randfunc   typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc    new exprnodeconstantdesc integer
valueof 460476415
exprnodedesc samplepred   gensamplepredicate tssample  null  false
alias  rwsch  qb getmetadata    randfunc
tableop   operatorfactory getandmakechild new filterdesc
samplepred  true
new rowschema rwsch getcolumninfos     top
operator output   putopinsertmap tableop  rwsch
if  log isdebugenabled
log debug     alias       tableop tostring
return output
private boolean isskewedcol string alias  qb qb  string colname
boolean isskewedcol   false
list<string> skewedcols   qb getskewedcolumnnames alias
for  string skewedcol skewedcols
if  skewedcol equalsignorecase colname
isskewedcol   true
return isskewedcol
private void setupstats tablescandesc tsdesc  qbparseinfo qbp  table tab  string alias  rowresolver rwsch
throws semanticexception
if   qbp isanalyzecommand
tsdesc setgatherstats false
else
tsdesc setgatherstats true
tsdesc setstatsreliable conf getboolvar hiveconf confvars hive_stats_reliable
// append additional virtual columns for storing statistics
iterator<virtualcolumn> vcs   virtualcolumn getstatsregistry conf  iterator
list<virtualcolumn> vclist   new arraylist<virtualcolumn>
while  vcs hasnext
virtualcolumn vc   vcs next
rwsch put alias  vc getname    new columninfo vc getname
vc gettypeinfo    alias  true  vc getishidden
vclist add vc
tsdesc addvirtualcols vclist
string tblname   tab gettablename
tablespec tblspec   qbp gettablespec alias
map<string  string> partspec   tblspec getpartspec
if  partspec    null
list<string> cols   new arraylist<string>
cols addall partspec keyset
tsdesc setpartcolumns cols
// theoretically the key prefix could be any unique string shared
// between tablescanoperator (when publishing) and statstask (when aggregating).
// here we use
//       table_name + partitionsec
// as the prefix for easy of read during explain and debugging.
// currently, partition spec can only be static partition.
string k   tblname   path separator
tsdesc setstatsaggprefix k
// set up writenentity for replication
outputs add new writeentity tab  true
// add writeentity for each matching partition
if  tab ispartitioned
if  partspec    null
throw new semanticexception errormsg need_partition_specification getmsg
list<partition> partitions   qbp gettablespec   partitions
if  partitions    null
for  partition partn   partitions
// inputs.add(new readentity(partn)); // is this needed at all?
outputs add new writeentity partn  true
private operator genplan qbexpr qbexpr  throws semanticexception
if  qbexpr getopcode      qbexpr opcode nullop
return genplan qbexpr getqb
if  qbexpr getopcode      qbexpr opcode union
operator qbexpr1ops   genplan qbexpr getqbexpr1
operator qbexpr2ops   genplan qbexpr getqbexpr2
return genunionplan qbexpr getalias    qbexpr getqbexpr1   getalias
qbexpr1ops  qbexpr getqbexpr2   getalias    qbexpr2ops
return null
@suppresswarnings
public operator genplan qb qb  throws semanticexception
// first generate all the opinfos for the elements in the from clause
hashmap<string  operator> aliastoopinfo   new hashmap<string  operator>
// recurse over the subqueries to fill the subquery part of the plan
for  string alias   qb getsubqaliases
qbexpr qbexpr   qb getsubqforalias alias
aliastoopinfo put alias  genplan qbexpr
qbexpr setalias alias
// recurse over all the source tables
for  string alias   qb gettabaliases
operator op   gentableplan alias  qb
aliastoopinfo put alias  op
// for all the source tables that have a lateral view, attach the
// appropriate operators to the ts
genlateralviewplans aliastoopinfo  qb
operator srcopinfo   null
// process join
if  qb getparseinfo   getjoinexpr      null
astnode joinexpr   qb getparseinfo   getjoinexpr
if  joinexpr gettoken   gettype      hiveparser tok_uniquejoin
qbjointree jointree   genuniquejointree qb  joinexpr
qb setqbjointree jointree
else
qbjointree jointree   genjointree qb  joinexpr
qb setqbjointree jointree
mergejointree qb
// if any filters are present in the join tree, push them on top of the
// table
pushjoinfilters qb  qb getqbjointree    aliastoopinfo
srcopinfo   genjoinplan qb  aliastoopinfo
else
// now if there are more than 1 sources then we have a join case
// later we can extend this to the union all case as well
srcopinfo   aliastoopinfo values   iterator   next
operator bodyopinfo   genbodyplan qb  srcopinfo
if  log isdebugenabled
log debug     qb getid
this qb   qb
return bodyopinfo
/**
* generates the operator dag needed to implement lateral views and attaches
* it to the ts operator.
*
* @param aliastoopinfo
*          a mapping from a table alias to the ts operator. this function
*          replaces the operator mapping as necessary
* @param qb
* @throws semanticexception
*/
void genlateralviewplans hashmap<string  operator> aliastoopinfo  qb qb
throws semanticexception
map<string  arraylist<astnode>> aliastolateralviews   qb getparseinfo
getaliastolateralviews
for  entry<string  operator> e   aliastoopinfo entryset
string alias   e getkey
// see if the alias has a lateral view. if so, chain the lateral view
// operator on
arraylist<astnode> lateralviews   aliastolateralviews get alias
if  lateralviews    null
operator op   e getvalue
for  astnode lateralviewtree   aliastolateralviews get alias
// there are 2 paths from the ts operator (or a previous lvj operator)
// to the same lateralviewjoinoperator.
// ts -> selectoperator(*) -> lateralviewjoinoperator
// ts -> selectoperator (gets cols for udtf) -> udtfoperator0
// -> lateralviewjoinoperator
//
rowresolver lvforwardrr   new rowresolver
rowresolver source   opparsectx get op  getrowresolver
for  columninfo col   source getcolumninfos
if col getisvirtualcol      col ishiddenvirtualcol
continue
string tabcol   source reverselookup col getinternalname
lvforwardrr put tabcol  tabcol  col
operator lvforward   putopinsertmap operatorfactory getandmakechild
new lateralviewforwarddesc    new rowschema lvforwardrr getcolumninfos
op   lvforwardrr
// the order in which the two paths are added is important. the
// lateral view join operator depends on having the select operator
// give it the row first.
// get the all path by making a select(*).
rowresolver allpathrr   opparsectx get lvforward  getrowresolver
//operator allpath = op;
operator allpath   putopinsertmap operatorfactory getandmakechild
new selectdesc true   new rowschema allpathrr getcolumninfos
lvforward   allpathrr
// get the udtf path
qb blankqb   new qb null  null  false
operator udtfpath   genselectplan  astnode  lateralviewtree
getchild 0   blankqb  lvforward
// add udtf aliases to qb
for  string udtfalias   blankqb getaliases
qb addalias udtfalias
rowresolver udtfpathrr   opparsectx get udtfpath  getrowresolver
// merge the two into the lateral view join
// the cols of the merged result will be the combination of both the
// cols of the udtf path and the cols of the all path. the internal
// names have to be changed to avoid conflicts
rowresolver lateralviewrr   new rowresolver
arraylist<string> outputinternalcolnames   new arraylist<string>
lvmergerowresolvers allpathrr  lateralviewrr  outputinternalcolnames
lvmergerowresolvers udtfpathrr  lateralviewrr  outputinternalcolnames
// for ppd, we need a column to expression map so that during the walk,
// the processor knows how to transform the internal col names.
// following steps are dependant on the fact that we called
// lvmerge.. in the above order
map<string  exprnodedesc> colexprmap   new hashmap<string  exprnodedesc>
int i 0
for  columninfo c   allpathrr getcolumninfos
string internalname   getcolumninternalname i
i
colexprmap put internalname
new exprnodecolumndesc c gettype    c getinternalname
c gettabalias    c getisvirtualcol
operator lateralviewjoin   putopinsertmap operatorfactory
getandmakechild new lateralviewjoindesc outputinternalcolnames
new rowschema lateralviewrr getcolumninfos     allpath
udtfpath   lateralviewrr
lateralviewjoin setcolumnexprmap colexprmap
op   lateralviewjoin
e setvalue op
/**
* a helper function that gets all the columns and respective aliases in the
* source and puts them into dest. it renames the internal names of the
* columns based on getcolumninternalname(position).
*
* note that this helper method relies on rowresolver.getcolumninfos()
* returning the columns in the same order as they will be passed in the
* operator dag.
*
* @param source
* @param dest
* @param outputcolnames
*          - a list to which the new internal column names will be added, in
*          the same order as in the dest row resolver
*/
private void lvmergerowresolvers rowresolver source  rowresolver dest
arraylist<string> outputinternalcolnames
for  columninfo c   source getcolumninfos
string internalname   getcolumninternalname outputinternalcolnames size
outputinternalcolnames add internalname
columninfo newcol   new columninfo internalname  c gettype    c
gettabalias    c getisvirtualcol
string tablecol   source reverselookup c getinternalname
string tablealias   tablecol
string colalias   tablecol
dest put tablealias  colalias  newcol
/**
* a helper function to generate a column stats task on top of map-red task. the column stats
* task fetches from the output of the map-red task, constructs the column stats object and
* persists  it to the metastore.
*
* this method generates a plan with a column stats task on top of map-red task and sets up the
* appropriate metadata to be used during execution.
*
* @param qb
*/
private void gencolumnstatstask qb qb
qbparseinfo qbparseinfo   qb getparseinfo
columnstatstask cstatstask   null
columnstatswork cstatswork   null
fetchwork fetch   null
string tablename   qbparseinfo gettablename
string partname   qbparseinfo getpartname
list<string> colname   qbparseinfo getcolname
list<string> coltype   qbparseinfo getcoltype
boolean istbllevel   qbparseinfo istbllvl
string cols   loadfilework get 0  getcolumns
string coltypes   loadfilework get 0  getcolumntypes
string resfileformat   hiveconf getvar conf  hiveconf confvars hivequeryresultfileformat
tabledesc resulttab   planutils getdefaultqueryoutputtabledesc cols  coltypes  resfileformat
fetch   new fetchwork new path loadfilework get 0  getsourcedir    tostring
resulttab  qb getparseinfo   getouterquerylimit
columnstatsdesc cstatsdesc   new columnstatsdesc tablename  partname
colname  coltype  istbllevel
cstatswork   new columnstatswork fetch  cstatsdesc
cstatstask    columnstatstask  taskfactory get cstatswork  conf
roottasks add cstatstask
@suppresswarnings
private void genmapredtasks parsecontext pctx  throws semanticexception
boolean iscstats   qb isanalyzerewrite
if  pctx getfetchtask      null
// replaced by single fetch task
initparsectx pctx
return
initparsectx pctx
list<task<movework>> mvtask   new arraylist<task<movework>>
/* in case of a select, use a fetch task instead of a move task.
* if the select is from analyze table column rewrite, don't create a fetch task. instead create
* a column stats task later.
*/
if  qb getisquery       iscstats
if    loadtablework isempty        loadfilework size      1
throw new semanticexception errormsg generic_error getmsg
string cols   loadfilework get 0  getcolumns
string coltypes   loadfilework get 0  getcolumntypes
string resfileformat   hiveconf getvar conf  hiveconf confvars hivequeryresultfileformat
tabledesc resulttab   planutils getdefaultqueryoutputtabledesc cols  coltypes  resfileformat
fetchwork fetch   new fetchwork new path loadfilework get 0  getsourcedir    tostring
resulttab  qb getparseinfo   getouterquerylimit
fetchtask fetchtask    fetchtask  taskfactory get fetch  conf
setfetchtask fetchtask
// for the fetchtask, the limit optimiztion requires we fetch all the rows
// in memory and count how many rows we get. it's not practical if the
// limit factor is too big
int fetchlimit   hiveconf getintvar conf  hiveconf confvars hivelimitoptmaxfetch
if  globallimitctx isenable      globallimitctx getgloballimit   > fetchlimit
log info     globallimitctx getgloballimit         fetchlimit
globallimitctx disableopt
else if   iscstats
for  loadtabledesc ltd   loadtablework
task<movework> tsk   taskfactory get new movework null  null  ltd  null  false
conf
mvtask add tsk
// check to see if we are stale'ing any indexes and auto-update them if we want
if  hiveconf getboolvar conf  hiveconf confvars hiveindexautoupdate
indexupdater indexupdater   new indexupdater loadtablework  getinputs    conf
try
list<task<? extends serializable>> indexupdatetasks   indexupdater generateupdatetasks
for  task<? extends serializable> updatetask   indexupdatetasks
tsk adddependenttask updatetask
catch  hiveexception e
console printinfo
boolean oneloadfile   true
for  loadfiledesc lfd   loadfilework
if  qb isctas
assert  oneloadfile      should not have more than 1 load file for
// ctas
// make the movetask's destination directory the table's destination.
string location   qb gettabledesc   getlocation
if  location    null
// get the table's default location
table dumptable
path targetpath
try
dumptable   db newtable qb gettabledesc   gettablename
if   db databaseexists dumptable getdbname
throw new semanticexception     dumptable getdbname
warehouse wh   new warehouse conf
targetpath   wh gettablepath db getdatabase dumptable getdbname     dumptable
gettablename
catch  hiveexception e
throw new semanticexception e
catch  metaexception e
throw new semanticexception e
location   targetpath tostring
lfd settargetdir location
oneloadfile   false
mvtask add taskfactory get new movework null  null  null  lfd  false
conf
// generate map reduce plans
parsecontext tempparsecontext   getparsecontext
genmrproccontext procctx   new genmrproccontext
conf
new hashmap<operator<? extends operatordesc>  task<? extends serializable>>
new arraylist<operator<? extends operatordesc>>    tempparsecontext
mvtask  roottasks
new linkedhashmap<operator<? extends operatordesc>  genmapredctx>
inputs  outputs
// create a walker which walks the tree in a dfs manner while maintaining
// the operator stack.
// the dispatcher generates the plan from the operator tree
map<rule  nodeprocessor> oprules   new linkedhashmap<rule  nodeprocessor>
oprules put new ruleregexp new string
tablescanoperator getoperatorname
new genmrtablescan1
oprules put new ruleregexp new string
tablescanoperator getoperatorname         reducesinkoperator getoperatorname
new genmrredsink1
oprules put new ruleregexp new string
reducesinkoperator getoperatorname         reducesinkoperator getoperatorname
new genmrredsink2
oprules put new ruleregexp new string
filesinkoperator getoperatorname
new genmrfilesink1
oprules put new ruleregexp new string
unionoperator getoperatorname
new genmrunion1
oprules put new ruleregexp new string
unionoperator getoperatorname         reducesinkoperator getoperatorname
new genmrredsink3
oprules put new ruleregexp new string
mapjoinoperator getoperatorname         reducesinkoperator getoperatorname
new genmrredsink4
oprules put new ruleregexp new string
tablescanoperator getoperatorname         mapjoinoperator getoperatorname
mapjoinfactory gettablescanmapjoin
oprules put new ruleregexp new string
reducesinkoperator getoperatorname         mapjoinoperator getoperatorname
mapjoinfactory getreducesinkmapjoin
oprules put new ruleregexp new string
unionoperator getoperatorname         mapjoinoperator getoperatorname
mapjoinfactory getunionmapjoin
oprules put new ruleregexp new string
mapjoinoperator getoperatorname         mapjoinoperator getoperatorname
mapjoinfactory getmapjoinmapjoin
oprules put new ruleregexp new string
mapjoinoperator getoperatorname         selectoperator getoperatorname
mapjoinfactory getmapjoin
// the dispatcher fires the processor corresponding to the closest matching
// rule and passes the context along
dispatcher disp   new defaultruledispatcher new genmroperator    oprules
procctx
graphwalker ogw   new genmapredwalker disp
arraylist<node> topnodes   new arraylist<node>
topnodes addall topops values
ogw startwalking topnodes  null
/* if the query was the result of analyze table column compute statistics rewrite, create
* a column stats task instead of a fetch task to persist stats to the metastore.
*/
if  iscstats
gencolumnstatstask qb
// reduce sink does not have any kids - since the plan by now has been
// broken up into multiple
// tasks, iterate over all tasks.
// for each task, go over all operators recursively
for  task<? extends serializable> roottask   roottasks
breaktasktree roottask
// for each task, set the key descriptor for the reducer
for  task<? extends serializable> roottask   roottasks
setkeydesctasktree roottask
// if a task contains an operator which instructs bucketizedhiveinputformat
// to be used, please do so
for  task<? extends serializable> roottask   roottasks
setinputformat roottask
physicalcontext physicalcontext   new physicalcontext conf
getparsecontext    ctx  roottasks  fetchtask
physicaloptimizer physicaloptimizer   new physicaloptimizer
physicalcontext  conf
physicaloptimizer optimize
// for each operator, generate the counters if needed
if  hiveconf getboolvar conf  hiveconf confvars hivejobprogress
for  task<? extends serializable> roottask   roottasks
generatecounterstask roottask
decideexecmode roottasks  ctx  globallimitctx
if  qb isctas
// generate a ddl task and make it a dependent task of the leaf
createtabledesc crttbldesc   qb gettabledesc
crttbldesc validate
// clear the output for ctas since we don't need the output from the
// mapredwork, the
// ddlwork at the tail of the chain will have the output
getoutputs   clear
task<? extends serializable> crttbltask   taskfactory get new ddlwork
getinputs    getoutputs    crttbldesc   conf
// find all leaf tasks and make the ddltask as a dependent task of all of
// them
hashset<task<? extends serializable>> leaves   new hashset<task<? extends serializable>>
getleaftasks roottasks  leaves
assert  leaves size   > 0
for  task<? extends serializable> task   leaves
if  task instanceof statstask
//statstask require table to already exist
for  task<? extends serializable> parentofstatstask   task getparenttasks
parentofstatstask adddependenttask crttbltask
for  task<? extends serializable> parentofcrttbltask   crttbltask getparenttasks
parentofcrttbltask removedependenttask task
crttbltask adddependenttask task
else
task adddependenttask crttbltask
if  globallimitctx isenable      fetchtask    null
int fetchlimit   hiveconf getintvar conf  hiveconf confvars hivelimitoptmaxfetch
log info     globallimitctx getgloballimit
fetchtask getwork   setleastnumrows globallimitctx getgloballimit
if  globallimitctx isenable      globallimitctx getlastreducelimitdesc      null
log info     globallimitctx getgloballimit
globallimitctx getlastreducelimitdesc   setleastrows globallimitctx getgloballimit
list<execdriver> mrtasks   utilities getmrtasks roottasks
for  execdriver tsk   mrtasks
tsk setretrycmdwhenfail true
/**
* find all leaf tasks of the list of root tasks.
*/
private void getleaftasks list<task<? extends serializable>> roottasks
hashset<task<? extends serializable>> leaves
for  task<? extends serializable> root   roottasks
getleaftasks root  leaves
private void getleaftasks task<? extends serializable> task
hashset<task<? extends serializable>> leaves
if  task getdependenttasks      null
if   leaves contains task
leaves add task
else
getleaftasks task getdependenttasks    leaves
// loop over all the tasks recursviely
private void generatecounterstask task<? extends serializable> task
if  task instanceof execdriver
hashmap<string  operator<? extends operatordesc>> opmap     mapredwork  task
getwork    getaliastowork
if   opmap isempty
for  operator<? extends operatordesc> op   opmap values
generatecountersoperator op
operator<? extends operatordesc> reducer     mapredwork  task getwork
getreducer
if  reducer    null
log info     reducer
generatecountersoperator reducer
else if  task instanceof conditionaltask
list<task<? extends serializable>> listtasks     conditionaltask  task
getlisttasks
for  task<? extends serializable> tsk   listtasks
generatecounterstask tsk
// start the counters from scratch - a hack for hadoop 17.
operator resetlastenumused
if  task getchildtasks      null
return
for  task<? extends serializable> childtask   task getchildtasks
generatecounterstask childtask
private void generatecountersoperator operator<? extends operatordesc> op
op assigncounternametoenum
if  op getchildoperators      null
return
for  operator<? extends operatordesc> child   op getchildoperators
generatecountersoperator child
// loop over all the tasks recursviely
private void breaktasktree task<? extends serializable> task
if  task instanceof execdriver
hashmap<string  operator<? extends operatordesc>> opmap     mapredwork  task
getwork    getaliastowork
if   opmap isempty
for  operator<? extends operatordesc> op   opmap values
breakoperatortree op
else if  task instanceof conditionaltask
list<task<? extends serializable>> listtasks     conditionaltask  task
getlisttasks
for  task<? extends serializable> tsk   listtasks
breaktasktree tsk
if  task getchildtasks      null
return
for  task<? extends serializable> childtask   task getchildtasks
breaktasktree childtask
// loop over all the operators recursviely
private void breakoperatortree operator<? extends operatordesc> topop
if  topop instanceof reducesinkoperator
topop setchildoperators null
if  topop getchildoperators      null
return
for  operator<? extends operatordesc> op   topop getchildoperators
breakoperatortree op
private void setinputformat mapredwork work  operator<? extends operatordesc> op
if  op isusebucketizedhiveinputformat
work setusebucketizedhiveinputformat true
return
if  op getchildoperators      null
for  operator<? extends operatordesc> childop   op getchildoperators
setinputformat work  childop
// loop over all the tasks recursviely
private void setinputformat task<? extends serializable> task
if  task instanceof execdriver
mapredwork work    mapredwork  task getwork
hashmap<string  operator<? extends operatordesc>> opmap   work getaliastowork
if   opmap isempty
for  operator<? extends operatordesc> op   opmap values
setinputformat work  op
else if  task instanceof conditionaltask
list<task<? extends serializable>> listtasks     conditionaltask  task  getlisttasks
for  task<? extends serializable> tsk   listtasks
setinputformat tsk
if  task getchildtasks      null
for  task<? extends serializable> childtask   task getchildtasks
setinputformat childtask
// loop over all the tasks recursviely
private void setkeydesctasktree task<? extends serializable> task
if  task instanceof execdriver
mapredwork work    mapredwork  task getwork
work deriveexplainattributes
hashmap<string  operator<? extends operatordesc>> opmap   work
getaliastowork
if   opmap isempty
for  operator<? extends operatordesc> op   opmap values
genmapredutils setkeyandvaluedesc work  op
else if  task instanceof conditionaltask
list<task<? extends serializable>> listtasks     conditionaltask  task
getlisttasks
for  task<? extends serializable> tsk   listtasks
setkeydesctasktree tsk
if  task getchildtasks      null
return
for  task<? extends serializable> childtask   task getchildtasks
setkeydesctasktree childtask
@suppresswarnings
public phase1ctx initphase1ctx
phase1ctx ctx_1   new phase1ctx
ctx_1 nextnum   0
ctx_1 dest
return ctx_1
@override
public void init
// clear most members
reset
// init
qb qb   new qb null  null  false
this qb   qb
@override
@suppresswarnings
public void analyzeinternal astnode ast  throws semanticexception
astnode child   ast
this ast   ast
viewsexpanded   new arraylist<string>
log info
// analyze create table command
if  ast gettoken   gettype      hiveparser tok_createtable
// if it is not ctas, we don't need to go further and just return
if   child   analyzecreatetable ast  qb      null
return
else
sessionstate get   setcommandtype hiveoperation query
// analyze create view command
if  ast gettoken   gettype      hiveparser tok_createview
child   analyzecreateview ast  qb
sessionstate get   setcommandtype hiveoperation createview
if  child    null
return
viewselect   child
// prevent view from referencing itself
viewsexpanded add db getcurrentdatabase     createvwdesc getviewname
// continue analyzing from the child astnode.
if   dophase1 child  qb  initphase1ctx
// if phase1result false return
return
log info
getmetadata qb
log info
// save the result schema derived from the sink operator produced
// by genplan.  this has the correct column names, which clients
// such as jdbc would prefer instead of the c0, c1 we'll end
// up with later.
operator sinkop   genplan qb
resultschema
convertrowschematoviewschema opparsectx get sinkop  getrowresolver
if  createvwdesc    null
saveviewdefinition
// since we're only creating a view (not executing it), we
// don't need to optimize or translate the plan (and in fact, those
// procedures can interfere with the view creation). so
// skip the rest of this method.
ctx setresdir null
ctx setresfile null
return
parsecontext pctx   new parsecontext conf  qb  child  optopartpruner
optopartlist  topops  topselops  opparsectx  joincontext  toptotable
loadtablework  loadfilework  ctx  idtotablenamemap  desttableid  uctx
listmapjoinopsnoreducer  groupoptoinputtables  prunedpartitions
optosamplepruner  globallimitctx  nametosplitsample  inputs  roottasks
optoparttoskewedpruner
// generate table access stats if required
if  hiveconf getboolvar this conf  hiveconf confvars hive_stats_collect_tablekeys     true
tableaccessanalyzer tableaccessanalyzer   new tableaccessanalyzer pctx
settableaccessinfo tableaccessanalyzer analyzetableaccess
optimizer optm   new optimizer
optm setpctx pctx
optm initialize conf
pctx   optm optimize
// at this point we have the complete operator tree
// from which we want to find the reduce operator
genmapredtasks pctx
log info
return
@override
public list<fieldschema> getresultschema
return resultschema
private void saveviewdefinition   throws semanticexception
// make a copy of the statement's result schema, since we may
// modify it below as part of imposing view column names.
list<fieldschema> derivedschema
new arraylist<fieldschema> resultschema
parseutils validatecolumnnameuniqueness derivedschema
list<fieldschema> imposedschema   createvwdesc getschema
if  imposedschema    null
int explicitcolcount   imposedschema size
int derivedcolcount   derivedschema size
if  explicitcolcount    derivedcolcount
throw new semanticexception generateerrormessage
viewselect
errormsg view_col_mismatch getmsg
// preserve the original view definition as specified by the user.
string originaltext   ctx gettokenrewritestream   tostring
viewselect gettokenstartindex    viewselect gettokenstopindex
createvwdesc setvieworiginaltext originaltext
// now expand the view definition with extras such as explicit column
// references; this expanded form is what we'll re-parse when the view is
// referenced later.
unparsetranslator applytranslations ctx gettokenrewritestream
string expandedtext   ctx gettokenrewritestream   tostring
viewselect gettokenstartindex    viewselect gettokenstopindex
if  imposedschema    null
// merge the names from the imposed schema into the types
// from the derived schema.
stringbuilder sb   new stringbuilder
sb append
int n   derivedschema size
for  int i   0  i < n    i
if  i > 0
sb append
fieldschema fieldschema   derivedschema get i
// modify a copy, not the original
fieldschema   new fieldschema fieldschema
derivedschema set i  fieldschema
sb append hiveutils unparseidentifier fieldschema getname
sb append
string imposedname   imposedschema get i  getname
sb append hiveutils unparseidentifier imposedname
fieldschema setname imposedname
// we don't currently allow imposition of a type
fieldschema setcomment imposedschema get i  getcomment
sb append
sb append expandedtext
sb append
sb append hiveutils unparseidentifier createvwdesc getviewname
expandedtext   sb tostring
if  createvwdesc getpartcolnames      null
// make sure all partitioning columns referenced actually
// exist and are in the correct order at the end
// of the list of columns produced by the view.  also move the field
// schema descriptors from derivedschema to the partitioning key
// descriptor.
list<string> partcolnames   createvwdesc getpartcolnames
if  partcolnames size   > derivedschema size
throw new semanticexception
errormsg view_partition_mismatch getmsg
// get the partition columns from the end of derivedschema.
list<fieldschema> partitioncolumns   derivedschema sublist
derivedschema size     partcolnames size
derivedschema size
// verify that the names match the partitioned on clause.
iterator<string> colnameiter   partcolnames iterator
iterator<fieldschema> schemaiter   partitioncolumns iterator
while  colnameiter hasnext
string colname   colnameiter next
fieldschema fieldschema   schemaiter next
if   fieldschema getname   equals colname
throw new semanticexception
errormsg view_partition_mismatch getmsg
// boundary case:  require at least one non-partitioned column
// for consistency with tables.
if  partcolnames size      derivedschema size
throw new semanticexception
errormsg view_partition_total getmsg
// now make a copy.
createvwdesc setpartcols
new arraylist<fieldschema> partitioncolumns
// finally, remove the partition columns from the end of derivedschema.
// (clearing the sublist writes through to the underlying
// derivedschema arraylist.)
partitioncolumns clear
createvwdesc setschema derivedschema
createvwdesc setviewexpandedtext expandedtext
private list<fieldschema> convertrowschematoviewschema rowresolver rr
list<fieldschema> fieldschemas   new arraylist<fieldschema>
for  columninfo colinfo   rr getcolumninfos
if  colinfo ishiddenvirtualcol
continue
string colname   rr reverselookup colinfo getinternalname
fieldschemas add new fieldschema colname
colinfo gettype   gettypename    null
return fieldschemas
/**
* generates an expression node descriptor for the expression passed in the
* arguments. this function uses the row resolver and the metadata information
* that are passed as arguments to resolve the column names to internal names.
*
* @param expr
*          the expression
* @param input
*          the row resolver
* @return exprnodedesc
* @throws semanticexception
*/
public exprnodedesc genexprnodedesc astnode expr  rowresolver input
throws semanticexception
// since the user didn't supply a customized type-checking context,
// use default settings.
typecheckctx tcctx   new typecheckctx input
return genexprnodedesc expr  input  tcctx
/**
* generates an expression node descriptor for the expression passed in the
* arguments. this function uses the row resolver and the metadata information
* that are passed as arguments to resolve the column names to internal names.
*
* @param expr
*          the expression
* @param input
*          the row resolver
* @param tcctx
*          customized type-checking context
* @return exprnodedesc
* @throws semanticexception
*/
@suppresswarnings
public exprnodedesc genexprnodedesc astnode expr  rowresolver input
typecheckctx tcctx  throws semanticexception
// we recursively create the exprnodedesc. base cases: when we encounter
// a column ref, we convert that into an exprnodecolumndesc; when we
// encounter
// a constant, we convert that into an exprnodeconstantdesc. for others we
// just
// build the exprnodefuncdesc with recursively built children.
// if the current subexpression is pre-calculated, as in group-by etc.
columninfo colinfo   input getexpression expr
if  colinfo    null
astnode source   input getexpressionsource expr
if  source    null
unparsetranslator addcopytranslation expr  source
return new exprnodecolumndesc colinfo gettype    colinfo
getinternalname    colinfo gettabalias    colinfo
getisvirtualcol    colinfo isskewedcol
// create the walker and  the rules dispatcher.
tcctx setunparsetranslator unparsetranslator
hashmap<node  object> nodeoutputs
typecheckprocfactory genexprnode expr  tcctx
exprnodedesc desc    exprnodedesc  nodeoutputs get expr
if  desc    null
string errmsg   tcctx geterror
if   errmsg    null
errmsg
throw new semanticexception errmsg
if   unparsetranslator isenabled
// not creating a view, so no need to track view expansions.
return desc
for  map entry<node  object> entry   nodeoutputs entryset
if    entry getkey   instanceof astnode
continue
if    entry getvalue   instanceof exprnodecolumndesc
continue
astnode node    astnode  entry getkey
exprnodecolumndesc columndesc    exprnodecolumndesc  entry getvalue
if   columndesc gettabalias      null
columndesc gettabalias   length      0
// these aren't real column refs; instead, they are special
// internal expressions used in the representation of aggregation.
continue
string tmp   input reverselookup columndesc getcolumn
stringbuilder replacementtext   new stringbuilder
replacementtext append hiveutils unparseidentifier tmp
replacementtext append
replacementtext append hiveutils unparseidentifier tmp
unparsetranslator addtranslation node  replacementtext tostring
return desc
@override
public void validate   throws semanticexception
log debug
// validate inputs and outputs have right protectmode to execute the query
for  readentity readentity  getinputs
readentity type type   readentity gettype
if  type    readentity type table
type    readentity type partition
// in current implementation it will never happen, but we leave it
// here to make the logic complete.
continue
table tbl   readentity gettable
partition p   readentity getpartition
if  tbl isoffline
throw new semanticexception
errormsg offline_table_or_partition getmsg
tbl gettablename
if  type    readentity type partition    p    null    p isoffline
throw new semanticexception
errormsg offline_table_or_partition getmsg
tbl gettablename
p getname
for  writeentity writeentity  getoutputs
writeentity type type   writeentity gettype
if type    writeentity type partition    type    writeentity type dummypartition
string conflictingarchive
try
partition usedp   writeentity getpartition
table tbl   usedp gettable
log debug     usedp getname
log debug usedp gettable
conflictingarchive   archiveutils
conflictingarchivenameornull db  tbl  usedp getspec
catch  hiveexception e
throw new semanticexception e
if conflictingarchive    null
string message   string format
conflictingarchive
throw new semanticexception message
if  type    writeentity type table
type    writeentity type partition
log debug
continue
table tbl
partition p
if  type    writeentity type partition
partition inputpartition   writeentity getpartition
// if it is a partition, partition's metastore is not fetched. we
// need to fetch it.
try
p   hive get   getpartition
inputpartition gettable    inputpartition getspec    false
if  p    null
tbl   p gettable
else
// if p is null, we assume that we insert to a new partition
tbl   inputpartition gettable
catch  hiveexception e
throw new semanticexception e
if  type    writeentity type partition    p  null    p isoffline
throw new semanticexception
errormsg offline_table_or_partition getmsg
tbl gettablename
p getname
else
log debug
tbl   writeentity gettable
if  tbl isoffline
throw new semanticexception
errormsg offline_table_or_partition getmsg
tbl gettablename
boolean reworkmapredwork   hiveconf getboolvar this conf  hiveconf confvars hive_rework_mapredwork
// validate all tasks
for  task<? extends serializable> roottask   roottasks
validate roottask  reworkmapredwork
private void validate task<? extends serializable> task  boolean reworkmapredwork
throws semanticexception
utilities reworkmapredwork task  reworkmapredwork  conf
if  task getchildtasks      null
return
for  task<? extends serializable> childtask   task getchildtasks
validate childtask  reworkmapredwork
/**
* get the row resolver given an operator.
*/
public rowresolver getrowresolver operator opt
return opparsectx get opt  getrowresolver
/**
* add default properties for table property. if a default parameter exists
* in the tblprop, the value in tblprop will be kept.
* @param table property map
* @return modified table property map
*/
private map<string  string> adddefaultproperties map<string  string> tblprop
map<string  string> retvalue
if  tblprop    null
retvalue   new hashmap<string  string>
else
retvalue   tblprop
string parastring   hiveconf getvar conf  confvars newtabledefaultpara
if  parastring    null     parastring isempty
for  string keyvaluepair  parastring split
string keyvalue   keyvaluepair split    2
if  keyvalue length    2
continue
if   retvalue containskey keyvalue
retvalue put keyvalue  keyvalue
return retvalue
/**
* analyze the create table command. if it is a regular create-table or
* create-table-like statements, we create a ddlwork and return true. if it is
* a create-table-as-select, we get the necessary info such as the serde and
* storage format and put it in qb, and return false, indicating the rest of
* the semantic analyzer need to deal with the select statement with respect
* to the serde and storage format.
*/
private astnode analyzecreatetable astnode ast  qb qb
throws semanticexception
string tablename   getunescapedname  astnode ast getchild 0
string liketablename   null
list<fieldschema> cols   new arraylist<fieldschema>
list<fieldschema> partcols   new arraylist<fieldschema>
list<string> bucketcols   new arraylist<string>
list<order> sortcols   new arraylist<order>
int numbuckets    1
string comment   null
string location   null
map<string  string> tblprops   null
boolean ifnotexists   false
boolean isext   false
astnode selectstmt   null
final int create_table   0     regular create table
final int ctlt   1     create table like      ctlt
final int ctas   2     create table as select      ctas
int command_type   create_table
list<string> skewedcolnames   new arraylist<string>
list<list<string>> skewedvalues   new arraylist<list<string>>
map<list<string>  string> listbucketcolvaluesmapping   new hashmap<list<string>  string>
boolean storedasdirs   false
rowformatparams rowformatparams   new rowformatparams
storageformat storageformat   new storageformat
analyzecreatecommonvars shared   new analyzecreatecommonvars
log info     tablename
ast getcharpositioninline
int numch   ast getchildcount
/*
* check the 1st-level children and do simple semantic checks: 1) ctlt and
* ctas should not coexists. 2) ctlt or ctas should not coexists with column
* list (target table schema). 3) ctas does not support partitioning (for
* now).
*/
for  int num   1  num < numch  num
astnode child    astnode  ast getchild num
if  storageformat fillstorageformat child  shared
continue
switch  child gettoken   gettype
case hiveparser tok_ifnotexists
ifnotexists   true
break
case hiveparser kw_external
isext   true
break
case hiveparser tok_liketable
if  child getchildcount   > 0
liketablename   getunescapedname  astnode child getchild 0
if  liketablename    null
if  command_type    ctas
throw new semanticexception errormsg ctas_ctlt_coexistence
getmsg
if  cols size      0
throw new semanticexception errormsg ctlt_collst_coexistence
getmsg
command_type   ctlt
break
case hiveparser tok_query     ctas
if  command_type    ctlt
throw new semanticexception errormsg ctas_ctlt_coexistence getmsg
if  cols size      0
throw new semanticexception errormsg ctas_collst_coexistence getmsg
if  partcols size      0    bucketcols size      0
boolean dynpart   hiveconf getboolvar conf  hiveconf confvars dynamicpartitioning
if  dynpart    false
throw new semanticexception errormsg ctas_parcol_coexistence getmsg
else
// todo: support dynamic partition for ctas
throw new semanticexception errormsg ctas_parcol_coexistence getmsg
if  isext
throw new semanticexception errormsg ctas_exttbl_coexistence getmsg
command_type   ctas
selectstmt   child
break
case hiveparser tok_tabcollist
cols   getcolumns child
break
case hiveparser tok_tablecomment
comment   unescapesqlstring child getchild 0  gettext
break
case hiveparser tok_tablepartcols
partcols   getcolumns  astnode  child getchild 0   false
break
case hiveparser tok_tablebuckets
bucketcols   getcolumnnames  astnode  child getchild 0
if  child getchildcount      2
numbuckets    integer valueof child getchild 1  gettext
intvalue
else
sortcols   getcolumnnamesorder  astnode  child getchild 1
numbuckets    integer valueof child getchild 2  gettext
intvalue
break
case hiveparser tok_tablerowformat
rowformatparams analyzerowformat shared  child
break
case hiveparser tok_tablelocation
location   unescapesqlstring child getchild 0  gettext
location   eximutil relativetoabsolutepath conf  location
break
case hiveparser tok_tableproperties
tblprops   ddlsemanticanalyzer getprops  astnode  child getchild 0
break
case hiveparser tok_tableserializer
child    astnode  child getchild 0
shared serde   unescapesqlstring child getchild 0  gettext
if  child getchildcount      2
readprops  astnode   child getchild 1  getchild 0
shared serdeprops
break
case hiveparser tok_fileformat_generic
handlegenericfileformat child
break
case hiveparser tok_tableskewed
/**
* throw an error if the user tries to use the ddl with
* hive.internal.ddl.list.bucketing.enable set to false.
*/
hiveconf hiveconf   sessionstate get   getconf
if    hiveconf getboolvar hiveconf confvars hive_internal_ddl_list_bucketing_enable
throw new semanticexception errormsg hive_internal_ddl_list_bucketing_disabled getmsg
// skewed column names
skewedcolnames   analyzeskewedtablddlcolnames skewedcolnames  child
// skewed value
analyzeddlskewedvalues skewedvalues  child
// stored as directories
storedasdirs   analyzestoredaddirs child
break
default
assert false
storageformat filldefaultstorageformat shared
if   command_type    ctas      storageformat storagehandler    null
throw new semanticexception errormsg create_non_native_as getmsg
// check for existence of table
if  ifnotexists
try
list<string> tables   db gettablesbypattern tablename
if  tables    null    tables size   > 0       table exists
return null
catch  hiveexception e
e printstacktrace
// handle different types of create table command
createtabledesc crttbldesc   null
switch  command_type
case create_table     regular create table ddl
tblprops   adddefaultproperties tblprops
crttbldesc   new createtabledesc tablename  isext  cols  partcols
bucketcols  sortcols  numbuckets  rowformatparams fielddelim  rowformatparams fieldescape
rowformatparams collitemdelim  rowformatparams mapkeydelim  rowformatparams linedelim  comment
storageformat inputformat  storageformat outputformat  location  shared serde
storageformat storagehandler  shared serdeprops  tblprops  ifnotexists  skewedcolnames
skewedvalues
crttbldesc setstoredassubdirectories storedasdirs
crttbldesc validate
// outputs is empty, which means this create table happens in the current
// database.
sessionstate get   setcommandtype hiveoperation createtable
roottasks add taskfactory get new ddlwork getinputs    getoutputs
crttbldesc   conf
break
case ctlt     create table like <tbl_name>
createtablelikedesc crttbllikedesc   new createtablelikedesc tablename  isext
storageformat inputformat  storageformat outputformat  location
shared serde  shared serdeprops  ifnotexists  liketablename
sessionstate get   setcommandtype hiveoperation createtable
roottasks add taskfactory get new ddlwork getinputs    getoutputs
crttbllikedesc   conf
break
case ctas     create table as select
// verify that the table does not already exist
string databasename
try
table dumptable   db newtable tablename
databasename   dumptable getdbname
if  null    db getdatabase dumptable getdbname
throw new semanticexception errormsg database_not_exists getmsg dumptable getdbname
if  null    db gettable dumptable getdbname    dumptable gettablename    false
throw new semanticexception errormsg table_already_exists getmsg tablename
catch  hiveexception e
throw new semanticexception e
tblprops   adddefaultproperties tblprops
crttbldesc   new createtabledesc databasename  tablename  isext  cols  partcols
bucketcols  sortcols  numbuckets  rowformatparams fielddelim  rowformatparams fieldescape
rowformatparams collitemdelim  rowformatparams mapkeydelim  rowformatparams linedelim  comment  storageformat inputformat
storageformat outputformat  location  shared serde  storageformat storagehandler  shared serdeprops
tblprops  ifnotexists  skewedcolnames  skewedvalues
crttbldesc setstoredassubdirectories storedasdirs
qb settabledesc crttbldesc
sessionstate get   setcommandtype hiveoperation createtable_as_select
return selectstmt
default
throw new semanticexception
return null
private astnode analyzecreateview astnode ast  qb qb
throws semanticexception
string tablename   getunescapedname  astnode ast getchild 0
list<fieldschema> cols   null
boolean ifnotexists   false
boolean orreplace   false
string comment   null
astnode selectstmt   null
map<string  string> tblprops   null
list<string> partcolnames   null
log info     tablename
ast getcharpositioninline
int numch   ast getchildcount
for  int num   1  num < numch  num
astnode child    astnode  ast getchild num
switch  child gettoken   gettype
case hiveparser tok_ifnotexists
ifnotexists   true
break
case hiveparser tok_orreplace
orreplace   true
break
case hiveparser tok_query
selectstmt   child
break
case hiveparser tok_tabcolname
cols   getcolumns child
break
case hiveparser tok_tablecomment
comment   unescapesqlstring child getchild 0  gettext
break
case hiveparser tok_tableproperties
tblprops   ddlsemanticanalyzer getprops  astnode  child getchild 0
break
case hiveparser tok_viewpartcols
partcolnames   getcolumnnames  astnode  child getchild 0
break
default
assert false
if  ifnotexists    orreplace
throw new semanticexception
createvwdesc   new createviewdesc
tablename  cols  comment  tblprops  partcolnames  ifnotexists  orreplace
unparsetranslator enable
roottasks add taskfactory get new ddlwork getinputs    getoutputs
createvwdesc   conf
return selectstmt
private void decideexecmode list<task<? extends serializable>> roottasks  context ctx
globallimitctx globallimitctx
throws semanticexception
// bypass for explain queries for now
if  ctx getexplain
return
// user has told us to run in local mode or doesn't want auto-local mode
if  ctx islocalonlyexecutionmode
conf getboolvar hiveconf confvars localmodeauto
return
final context lctx   ctx
pathfilter p   new pathfilter
public boolean accept path file
return  lctx ismrtmpfileuri file touri   getpath
list<execdriver> mrtasks   utilities getmrtasks roottasks
// map-reduce jobs will be run locally based on data size
// first find out if any of the jobs needs to run non-locally
boolean hasnonlocaljob   false
for  execdriver mrtask  mrtasks
try
contentsummary inputsummary   utilities getinputsummary
ctx   mapredwork mrtask getwork    p
int numreducers   getnumberofreducers mrtask getwork    conf
long estimatedinput
if  globallimitctx    null    globallimitctx isenable
// if the global limit optimization is triggered, we will
// estimate input data actually needed based on limit rows.
// estimated input = (num_limit * max_size_per_row) * (estimated_map + 2)
//
long sizeperrow   hiveconf getlongvar conf
hiveconf confvars hivelimitmaxrowsize
estimatedinput   globallimitctx getgloballimit     sizeperrow
long minsplitsize   hiveconf getlongvar conf
hiveconf confvars mapredminsplitsize
long estimatednummap   inputsummary getlength     minsplitsize   1
estimatedinput   estimatedinput    estimatednummap   1
else
estimatedinput   inputsummary getlength
if  log isdebugenabled
log debug     mrtask getid
inputsummary getlength         inputsummary getfilecount
numreducers       estimatedinput
if  mapredtask iseligibleforlocalmode conf  numreducers
estimatedinput  inputsummary getfilecount       null
hasnonlocaljob   true
break
else
mrtask setlocalmode true
catch  ioexception e
throw new semanticexception  e
if  hasnonlocaljob
// entire query can be run locally.
// save the current tracker value and restore it when done.
ctx setoriginaltracker shimloader gethadoopshims   getjoblauncherrpcaddress conf
shimloader gethadoopshims   setjoblauncherrpcaddress conf
console printinfo
// if all the tasks can be run locally, we can use local disk for
// storing intermediate data.
/**
* this code is commented out pending further testing/development
* for (task<? extends operatordesc> t: roottasks)
* t.localizemrtmpfiles(ctx);
*/
/**
* make a best guess at trying to find the number of reducers
*/
private static int getnumberofreducers mapredwork mrwork  hiveconf conf
if  mrwork getreducer      null
return 0
if  mrwork getnumreducetasks   >  0
return mrwork getnumreducetasks
return conf getintvar hiveconf confvars hadoopnumreducers
public qb getqb
return qb
public void setqb qb qb
this qb   qb