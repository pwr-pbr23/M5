/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive conf
import java io file
import java io fileoutputstream
import java io ioexception
import java io printstream
import java net url
import java util hashmap
import java util iterator
import java util map
import java util map entry
import java util properties
import java util regex matcher
import java util regex pattern
import javax security auth login loginexception
import org apache commons lang stringutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop hive shims shimloader
import org apache hadoop mapred jobconf
import org apache hadoop security usergroupinformation
import org apache hadoop util shell
/**
* hive configuration.
*/
public class hiveconf extends configuration
protected string hivejar
protected properties origprop
protected string auxjars
private static final log l4j   logfactory getlog hiveconf class
private static url hivesiteurl   null
private static url confvarurl   null
private static final map<string  confvars> vars   new hashmap<string  confvars>
static
classloader classloader   thread currentthread   getcontextclassloader
if  classloader    null
classloader   hiveconf class getclassloader
// log a warning if hive-default.xml is found on the classpath
url hivedefaulturl   classloader getresource
if  hivedefaulturl    null
l4j warn
hivedefaulturl getpath
// look for hive-site.xml on the classpath and log its location if found.
hivesiteurl   classloader getresource
if  hivesiteurl    null
l4j warn
else
l4j debug     hivesiteurl getpath
for  confvars confvar   confvars values
vars put confvar varname  confvar
/**
* metastore related options that the db is initialized against. when a conf
* var in this is list is changed, the metastore instance for the cli will
* be recreated so that the change will take effect.
*/
public static final hiveconf confvars metavars
hiveconf confvars metastoredirectory
hiveconf confvars metastorewarehouse
hiveconf confvars metastoreuris
hiveconf confvars metastorethriftconnectionretries
hiveconf confvars metastorethriftfailureretries
hiveconf confvars metastore_client_connect_retry_delay
hiveconf confvars metastore_client_socket_timeout
hiveconf confvars metastorepwd
hiveconf confvars metastoreconnecturlhook
hiveconf confvars metastoreconnecturlkey
hiveconf confvars metastoreattempts
hiveconf confvars metastoreinterval
hiveconf confvars metastoreforcereloadconf
hiveconf confvars metastoreserverminthreads
hiveconf confvars metastoreservermaxthreads
hiveconf confvars metastore_tcp_keep_alive
hiveconf confvars metastore_int_original
hiveconf confvars metastore_int_archived
hiveconf confvars metastore_int_extracted
hiveconf confvars metastore_kerberos_keytab_file
hiveconf confvars metastore_kerberos_principal
hiveconf confvars metastore_use_thrift_sasl
hiveconf confvars metastore_cache_pinobjtypes
hiveconf confvars metastore_connection_pooling_type
hiveconf confvars metastore_validate_tables
hiveconf confvars metastore_validate_columns
hiveconf confvars metastore_validate_constraints
hiveconf confvars metastore_store_manager_type
hiveconf confvars metastore_auto_create_schema
hiveconf confvars metastore_auto_start_mechanism_mode
hiveconf confvars metastore_transaction_isolation
hiveconf confvars metastore_cache_level2
hiveconf confvars metastore_cache_level2_type
hiveconf confvars metastore_identifier_factory
hiveconf confvars metastore_plugin_registry_bundle_check
hiveconf confvars metastore_authorization_storage_auth_checks
hiveconf confvars metastore_batch_retrieve_max
hiveconf confvars metastore_event_listeners
hiveconf confvars metastore_event_clean_freq
hiveconf confvars metastore_event_expiry_duration
hiveconf confvars metastore_raw_store_impl
hiveconf confvars metastore_end_function_listeners
hiveconf confvars metastore_part_inherit_tbl_props
hiveconf confvars metastore_batch_retrieve_table_partition_max
hiveconf confvars metastore_pre_event_listeners
hiveconf confvars hmshandlerattempts
hiveconf confvars hmshandlerinterval
hiveconf confvars hmshandlerforcereloadconf
/**
* dbvars are the parameters can be set per database. if these
* parameters are set as a database property, when switching to that
* database, the hiveconf variable will be changed. the change of these
* parameters will effectively change the dfs and mapreduce clusters
* for different databases.
*/
public static final hiveconf confvars dbvars
hiveconf confvars hadoopbin
hiveconf confvars hadoopjt
hiveconf confvars metastorewarehouse
hiveconf confvars scratchdir
/**
* confvars.
*
* these are the default configuration properties for hive. each hiveconf
* object is initialized as follows:
*
* 1) hadoop configuration properties are applied.
* 2) confvar properties with non-null values are overlayed.
* 3) hive-site.xml properties are overlayed.
*
* warning: think twice before adding any hadoop configuration properties
* with non-null values to this list as they will override any values defined
* in the underlying hadoop configuration.
*/
public static enum confvars
// ql execution stuff
scriptwrapper    null
plan
scratchdir        system getproperty
localscratchdir        system getproperty
submitviachild    false
scripterrorlimit    100000
allowpartialconsump    false
compressresult    false
compressintermediate    false
compressintermediatecodec
compressintermediatetype
bytesperreducer     long   1000   1000   1000
maxreducers    999
preexechooks
postexechooks
onfailurehooks
clientstatspublishers
execparallel    false      parallel query launching
execparallethreadnumber    8
hivespeculativeexecreducers    true
hivecounterspullinterval    1000l
dynamicpartitioning    true
dynamicpartitioningmode
dynamicpartitionmaxparts    1000
dynamicpartitionmaxpartspernode    100
maxcreatedfiles    100000l
downloaded_resources_dir      system getproperty
defaultpartitionname
default_zookeeper_partition_name
// whether to show a link to the most failed task + debugging tips
show_job_fail_debug_info    true
job_debug_capture_stacktraces    true
job_debug_timeout    30000
tasklog_debug_timeout    20000
output_file_extension    null
// should hive determine whether to run in local mode automatically ?
localmodeauto    false
// if yes:
// run in local mode only if input bytes is less than this. 128mb by default
localmodemaxbytes    134217728l
// run in local mode only if number of tasks (for map and reduce each) is
// less than this
localmodemaxinputfiles    4
// if true, drop table/view does not fail if table/view doesn't exist and if exists is
// not specified
dropignoresnonexistent    true
// hadoop configuration properties
// properties with null values are ignored and exist only for the purpose of giving us
// a symbolic name to reference in the hive source code. properties with non-null
// values will override any values set in the underlying hadoop configuration.
hadoopbin    findhadoopbinary
hadoopfs    null
hive_fs_har_impl
hadoopmapfilename    null
hadoopmapredinputdir    null
hadoopmapredinputdirrecursive    false
hadoopjt    null
mapredmaxsplitsize    256000000l
mapredminsplitsize    1l
mapredminsplitsizepernode    1l
mapredminsplitsizeperrack    1l
// the number of reduce tasks per job. hadoop sets this value to 1 by default
// by setting this property to -1, hive will automatically determine the correct
// number of reducers.
hadoopnumreducers     1
hadoopjobname    null
hadoopspeculativeexecreducers    true
// metastore stuff. be sure to update hiveconf.metavars when you add
// something here!
metastoredirectory
metastorewarehouse
metastoreuris
// number of times to retry a connection to a thrift metastore server
metastorethriftconnectionretries    3
// number of times to retry a thrift metastore call upon failure
metastorethriftfailureretries    1
// number of seconds the client should wait between connection attempts
metastore_client_connect_retry_delay    1
// socket timeout for the client connection (in seconds)
metastore_client_socket_timeout    20
metastorepwd
// class name of jdo connection url hook
metastoreconnecturlhook
metastoremultithreaded    true
// name of the connection url in the configuration
metastoreconnecturlkey
// number of attempts to retry connecting after there is a jdo datastore err
metastoreattempts    1
// number of miliseconds to wait between attepting
metastoreinterval    1000
// whether to force reloading of the metastore configuration (including
// the connection url, before the next metastore query that accesses the
// datastore. once reloaded, this value is reset to false. used for
// testing only.
metastoreforcereloadconf    false
// number of attempts to retry connecting after there is a jdo datastore err
hmshandlerattempts    1
// number of miliseconds to wait between attepting
hmshandlerinterval    1000
// whether to force reloading of the hmshandler configuration (including
// the connection url, before the next metastore query that accesses the
// datastore. once reloaded, this value is reset to false. used for
// testing only.
hmshandlerforcereloadconf    false
metastoreserverminthreads    200
metastoreservermaxthreads    100000
metastore_tcp_keep_alive    true
// intermediate dir suffixes used for archiving. not important what they
// are, as long as collisions are avoided
metastore_int_original
metastore_int_archived
metastore_int_extracted
metastore_kerberos_keytab_file
metastore_kerberos_principal
metastore_use_thrift_sasl    false
metastore_use_thrift_framed_transport    false
metastore_cluster_delegation_token_store_cls
metastore_cluster_delegation_token_store_zk_connectstr
metastore_cluster_delegation_token_store_zk_znode
metastore_cluster_delegation_token_store_zk_acl
metastore_cache_pinobjtypes
metastore_connection_pooling_type
metastore_validate_tables    false
metastore_validate_columns    false
metastore_validate_constraints    false
metastore_store_manager_type
metastore_auto_create_schema    true
metastore_auto_start_mechanism_mode
metastore_transaction_isolation
metastore_cache_level2    false
metastore_cache_level2_type
metastore_identifier_factory
metastore_plugin_registry_bundle_check
metastore_batch_retrieve_max    300
metastore_batch_retrieve_table_partition_max
1000
metastore_pre_event_listeners
metastore_event_listeners
// should we do checks against the storage (usually hdfs) for operations like drop_partition
metastore_authorization_storage_auth_checks    false
metastore_event_clean_freq   0l
metastore_event_expiry_duration   0l
metastore_execute_set_ugi    false
metastore_partition_name_whitelist_pattern
// default parameters for creating tables
newtabledefaultpara
// parameters to copy over when creating a table with create table like.
ddl_ctl_parameters_whitelist
metastore_raw_store_impl
metastore_connection_driver
metastore_manager_factory_class
metastore_detach_all_on_commit    true
metastore_non_transactional_read    true
metastore_connection_user_name
metastore_end_function_listeners
metastore_part_inherit_tbl_props
// parameters for exporting metadata on table drop (requires the use of the)
// org.apache.hadoop.hive.ql.parse.metadataexportlistener preevent listener
metadata_export_location
move_exported_metadata_to_trash    true
// cli
cliignoreerrors    false
cliprintcurrentdb    false
cliprompt
hive_metastore_fs_handler_cls
// things we log in the jobconf
// session identifier
hivesessionid
// whether session is running in silent mode or not
hivesessionsilent    false
// query being executed (multiple per session)
hivequerystring
// id of query being executed (multiple per session)
hivequeryid
// id of the mapred plan being executed (multiple per query)
hiveplanid
// max jobname length
hivejobnamelength    50
// hive jar
hivejar
hiveauxjars
// hive added files and jars
hiveaddedfiles
hiveaddedjars
hiveaddedarchives
// for hive script operator
hives_auto_progress_timeout    0
hivetablename
hivepartitionname
hivescriptautoprogress    false
hivescriptidenvvar
hivescripttruncateenv    false
hivemapredmode
hivealias
hivemapsideaggregate    true
hivegroupbyskew    false
hivejoinemitinterval    1000
hivejoincachesize    25000
hivemapjoinbucketcachesize    100
hivemapjoinrowsize    10000
hivemapjoincacherows    25000
hivegroupbymapinterval    100000
hivemapaggrhashmemory     float  0 5
hivemapjoinfollowedbymapaggrhashmemory     float  0 3
hivemapaggrmemorythreshold     float  0 9
hivemapaggrhashminreduction     float  0 5
hivemultigroupbysinglereducer    true
hive_map_groupby_sort    false
// for hive udtf operator
hiveudtfautoprogress    false
// default file format for create table statement
// options: textfile, sequencefile
hivedefaultfileformat
hivequeryresultfileformat
hivecheckfileformat    true
//location of hive run time structured log file
hivehistoryfileloc        system getproperty
// whether to log the plan's progress every time a job's progress is checked
hive_log_incremental_plan_progress    true
// the interval between logging the plan's progress in milliseconds
hive_log_incremental_plan_progress_interval    60000l
// default serde and record reader for user scripts
hivescriptserde
hivescriptrecordreader
hivescriptrecordwriter
hivescriptescape    false
hivebinaryrecordmax    1000
// hwi
hivehwilistenhost
hivehwilistenport
hivehwiwarfile    system getenv
// mapper/reducer memory in local mode
hivehadoopmaxmem    0
//small table file size
hivesmalltablesfilesize   25000000l     25m
// random number for split sampling
hivesamplerandomnum    0
// test mode in hive mode
hivetestmode    false
hivetestmodeprefix
hivetestmodesamplefreq    32
hivetestmodenosample
hivemergemapfiles    true
hivemergemapredfiles    false
hivemergemapfilessize     long   256   1000   1000
hivemergemapfilesavgsize     long   16   1000   1000
hivemergercfileblocklevel    true
hivemergeinputformatblocklevel
hivemergecurrentjobhasdynamicpartitions
false
hiveuseexplicitrcfileheader    true
hiveskewjoin    false
hiveconvertjoin    false
hiveskewjoinkey    100000
hiveskewjoinmapjoinnummaptask    10000
hiveskewjoinmapjoinminsplit    33554432l     32m
hivemergemaponly    true
hivesendheartbeat    1000
hivelimitmaxrowsize    100000l
hivelimitoptlimitfile    10
hivelimitoptenable    false
hivelimitoptmaxfetch    50000
hivehashtablethreshold    100000
hivehashtableloadfactor     float  0 75
hivehashtablefollowbygbymaxmemoryusage     float  0 55
hivehashtablemaxmemoryusage     float  0 90
hivehashtablescale     long 100000
hivedebuglocaltask   false
hivejobprogress    false
hiveinputformat
hiveenforcebucketing    false
hiveenforcesorting    false
hivepartitioner
hiveenforcesortmergebucketmapjoin    false
hiveenforcebucketmapjoin    false
hivescriptoperatortrust    false
hiverowoffset    false
hive_combine_input_format_supports_splittable    false
// optimizer
hiveoptcp    true      column pruner
hiveoptindexfilter    false      automatically use indexes
hiveindexautoupdate    false     automatically update stale indexes
hiveoptppd    true      predicate pushdown
hiveppdrecognizetransitivity    true      predicate pushdown
hiveppdremoveduplicatefilters    true
hivemetadataonlyqueries    true
// push predicates down to storage handlers
hiveoptppd_storage    true
hiveoptgroupby    true      optimize group by
hiveoptbucketmapjoin    false      optimize bucket map join
hiveoptsortmergebucketmapjoin    false      try to use sorted merge bucket map join
hiveoptreducededuplication    true
// whether to optimize union followed by select followed by filesink
// it creates sub-directories in the final output, so should not be turned on in systems
// where mapreduce-1501 is not present
hive_optimize_union_remove    false
// whether hadoop map-reduce supports sub-directories. it was added by mapreduce-1501.
// some optimizations can only be performed if the version of hadoop being used supports
// sub-directories
hive_hadoop_supports_subdirectories    false
// optimize skewed join by changing the query plan at compile time
hive_optimize_skewjoin_compiletime    false
// indexes
hiveoptindexfilter_compact_minsize     long  5   1024   1024   1024      5g
hiveoptindexfilter_compact_maxsize     long   1      infinity
hive_index_compact_query_max_entries     long  10000000      10m
hive_index_compact_query_max_size     long  10   1024   1024   1024      10g
hive_index_compact_binary_search    true
// statistics
hivestatsautogather    true
hivestatsdbclass
other options are jdbc mysql and hbase as defined in statssetupconst java
hivestatsjdbcdriver
jdbc driver specific to the dbclass
hivestatsdbconnectionstring
automatically create database
hive_stats_default_publisher
default stats publisher if none of jdbc hbase is specified
hive_stats_default_aggregator
default stats aggregator if none of jdbc hbase is specified
hive_stats_jdbc_timeout
30      default timeout in sec for jdbc connection   sql statements
hive_stats_atomic
false      whether to update metastore stats only if all stats are available
hive_stats_retries_max
0          maximum # of retries to insert select delete the stats db
hive_stats_retries_wait
3000       # milliseconds to wait before the next retry
hive_stats_collect_rawdatasize    true
// should the raw data size be collected when analyzing tables
client_stats_counters
//subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). non-display names should be used".
hive_stats_reliable    false
// collect table access keys information for operators that can benefit from bucketing
hive_stats_collect_tablekeys    false
// standard error allowed for ndv estimates. a lower value indicates higher accuracy and a
// higher compute cost.
hive_stats_ndv_error     float 20 0
// concurrency
hive_support_concurrency    false
hive_lock_manager
hive_lock_numretries    100
hive_unlock_numretries    10
hive_lock_sleep_between_retries    60
hive_lock_mapred_only    false
hive_zookeeper_quorum
hive_zookeeper_client_port
hive_zookeeper_session_timeout    600 1000
hive_zookeeper_namespace
hive_zookeeper_clean_extra_nodes    false
// for hbase storage handler
hive_hbase_wal_enabled    true
// for har files
hivearchiveenabled    false
//enable/disable gbtoidx rewrite rule
hiveoptgbyusingindex    false
hiveouterjoinsupportsfilters    true
// 'minimal', 'more' (and 'all' later)
hivefetchtaskconversion
// serde for fetchtask
hivefetchoutputserde
// hive variables
hivevariablesubstitute    true
hivevariablesubstitutedepth    40
hiveconfvalidation    true
semantic_analyzer_hook
hive_authorization_enabled    false
hive_authorization_manager
hive_authenticator_manager
hive_metastore_authorization_manager
hive_metastore_authenticator_manager
hive_authorization_table_user_grants
hive_authorization_table_group_grants
hive_authorization_table_role_grants
hive_authorization_table_owner_grants
// print column names in output
hive_cli_print_header    false
hive_error_on_empty_partition    false
hive_index_ignore_hdfs_loc    false
hive_exim_uri_scheme_wl
// temporary variable for testing. this is added just to turn off this feature in case of a bug in
// deployment. it has not been documented in hive-default.xml intentionally, this should be removed
// once the feature is stable
hive_mapper_cannot_span_multiple_partitions    false
hive_rework_mapredwork    false
hive_concatenate_check_index     true
hive_io_exception_handlers
//prefix used to auto generated column aliases
hive_autogen_columnalias_prefix_label
hive_autogen_columnalias_prefix_includefuncname
false
// the class responsible for logging client side performance metrics
// must be a subclass of org.apache.hadoop.hive.ql.log.perflogger
hive_perf_logger
// whether to delete the scratchdir while startup
hive_start_cleanup_scratchdir    false
hive_insert_into_multilevel_dirs    false
hive_warehouse_subdir_inherit_perms    false
// whether insert into external tables is allowed
hive_insert_into_external_tables    true
// a comma separated list of hooks which implement hivedriverrunhook and will be run at the
// beginning and end of driver.run, these will be run in the order specified
hive_driver_run_hooks
hive_ddl_output_format    null
hive_entity_separator
// if this is set all move tasks at the end of a multi-insert query will only begin once all
// outputs are ready
hive_multi_insert_move_tasks_share_dependencies
false
/* the following section contains all configurations used for list bucketing feature.*/
// enable list bucketing ddl. default value is false so that we disable it by default.
// this will be removed once the rest of the dml changes are committed.
hive_internal_ddl_list_bucketing_enable    false
// default list bucketing directory name.
hive_list_bucketing_default_dir_name
// enable list bucketing optimizer. default value is false so that we disable it by default.
// this will be removed once the rest of the dml changes are committed.
hiveoptlistbucketing    false
// allow tcp keep alive socket option for for hiveserver or a maximum timeout for the socket.
server_read_socket_timeout    10
server_tcp_keep_alive    true
public final string varname
public final string defaultval
public final int defaultintval
public final long defaultlongval
public final float defaultfloatval
public final class<?> valclass
public final boolean defaultboolval
private final vartype type
confvars string varname  string defaultval
this varname   varname
this valclass   string class
this defaultval   defaultval
this defaultintval    1
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   false
this type   vartype string
confvars string varname  int defaultintval
this varname   varname
this valclass   integer class
this defaultval   integer tostring defaultintval
this defaultintval   defaultintval
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   false
this type   vartype int
confvars string varname  long defaultlongval
this varname   varname
this valclass   long class
this defaultval   long tostring defaultlongval
this defaultintval    1
this defaultlongval   defaultlongval
this defaultfloatval    1
this defaultboolval   false
this type   vartype long
confvars string varname  float defaultfloatval
this varname   varname
this valclass   float class
this defaultval   float tostring defaultfloatval
this defaultintval    1
this defaultlongval    1
this defaultfloatval   defaultfloatval
this defaultboolval   false
this type   vartype float
confvars string varname  boolean defaultboolval
this varname   varname
this valclass   boolean class
this defaultval   boolean tostring defaultboolval
this defaultintval    1
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   defaultboolval
this type   vartype boolean
public boolean istype string value
return type istype value
public string typestring
return type typestring
@override
public string tostring
return varname
private static string findhadoopbinary
string val   system getenv
// in hadoop 1.x and hadoop 2.x hadoop_home is gone and replaced with hadoop_prefix
if  val    null
val   system getenv
// and if all else fails we can at least try /usr/bin/hadoop
val    val    null ? file separator       val
file separator       file separator
// launch hadoop command file on windows.
return val    shell windows ?
enum vartype
string   @override
void checktype string value  throws exception
int   @override
void checktype string value  throws exception   integer valueof value
long   @override
void checktype string value  throws exception   long valueof value
float   @override
void checktype string value  throws exception   float valueof value
boolean   @override
void checktype string value  throws exception   boolean valueof value
boolean istype string value
try   checktype value     catch  exception e    return false
return true
string typestring     return name   touppercase
abstract void checktype string value  throws exception
/**
* writes the default confvars out to a temporary file and returns
* a url pointing to the temporary file.
* we need this in order to initialize the confvar properties
* in the underling configuration object using the addresource(url)
* method.
*
* using configuration.addresource(inputstream) would be a preferable
* approach, but it turns out that method is broken since configuration
* tries to read the entire contents of the same inputstream repeatedly.
*/
private static synchronized url getconfvarurl
if  confvarurl    null
try
configuration conf   new configuration
file confvarfile   file createtempfile
confvarfile deleteonexit
applydefaultnonnullconfvars conf
fileoutputstream fout   new fileoutputstream confvarfile
conf writexml fout
fout close
confvarurl   confvarfile touri   tourl
catch  exception e
// we're pretty screwed if we can't load the default conf vars
throw new runtimeexception    e
return confvarurl
public static int getintvar configuration conf  confvars var
assert  var valclass    integer class
return conf getint var varname  var defaultintval
public static void setintvar configuration conf  confvars var  int val
assert  var valclass    integer class
conf setint var varname  val
public int getintvar confvars var
return getintvar this  var
public void setintvar confvars var  int val
setintvar this  var  val
public static long getlongvar configuration conf  confvars var
assert  var valclass    long class
return conf getlong var varname  var defaultlongval
public static long getlongvar configuration conf  confvars var  long defaultval
return conf getlong var varname  defaultval
public static void setlongvar configuration conf  confvars var  long val
assert  var valclass    long class
conf setlong var varname  val
public long getlongvar confvars var
return getlongvar this  var
public void setlongvar confvars var  long val
setlongvar this  var  val
public static float getfloatvar configuration conf  confvars var
assert  var valclass    float class
return conf getfloat var varname  var defaultfloatval
public static float getfloatvar configuration conf  confvars var  float defaultval
return conf getfloat var varname  defaultval
public static void setfloatvar configuration conf  confvars var  float val
assert  var valclass    float class
shimloader gethadoopshims   setfloatconf conf  var varname  val
public float getfloatvar confvars var
return getfloatvar this  var
public void setfloatvar confvars var  float val
setfloatvar this  var  val
public static boolean getboolvar configuration conf  confvars var
assert  var valclass    boolean class
return conf getboolean var varname  var defaultboolval
public static boolean getboolvar configuration conf  confvars var  boolean defaultval
return conf getboolean var varname  defaultval
public static void setboolvar configuration conf  confvars var  boolean val
assert  var valclass    boolean class
conf setboolean var varname  val
public boolean getboolvar confvars var
return getboolvar this  var
public void setboolvar confvars var  boolean val
setboolvar this  var  val
public static string getvar configuration conf  confvars var
assert  var valclass    string class
return conf get var varname  var defaultval
public static string getvar configuration conf  confvars var  string defaultval
return conf get var varname  defaultval
public static void setvar configuration conf  confvars var  string val
assert  var valclass    string class
conf set var varname  val
public static confvars getconfvars string name
return vars get name
public string getvar confvars var
return getvar this  var
public void setvar confvars var  string val
setvar this  var  val
public void logvars printstream ps
for  confvars one   confvars values
ps println one varname         get one varname     null  ? get one varname
public hiveconf
super
initialize this getclass
public hiveconf class<?> cls
super
initialize cls
public hiveconf configuration other  class<?> cls
super other
initialize cls
/**
* copy constructor
*/
public hiveconf hiveconf other
super other
hivejar   other hivejar
auxjars   other auxjars
origprop    properties other origprop clone
public properties getallproperties
return getproperties this
private static properties getproperties configuration conf
iterator<map entry<string  string>> iter   conf iterator
properties p   new properties
while  iter hasnext
map entry<string  string> e   iter next
p setproperty e getkey    e getvalue
return p
private void initialize class<?> cls
hivejar    new jobconf cls   getjar
// preserve the original configuration
origprop   getallproperties
// overlay the confvars. note that this ignores confvars with null values
addresource getconfvarurl
// overlay hive-site.xml if it exists
if  hivesiteurl    null
addresource hivesiteurl
// overlay the values of any system properties whose names appear in the list of confvars
applysystemproperties
if this get    null     null
l4j warn
// if the running class was loaded directly (through eclipse) rather than through a
// jar then this would be needed
if  hivejar    null
hivejar   this get confvars hivejar varname
if  auxjars    null
auxjars   this get confvars hiveauxjars varname
/**
* apply system properties to this object if the property name is defined in confvars
* and the value is non-null and not an empty string.
*/
private void applysystemproperties
map<string  string> systemproperties   getconfsystemproperties
for  entry<string  string> systemproperty   systemproperties entryset
this set systemproperty getkey    systemproperty getvalue
/**
* this method returns a mapping from config variable name to its value for all config variables
* which have been set using system properties
*/
public static map<string  string> getconfsystemproperties
map<string  string> systemproperties   new hashmap<string  string>
for  confvars onevar   confvars values
if  system getproperty onevar varname     null
if  system getproperty onevar varname  length   > 0
systemproperties put onevar varname  system getproperty onevar varname
return systemproperties
/**
* overlays confvar properties with non-null values
*/
private static void applydefaultnonnullconfvars configuration conf
for  confvars var   confvars values
if  var defaultval    null
// don't override confvars with null values
continue
if  conf get var varname     null
l4j debug     var varname       conf get var varname
var defaultval
conf set var varname  var defaultval
public properties getchangedproperties
properties ret   new properties
properties newprop   getallproperties
for  object one   newprop keyset
string oneprop    string  one
string oldvalue   origprop getproperty oneprop
if   stringutils equals oldvalue  newprop getproperty oneprop
ret setproperty oneprop  newprop getproperty oneprop
return  ret
public string gethivesitepath
return hivesiteurl getpath
public string getjar
return hivejar
/**
* @return the auxjars
*/
public string getauxjars
return auxjars
/**
* @param auxjars the auxjars to set
*/
public void setauxjars string auxjars
this auxjars   auxjars
setvar this  confvars hiveauxjars  auxjars
/**
* @return the user name set in hadoop.job.ugi param or the current user from system
* @throws ioexception
*/
public string getuser   throws ioexception
try
usergroupinformation ugi   shimloader gethadoopshims
getugiforconf this
return ugi getusername
catch  loginexception le
throw new ioexception le
public static string getcolumninternalname int pos
return     pos
public static int getpositionfrominternalname string internalname
pattern internalpattern   pattern compile
matcher m   internalpattern matcher internalname
if   m matches
return  1
else
return integer parseint m group 1