/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql optimizer physical
import java io serializable
import java util arraylist
import java util collection
import java util hashset
import java util linkedhashmap
import java util list
import java util map
import java util stack
import java util iterator
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs path
import org apache hadoop hive ql exec groupbyoperator
import org apache hadoop hive ql exec operator
import org apache hadoop hive ql exec tablescanoperator
import org apache hadoop hive ql exec task
import org apache hadoop hive ql lib defaultgraphwalker
import org apache hadoop hive ql lib defaultruledispatcher
import org apache hadoop hive ql lib dispatcher
import org apache hadoop hive ql lib graphwalker
import org apache hadoop hive ql lib node
import org apache hadoop hive ql lib nodeprocessor
import org apache hadoop hive ql lib nodeprocessorctx
import org apache hadoop hive ql lib preorderwalker
import org apache hadoop hive ql lib rule
import org apache hadoop hive ql lib ruleregexp
import org apache hadoop hive ql parse parsecontext
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql io onenullrowinputformat
import org apache hadoop hive serde2 nullstructserde
/**
*
* metadataonlyoptimizer determines to which tablescanoperators "metadata only"
* optimization can be applied. such operator must use only partition columns
* (it is easy to check, because we are after column pruning and all places
* where the data from the operator is used must go through groupbyoperator
* distinct or distinct-like aggregations. aggregation is distinct-like if
* adding distinct wouldn't change the result, for example min, max.
*
* we cannot apply the optimization without group by, because the results depend
* on the numbers of rows in partitions, for example count(hr) will count all
* rows in matching partitions.
*
*/
public class metadataonlyoptimizer implements physicalplanresolver
private static final log log   logfactory getlog metadataonlyoptimizer class getname
static private class walkerctx implements nodeprocessorctx
/* operators for which there is chance the optimization can be applied */
private final hashset<tablescanoperator> possible   new hashset<tablescanoperator>
/* operators for which the optimization will be successful */
private final hashset<tablescanoperator> success   new hashset<tablescanoperator>
/**
* sets operator as one for which there is a chance to apply optimization
*
* @param op
*          the operator
*/
public void setmaybemetadataonly tablescanoperator op
possible add op
/** convert all possible operators to success */
public void convertmetadataonly
success addall possible
possible clear
/**
* convert all possible operators to banned
*/
public void convertnotmetadataonly
possible clear
success clear
/**
* returns hashset of collected operators for which the optimization may be
* applicable.
*/
public hashset<tablescanoperator> getmaybemetadataonlytablescans
return possible
/**
* returns hashset of collected operators for which the optimization is
* applicable.
*/
public hashset<tablescanoperator> getmetadataonlytablescans
return success
static private class tablescanprocessor implements nodeprocessor
public tablescanprocessor
@override
public object process node nd  stack<node> stack  nodeprocessorctx procctx
object    nodeoutputs  throws semanticexception
tablescanoperator node    tablescanoperator  nd
walkerctx walkerctx    walkerctx  procctx
if    node getneededcolumnids      null      node getneededcolumnids   size      0
node getconf      null
node getconf   getvirtualcols      null
node getconf   getvirtualcols   isempty
walkerctx setmaybemetadataonly node
return nd
static private class filesinkprocessor implements nodeprocessor
@override
public object process node nd  stack<node> stack  nodeprocessorctx procctx
object    nodeoutputs  throws semanticexception
walkerctx walkerctx    walkerctx  procctx
// there can be atmost one element eligible to be converted to
// metadata only
if   walkerctx getmaybemetadataonlytablescans   isempty
walkerctx getmaybemetadataonlytablescans   size   > 1
return nd
for  node op   stack
if  op instanceof groupbyoperator
groupbyoperator gby    groupbyoperator  op
if   gby getconf   isdistinctlike
// groupby not distinct like, disabling
walkerctx convertnotmetadataonly
return nd
walkerctx convertmetadataonly
return nd
@override
public physicalcontext resolve physicalcontext pctx  throws semanticexception
dispatcher disp   new metadataonlytaskdispatcher pctx
graphwalker ogw   new defaultgraphwalker disp
arraylist<node> topnodes   new arraylist<node>
topnodes addall pctx roottasks
ogw startwalking topnodes  null
return pctx
/**
* iterate over all tasks one-to-one and convert them to metadata only
*/
class metadataonlytaskdispatcher implements dispatcher
private physicalcontext physicalcontext
public metadataonlytaskdispatcher physicalcontext context
super
physicalcontext   context
private string getaliasfortablescanoperator mapredwork work
tablescanoperator tso
for  map entry<string  operator<? extends serializable>> entry   work getaliastowork   entryset
if  entry getvalue      tso
return entry getkey
return null
private partitiondesc changepartitiontometadataonly partitiondesc desc
if  desc    null
desc setinputfileformatclass onenullrowinputformat class
desc setdeserializerclass nullstructserde class
desc setserdeclassname nullstructserde class getname
return desc
private list<string> getpathsforalias mapredwork work  string alias
list<string> paths   new arraylist<string>
for  map entry<string  arraylist<string>> entry   work getpathtoaliases   entryset
if  entry getvalue   contains alias
paths add entry getkey
return paths
private void processalias mapredwork work  string alias
// change the alias partition desc
partitiondesc aliaspartn   work getaliastopartninfo   get alias
changepartitiontometadataonly aliaspartn
list<string> paths   getpathsforalias work  alias
for  string path   paths
partitiondesc newpartition   changepartitiontometadataonly work getpathtopartitioninfo   get
path
path fakepath   new path    null
newpartition gettablename
newpartition getpartspec   tostring
work getpathtopartitioninfo   remove path
work getpathtopartitioninfo   put fakepath getname    newpartition
arraylist<string> aliases   work getpathtoaliases   remove path
work getpathtoaliases   put fakepath getname    aliases
private void converttometadataonlyquery mapredwork work
tablescanoperator tso
string alias   getaliasfortablescanoperator work  tso
processalias work  alias
@override
public object dispatch node nd  stack<node> stack  object    nodeoutputs
throws semanticexception
task<? extends serializable> task    task<? extends serializable>  nd
collection<operator<? extends serializable>> topoperators
task gettopoperators
if  topoperators size      0
return null
log info
// create a the context for walking operators
parsecontext parsecontext   physicalcontext getparsecontext
walkerctx walkerctx   new walkerctx
map<rule  nodeprocessor> oprules   new linkedhashmap<rule  nodeprocessor>
oprules put new ruleregexp        new tablescanprocessor
oprules put new ruleregexp        new filesinkprocessor
// the dispatcher fires the processor corresponding to the closest
// matching rule and passes the context along
dispatcher disp   new defaultruledispatcher null  oprules  walkerctx
graphwalker ogw   new preorderwalker disp
// create a list of topop nodes
arraylist<node> topnodes   new arraylist<node>
// get the top nodes for this map-reduce task
for  operator<? extends serializable>
workoperator   topoperators
if  parsecontext gettopops   values   contains workoperator
topnodes add workoperator
if  task getreducer      null
topnodes add task getreducer
ogw startwalking topnodes  null
log info string format
walkerctx getmetadataonlytablescans   size
iterator<tablescanoperator> iterator
walkerctx getmetadataonlytablescans   iterator
while  iterator hasnext
tablescanoperator tso   iterator next
log info     tso getconf   getalias
converttometadataonlyquery  mapredwork  task getwork    tso
return null