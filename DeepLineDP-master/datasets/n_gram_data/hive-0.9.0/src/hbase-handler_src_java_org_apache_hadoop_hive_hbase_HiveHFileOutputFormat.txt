/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive hbase
import java io ioexception
import java util map
import java util properties
import java util sortedmap
import java util treemap
import org apache commons lang notimplementedexception
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hbase keyvalue
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase mapreduce hfileoutputformat
import org apache hadoop hbase util bytes
import org apache hadoop hive ql exec filesinkoperator recordwriter
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive shims shimloader
import org apache hadoop io text
import org apache hadoop io writable
import org apache hadoop mapred jobconf
import org apache hadoop mapreduce job
import org apache hadoop mapreduce jobcontext
import org apache hadoop mapreduce lib output fileoutputformat
import org apache hadoop util progressable
/**
* hivehfileoutputformat implements hiveoutputformat for hfile bulk
* loading.  until hbase-1861 is implemented, it can only be used
* for loading a table with a single column family.
*/
public class hivehfileoutputformat extends
hfileoutputformat implements
hiveoutputformat<immutablebyteswritable  keyvalue>
private static final string hfile_family_path
static final log log   logfactory getlog
hivehfileoutputformat class getname
private
org apache hadoop mapreduce recordwriter<immutablebyteswritable  keyvalue>
getfilewriter org apache hadoop mapreduce taskattemptcontext tac
throws ioexception
try
return super getrecordwriter tac
catch  interruptedexception ex
throw new ioexception ex
@override
public recordwriter gethiverecordwriter
final jobconf jc
final path finaloutpath
class<? extends writable> valueclass
boolean iscompressed
properties tableproperties
final progressable progressable  throws ioexception
// read configuration for the target path
string hfilepath   tableproperties getproperty hfile_family_path
if  hfilepath    null
throw new runtimeexception
hfile_family_path
// target path's last component is also the column family name.
final path columnfamilypath   new path hfilepath
final string columnfamilyname   columnfamilypath getname
final byte  columnfamilynamebytes   bytes tobytes columnfamilyname
final job job   new job jc
setcompressoutput job  iscompressed
setoutputpath job  finaloutpath
// create the hfile writer
final org apache hadoop mapreduce taskattemptcontext tac
shimloader gethadoopshims   newtaskattemptcontext
job getconfiguration    progressable
final path outputdir   fileoutputformat getoutputpath tac
final org apache hadoop mapreduce recordwriter<
immutablebyteswritable  keyvalue> filewriter   getfilewriter tac
// individual columns are going to be pivoted to hbase cells,
// and for each row, they need to be written out in order
// of column name, so sort the column names now, creating a
// mapping to their column position.  however, the first
// column is interpreted as the row key.
string columnlist   tableproperties getproperty
string  columnarray   columnlist split
final sortedmap<byte   integer> columnmap
new treemap<byte   integer> bytes bytes_comparator
int i   0
for  string columnname   columnarray
if  i    0
columnmap put bytes tobytes columnname   i
i
return new recordwriter
@override
public void close boolean abort  throws ioexception
try
filewriter close null
if  abort
return
// move the region file(s) from the task output directory
// to the location specified by the user.  there should
// actually only be one (each reducer produces one hfile),
// but we don't know what its name is.
filesystem fs   outputdir getfilesystem jc
fs mkdirs columnfamilypath
path srcdir   outputdir
for
filestatus  files   fs liststatus srcdir
if   files    null      files length    0
throw new ioexception     srcdir
if  files length    1
throw new ioexception     srcdir
srcdir   files getpath
if  srcdir getname   equals columnfamilyname
break
for  filestatus regionfile   fs liststatus srcdir
fs rename
regionfile getpath
new path
columnfamilypath
regionfile getpath   getname
// hive actually wants a file as task output (not a directory), so
// replace the empty directory with an empty file to keep it happy.
fs delete outputdir  true
fs createnewfile outputdir
catch  interruptedexception ex
throw new ioexception ex
@override
public void write writable w  throws ioexception
// decompose the incoming text row into fields.
string s     text  w  tostring
string  fields   s split
assert fields length <   columnmap size     1
// first field is the row key.
byte  rowkeybytes   bytes tobytes fields
// remaining fields are cells addressed by column name within row.
for  map entry<byte   integer> entry   columnmap entryset
byte  columnnamebytes   entry getkey
int icolumn   entry getvalue
string val
if  icolumn >  fields length
// trailing blank field
val
else
val   fields
if    equals val
// omit nulls
continue
byte  valbytes   bytes tobytes val
keyvalue kv   new keyvalue
rowkeybytes
columnfamilynamebytes
columnnamebytes
valbytes
try
filewriter write null  kv
catch  interruptedexception ex
throw new ioexception ex
@override
public void checkoutputspecs filesystem ignored  jobconf jc  throws ioexception
//delegate to the new api
job job   new job jc
jobcontext jobcontext   shimloader gethadoopshims   newjobcontext job
checkoutputspecs jobcontext
@override
public org apache hadoop mapred recordwriter<immutablebyteswritable  keyvalue> getrecordwriter
filesystem ignored  jobconf job  string name  progressable progress  throws ioexception
throw new notimplementedexception