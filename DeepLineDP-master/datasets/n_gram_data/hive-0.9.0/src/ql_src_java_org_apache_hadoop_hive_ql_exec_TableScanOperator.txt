/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java io serializable
import java util arraylist
import java util hashmap
import java util list
import java util map
import org apache hadoop conf configuration
import org apache hadoop fs path
import org apache hadoop hive common fileutils
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata virtualcolumn
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive ql plan api operatortype
import org apache hadoop hive ql stats statspublisher
import org apache hadoop hive ql stats statssetupconst
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspectorutils
import org apache hadoop hive serde2 objectinspector objectinspectorutils objectinspectorcopyoption
import org apache hadoop hive serde2 objectinspector structfield
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop io longwritable
import org apache hadoop mapred jobconf
/**
* table scan operator if the data is coming from the map-reduce framework, just
* forward it. this will be needed as part of local work when data is not being
* read as part of map-reduce framework
**/
public class tablescanoperator extends operator<tablescandesc> implements
serializable
private static final long serialversionuid   1l
protected transient jobconf jc
private transient configuration hconf
private transient string partitionspecs
private transient boolean inputfilechanged   false
private tabledesc tabledesc
private transient stat currentstat
private transient map<string  stat> stats
public tabledesc gettabledesc
return tabledesc
public void settabledesc tabledesc tabledesc
this tabledesc   tabledesc
/**
* other than gathering statistics for the analyze command, the table scan operator
* does not do anything special other than just forwarding the row. since the table
* data is always read as part of the map-reduce framework by the mapper. but, when this
* assumption stops to be true, i.e table data won't be only read by the mapper, this
* operator will be enhanced to read the table.
**/
@override
public void processop object row  int tag  throws hiveexception
if  conf    null    conf isgatherstats
gatherstats row
forward row  inputobjinspectors
// change the table partition for collecting stats
@override
public void cleanupinputfilechangedop   throws hiveexception
inputfilechanged   true
private void gatherstats object row
// first row/call or a new partition
if   currentstat    null     inputfilechanged
inputfilechanged   false
if  conf getpartcolumns      null    conf getpartcolumns   size      0
partitionspecs         non partitioned
else
// figure out the partition spec from the input.
// this is only done once for the first row (when stat == null)
// since all rows in the same mapper should be from the same partition.
list<object> writable
list<string> values
int dpstartcol     the first position of partition column
assert inputobjinspectors getcategory      objectinspector category struct
writable   new arraylist<object> conf getpartcolumns   size
values   new arraylist<string> conf getpartcolumns   size
dpstartcol   0
structobjectinspector soi    structobjectinspector  inputobjinspectors
for  structfield sf   soi getallstructfieldrefs
string fn   sf getfieldname
if   conf getpartcolumns   contains fn
dpstartcol
else
break
objectinspectorutils partialcopytostandardobject writable  row  dpstartcol  conf
getpartcolumns   size
structobjectinspector  inputobjinspectors  objectinspectorcopyoption writable
for  object o   writable
assert  o    null    o tostring   length   > 0
values add o tostring
partitionspecs   fileutils makepartname conf getpartcolumns    values
log info     partitionspecs
// find which column contains the raw data size (both partitioned and non partitioned
int usizecolumn    1
structobjectinspector soi    structobjectinspector  inputobjinspectors
for  int i   0  i < soi getallstructfieldrefs   size    i
if  soi getallstructfieldrefs   get i  getfieldname
equals virtualcolumn rawdatasize getname   tolowercase
usizecolumn   i
break
currentstat   stats get partitionspecs
if  currentstat    null
currentstat   new stat
currentstat setbookkeepinginfo statssetupconst raw_data_size  usizecolumn
stats put partitionspecs  currentstat
// increase the row count
currentstat addtostat statssetupconst row_count  1
// extract the raw data size, and update the stats for the current partition
int rdsizecolumn   currentstat getbookkeepinginfo statssetupconst raw_data_size
if rdsizecolumn     1
list<object> rdsize   new arraylist<object> 1
objectinspectorutils partialcopytostandardobject rdsize  row
rdsizecolumn  1   structobjectinspector  inputobjinspectors
objectinspectorcopyoption writable
currentstat addtostat statssetupconst raw_data_size     longwritable rdsize get 0   get
@override
protected void initializeop configuration hconf  throws hiveexception
initializechildren hconf
inputfilechanged   false
if  conf    null
return
if   conf isgatherstats
return
this hconf   hconf
if  hconf instanceof jobconf
jc    jobconf  hconf
else
// test code path
jc   new jobconf hconf  execdriver class
currentstat   null
stats   new hashmap<string  stat>
partitionspecs   null
if  conf getpartcolumns      null    conf getpartcolumns   size      0
// non partitioned table
return
@override
public void closeop boolean abort  throws hiveexception
if  conf    null
if  conf isgatherstats      stats size      0
publishstats
/**
* the operator name for this operator type. this is used to construct the
* rule for an operator
*
* @return the operator name
**/
@override
public string getname
return
// this 'neededcolumnids' field is included in this operator class instead of
// its desc class.the reason is that 1)tablescandesc can not be instantiated,
// and 2) it will fail some join and union queries if this is added forcibly
// into tablescandesc
java util arraylist<integer> neededcolumnids
public void setneededcolumnids java util arraylist<integer> orign_columns
neededcolumnids   orign_columns
public java util arraylist<integer> getneededcolumnids
return neededcolumnids
@override
public operatortype gettype
return operatortype tablescan
private void publishstats
// initializing a stats publisher
statspublisher statspublisher   utilities getstatspublisher jc
if   statspublisher connect jc
// just return, stats gathering should not block the main query.
log info
return
string key
string taskid   utilities gettaskidfromfilename utilities gettaskid hconf
map<string  string> statstopublish   new hashmap<string  string>
for  string pspecs   stats keyset
statstopublish clear
if  pspecs isempty
// in case of a non-partitioned table, the key for temp storage is just
// "tablename + taskid"
key   conf getstatsaggprefix     taskid
else
// in case of a partition, the key for temp storage is
// "tablename + partitionspecs + taskid"
key   conf getstatsaggprefix     pspecs   path separator   taskid
for string stattype   stats get pspecs  getstoredstats
statstopublish put stattype  long tostring stats get pspecs  getstat stattype
statspublisher publishstat key  statstopublish
log info     key       statstopublish tostring
statspublisher closeconnection