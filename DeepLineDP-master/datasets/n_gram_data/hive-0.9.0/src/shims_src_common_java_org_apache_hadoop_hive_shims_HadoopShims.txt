/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive shims
import java io datainput
import java io dataoutput
import java io ioexception
import java security privilegedexceptionaction
import java util list
import javax security auth login loginexception
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop io text
import org apache hadoop mapred clusterstatus
import org apache hadoop mapred inputformat
import org apache hadoop mapred inputsplit
import org apache hadoop mapred jobconf
import org apache hadoop mapred recordreader
import org apache hadoop mapred reporter
import org apache hadoop mapred runningjob
import org apache hadoop mapred taskcompletionevent
import org apache hadoop mapreduce job
import org apache hadoop mapreduce jobcontext
import org apache hadoop mapreduce taskattemptcontext
import org apache hadoop security usergroupinformation
import org apache hadoop util progressable
/**
* in order to be compatible with multiple versions of hadoop, all parts
* of the hadoop interface that are not cross-version compatible are
* encapsulated in an implementation of this class. users should use
* the shimloader class as a factory to obtain an implementation of
* hadoopshims corresponding to the version of hadoop currently on the
* classpath.
*/
public interface hadoopshims
static final log log   logfactory getlog hadoopshims class
/**
* return true if the current version of hadoop uses the jobshell for
* command line interpretation.
*/
boolean usesjobshell
/**
* return true if the job has not switched to running state yet
* and is still in prep state
*/
boolean isjobpreparing runningjob job  throws ioexception
/**
* calls fs.deleteonexit(path) if such a function exists.
*
* @return true if the call was successful
*/
boolean filesystemdeleteonexit filesystem fs  path path  throws ioexception
/**
* calls fmt.validateinput(conf) if such a function exists.
*/
void inputformatvalidateinput inputformat fmt  jobconf conf  throws ioexception
/**
* if jobclient.getcommandlineconfig exists, sets the given
* property/value pair in that configuration object.
*
* this applies for hadoop 0.17 through 0.19
*/
void settmpfiles string prop  string files
/**
* return the last access time of the given file.
* @param file
* @return last access time. -1 if not supported.
*/
long getaccesstime filestatus file
/**
* returns a shim to wrap minidfscluster. this is necessary since this class
* was moved from org.apache.hadoop.dfs to org.apache.hadoop.hdfs
*/
minidfsshim getminidfs configuration conf
int numdatanodes
boolean format
string racks  throws ioexception
/**
* shim around the functions in minidfscluster that hive uses.
*/
public interface minidfsshim
filesystem getfilesystem   throws ioexception
void shutdown   throws ioexception
/**
* we define this function here to make the code compatible between
* hadoop 0.17 and hadoop 0.20.
*
* hive binary that compiled text.compareto(text) with hadoop 0.20 won't
* work with hadoop 0.17 because in hadoop 0.20, text.compareto(text) is
* implemented in org.apache.hadoop.io.binarycomparable, and java compiler
* references that class, which is not available in hadoop 0.17.
*/
int comparetext text a  text b
combinefileinputformatshim getcombinefileinputformat
string getinputformatclassname
/**
* wrapper for configuration.setfloat, which was not introduced
* until 0.20.
*/
void setfloatconf configuration conf  string varname  float val
/**
* gettaskjobids returns an array of string with two elements. the first
* element is a string representing the task id and the second is a string
* representing the job id. this is necessary as taskid and taskattemptid
* are not supported in haddop 0.17
*/
string gettaskjobids taskcompletionevent t
int createhadooparchive configuration conf  path parentdir  path destdir
string archivename  throws exception
/**
* hive uses side effect files exclusively for it's output. it also manages
* the setup/cleanup/commit of output from the hive client. as a result it does
* not need support for the same inside the mr framework
*
* this routine sets the appropriate options related to bypass setup/cleanup/commit
* support in the mr framework, but does not set the outputformat class.
*/
void preparejoboutput jobconf conf
/**
* get the ugi that the given job configuration will run as.
*
* in secure versions of hadoop, this simply returns the current
* access control context's user, ignoring the configuration.
*/
public usergroupinformation getugiforconf configuration conf  throws loginexception  ioexception
/**
* used by metastore server to perform requested rpc in client context.
* @param ugi
* @param pvea
* @throws ioexception
* @throws interruptedexception
*/
public void doas usergroupinformation ugi  privilegedexceptionaction<void> pvea  throws
ioexception  interruptedexception
/**
* used by metastore server to creates ugi object for a remote user.
* @param username remote user name
* @param groupnames group names associated with remote user name
* @return ugi created for the remote user.
*/
public usergroupinformation createremoteuser string username  list<string> groupnames
/**
* get the short name corresponding to the subject in the passed ugi
*
* in secure versions of hadoop, this returns the short name (after
* undergoing the translation in the kerberos name rule mapping).
* in unsecure versions of hadoop, this returns the name of the subject
*/
public string getshortusername usergroupinformation ugi
/**
* return true if the shim is based on hadoop security apis.
*/
public boolean issecureshimimpl
/**
* get the string form of the token given a token signature.
* the signature is used as the value of the "service" field in the token for lookup.
* ref: abstractdelegationtokenselector in hadoop. if there exists such a token
* in the token cache (credential store) of the job, the lookup returns that.
* this is relevant only when running against a "secure" hadoop release
* the method gets hold of the tokens if they are set up by hadoop - this should
* happen on the map/reduce tasks if the client added the tokens into hadoop's
* credential store in the front end during job submission. the method will
* select the hive delegation token among the set of tokens and return the string
* form of it
* @param tokensignature
* @return the string form of the token found
* @throws ioexception
*/
string gettokenstrform string tokensignature  throws ioexception
enum jobtrackerstate   initializing  running
/**
* convert the clusterstatus to its thrift equivalent: jobtrackerstate.
* see mapreduce-2455 for why this is a part of the shim.
* @param clusterstatus
* @return the matching jobtrackerstate
* @throws exception if no equivalent jobtrackerstate exists
*/
public jobtrackerstate getjobtrackerstate clusterstatus clusterstatus  throws exception
public taskattemptcontext newtaskattemptcontext configuration conf  final progressable progressable
public jobcontext newjobcontext job job
/**
* inputsplitshim.
*
*/
public interface inputsplitshim extends inputsplit
jobconf getjob
long getlength
/** returns an array containing the startoffsets of the files in the split. */
long getstartoffsets
/** returns an array containing the lengths of the files in the split. */
long getlengths
/** returns the start offset of the i<sup>th</sup> path. */
long getoffset int i
/** returns the length of the i<sup>th</sup> path. */
long getlength int i
/** returns the number of paths in the split. */
int getnumpaths
/** returns the i<sup>th</sup> path. */
path getpath int i
/** returns all the paths in the split. */
path getpaths
/** returns all the paths where this input-split resides. */
string getlocations   throws ioexception
void shrinksplit long length
string tostring
void readfields datainput in  throws ioexception
void write dataoutput out  throws ioexception
/**
* combinefileinputformatshim.
*
* @param <k>
* @param <v>
*/
interface combinefileinputformatshim<k  v>
path getinputpathsshim jobconf conf
void createpool jobconf conf  pathfilter    filters
inputsplitshim getsplits jobconf job  int numsplits  throws ioexception
inputsplitshim getinputsplitshim   throws ioexception
recordreader getrecordreader jobconf job  inputsplitshim split  reporter reporter
class<recordreader<k  v>> rrclass  throws ioexception