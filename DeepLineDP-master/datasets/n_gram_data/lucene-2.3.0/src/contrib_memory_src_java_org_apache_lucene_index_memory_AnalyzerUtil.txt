package org apache lucene index memory
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import java io ioexception
import java io printstream
import java io reader
import java io stringreader
import java util arraylist
import java util arrays
import java util comparator
import java util hashmap
import java util iterator
import java util map
import java util regex pattern
import org apache lucene analysis analyzer
import org apache lucene analysis porterstemfilter
import org apache lucene analysis token
import org apache lucene analysis tokenfilter
import org apache lucene analysis tokenstream
/**
* various fulltext analysis utilities avoiding redundant code in several
* classes.
*
* @author whoschek.at.lbl.dot.gov
*/
public class analyzerutil
private analyzerutil
/**
* returns a simple analyzer wrapper that logs all tokens produced by the
* underlying child analyzer to the given log stream (typically system.err);
* otherwise behaves exactly like the child analyzer, delivering the very
* same tokens; useful for debugging purposes on custom indexing and/or
* querying.
*
* @param child
*            the underlying child analyzer
* @param log
*            the print stream to log to (typically system.err)
* @param logname
*            a name for this logger (typically "log" or similar)
* @return a logging analyzer
*/
public static analyzer getlogginganalyzer final analyzer child
final printstream log  final string logname
if  child    null
throw new illegalargumentexception
if  log    null
throw new illegalargumentexception
return new analyzer
public tokenstream tokenstream final string fieldname  reader reader
return new tokenfilter child tokenstream fieldname  reader
private int position    1
public token next   throws ioexception
token token   input next       from filter super class
log println tostring token
return token
private string tostring token token
if  token    null  return     logname       fieldname
position    token getpositionincrement
return     logname       position       fieldname
token termtext         token startoffset
token endoffset         token type
/**
* returns an analyzer wrapper that returns at most the first
* <code>maxtokens</code> tokens from the underlying child analyzer,
* ignoring all remaining tokens.
*
* @param child
*            the underlying child analyzer
* @param maxtokens
*            the maximum number of tokens to return from the underlying
*            analyzer (a value of integer.max_value indicates unlimited)
* @return an analyzer wrapper
*/
public static analyzer getmaxtokenanalyzer
final analyzer child  final int maxtokens
if  child    null
throw new illegalargumentexception
if  maxtokens < 0
throw new illegalargumentexception
if  maxtokens    integer max_value
return child     no need to wrap
return new analyzer
public tokenstream tokenstream string fieldname  reader reader
return new tokenfilter child tokenstream fieldname  reader
private int todo   maxtokens
public token next   throws ioexception
return   todo >  0 ? input next     null
/**
* returns an english stemming analyzer that stems tokens from the
* underlying child analyzer according to the porter stemming algorithm. the
* child analyzer must deliver tokens in lower case for the stemmer to work
* properly.
* <p>
* background: stemming reduces token terms to their linguistic root form
* e.g. reduces "fishing" and "fishes" to "fish", "family" and "families" to
* "famili", as well as "complete" and "completion" to "complet". note that
* the root form is not necessarily a meaningful word in itself, and that
* this is not a bug but rather a feature, if you lean back and think about
* fuzzy word matching for a bit.
* <p>
* see the lucene contrib packages for stemmers (and stop words) for german,
* russian and many more languages.
*
* @param child
*            the underlying child analyzer
* @return an analyzer wrapper
*/
public static analyzer getporterstemmeranalyzer final analyzer child
if  child    null
throw new illegalargumentexception
return new analyzer
public tokenstream tokenstream string fieldname  reader reader
return new porterstemfilter
child tokenstream fieldname  reader
//        /* porterstemfilter and snowballfilter have the same behaviour,
//        but porterstemfilter is much faster. */
//        return new org.apache.lucene.analysis.snowball.snowballfilter(
//            child.tokenstream(fieldname, reader), "english");
/**
* returns an analyzer wrapper that wraps the underlying child analyzer's
* token stream into a {@link synonymtokenfilter}.
*
* @param child
*            the underlying child analyzer
* @param synonyms
*            the map used to extract synonyms for terms
* @param maxsynonyms
*            the maximum number of synonym tokens to return per underlying
*            token word (a value of integer.max_value indicates unlimited)
* @return a new analyzer
*/
public static analyzer getsynonymanalyzer final analyzer child
final synonymmap synonyms  final int maxsynonyms
if  child    null
throw new illegalargumentexception
if  synonyms    null
throw new illegalargumentexception
if  maxsynonyms < 0
throw new illegalargumentexception
if  maxsynonyms    0
return child     no need to wrap
return new analyzer
public tokenstream tokenstream string fieldname  reader reader
return new synonymtokenfilter
child tokenstream fieldname  reader   synonyms  maxsynonyms
/**
* returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
* token streams, and delivers those cached tokens on subsequent calls to
* <code>tokenstream(string fieldname, reader reader)</code>
* if the fieldname has been seen before, altogether ignoring the reader parameter on cache lookup.
* <p>
* if analyzer / tokenfilter chains are expensive in terms of i/o or cpu, such caching can
* help improve performance if the same document is added to multiple lucene indexes,
* because the text analysis phase need not be performed more than once.
* <p>
* caveats:
* <ul>
* <li>caching the tokens of large lucene documents can lead to out of memory exceptions.</li>
* <li>the token instances delivered by the underlying child analyzer must be immutable.</li>
* <li>the same caching analyzer instance must not be used for more than one document
* because the cache is not keyed on the reader parameter.</li>
* </ul>
*
* @param child
*            the underlying child analyzer
* @return a new analyzer
*/
public static analyzer gettokencachinganalyzer final analyzer child
if  child    null
throw new illegalargumentexception
return new analyzer
private final hashmap cache   new hashmap
public tokenstream tokenstream string fieldname  reader reader
final arraylist tokens    arraylist  cache get fieldname
if  tokens    null       not yet cached
final arraylist tokens2   new arraylist
tokenstream tokenstream   new tokenfilter child tokenstream fieldname  reader
public token next   throws ioexception
token token   input next       from filter super class
if  token    null  tokens2 add token
return token
cache put fieldname  tokens2
return tokenstream
else      already cached
return new tokenstream
private iterator iter   tokens iterator
public token next
if   iter hasnext    return null
return  token  iter next
/**
* returns (frequency:term) pairs for the top n distinct terms (aka words),
* sorted descending by frequency (and ascending by term, if tied).
* <p>
* example xquery:
* <pre>
* declare namespace util = "java:org.apache.lucene.index.memory.analyzerutil";
* declare namespace analyzer = "java:org.apache.lucene.index.memory.patternanalyzer";
*
* for $pair in util:get-most-frequent-terms(
*    analyzer:extended_analyzer(), doc("samples/shakespeare/othello.xml"), 10)
* return &lt;word word="{substring-after($pair, ':')}" frequency="{substring-before($pair, ':')}"/>
* </pre>
*
* @param analyzer
*            the analyzer to use for splitting text into terms (aka words)
* @param text
*            the text to analyze
* @param limit
*            the maximum number of pairs to return; zero indicates
*            "as many as possible".
* @return an array of (frequency:term) pairs in the form of (freq0:term0,
*         freq1:term1, ..., freqn:termn). each pair is a single string
*         separated by a ':' delimiter.
*/
public static string getmostfrequentterms analyzer analyzer  string text  int limit
if  analyzer    null
throw new illegalargumentexception
if  text    null
throw new illegalargumentexception
if  limit <  0  limit   integer max_value
// compute frequencies of distinct terms
hashmap map   new hashmap
tokenstream stream   analyzer tokenstream    new stringreader text
try
token token
while   token   stream next       null
mutableinteger freq    mutableinteger  map get token termtext
if  freq    null
freq   new mutableinteger 1
map put token termtext    freq
else
freq setvalue freq intvalue     1
catch  ioexception e
throw new runtimeexception e
finally
try
stream close
catch  ioexception e2
throw new runtimeexception e2
// sort by frequency, text
map entry entries   new map entry
map entryset   toarray entries
arrays sort entries  new comparator
public int compare object o1  object o2
map entry e1    map entry  o1
map entry e2    map entry  o2
int f1     mutableinteger  e1 getvalue    intvalue
int f2     mutableinteger  e2 getvalue    intvalue
if  f2   f1    0  return f2   f1
string s1    string  e1 getkey
string s2    string  e2 getkey
return s1 compareto s2
// return top n entries
int size   math min limit  entries length
string pairs   new string
for  int i 0  i < size  i
pairs   entries getvalue         entries getkey
return pairs
private static final class mutableinteger
private int value
public mutableinteger int value    this value   value
public int intvalue     return value
public void setvalue int value    this value   value
public string tostring     return string valueof value
// todo: could use a more general i18n approach ala http://icu.sourceforge.net/docs/papers/text_boundary_analysis_in_java/
/** (line terminator followed by zero or more whitespace) two or more times */
private static final pattern paragraphs   pattern compile
/**
* returns at most the first n paragraphs of the given text. delimiting
* characters are excluded from the results. each returned paragraph is
* whitespace-trimmed via string.trim(), potentially an empty string.
*
* @param text
*            the text to tokenize into paragraphs
* @param limit
*            the maximum number of paragraphs to return; zero indicates "as
*            many as possible".
* @return the first n paragraphs
*/
public static string getparagraphs string text  int limit
return tokenize paragraphs  text  limit
private static string tokenize pattern pattern  string text  int limit
string tokens   pattern split text  limit
for  int i tokens length    i >  0    tokens   tokens trim
return tokens
// todo: don't split on floating point numbers, e.g. 3.1415 (digit before or after '.')
/** divides text into sentences; includes inverted spanish exclamation and question mark */
private static final pattern sentences    pattern compile
/**
* returns at most the first n sentences of the given text. delimiting
* characters are excluded from the results. each returned sentence is
* whitespace-trimmed via string.trim(), potentially an empty string.
*
* @param text
*            the text to tokenize into sentences
* @param limit
*            the maximum number of sentences to return; zero indicates "as
*            many as possible".
* @return the first n sentences
*/
public static string getsentences string text  int limit
//    return tokenize(sentences, text, limit); // equivalent but slower
int len   text length
if  len    0  return new string   text
if  limit <  0  limit   integer max_value
// average sentence length heuristic
string tokens   new string
int size   0
int i   0
while  i < len    size < limit
// scan to end of current sentence
int start   i
while  i < len     issentenceseparator text charat i    i
if  size    tokens length       grow array
string tmp   new string
system arraycopy tokens  0  tmp  0  size
tokens   tmp
// add sentence (potentially empty)
tokens   text substring start  i  trim
// scan to beginning of next sentence
while  i < len    issentenceseparator text charat i    i
if  size    tokens length  return tokens
string results   new string
system arraycopy tokens  0  results  0  size
return results
private static boolean issentenceseparator char c
// regex [!\\.\\?\\xa1\\xbf]
switch  c
case    return true
case    return true
case    return true
case 0xa1  return true     spanish inverted exclamation mark
case 0xbf  return true     spanish inverted question mark
default  return false