/*
* created on 28-oct-2004
*/
package org apache lucene search highlight
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import java io ioexception
import java io stringreader
import java util arraylist
import java util arrays
import java util comparator
import org apache lucene analysis analyzer
import org apache lucene analysis token
import org apache lucene analysis tokenstream
import org apache lucene document document
import org apache lucene index indexreader
import org apache lucene index termfreqvector
import org apache lucene index termpositionvector
import org apache lucene index termvectoroffsetinfo
/**
* hides implementation issues associated with obtaining a tokenstream for use with
* the higlighter - can obtain from termfreqvectors with offsets and (optionally) positions or
* from analyzer class reparsing the stored content.
* @author maharwood
*/
public class tokensources
/**
* a convenience method that tries a number of approaches to getting a token stream.
* the cost of finding there are no termvectors in the index is minimal (1000 invocations still
* registers 0 ms). so this "lazy" (flexible?) approach to coding is probably acceptable
* @param reader
* @param docid
* @param field
* @param analyzer
* @return null if field not stored correctly
* @throws ioexception
*/
public static tokenstream getanytokenstream indexreader reader int docid  string field analyzer analyzer  throws ioexception
tokenstream ts null
termfreqvector tfv  termfreqvector  reader gettermfreqvector docid field
if tfv  null
if tfv instanceof termpositionvector
ts gettokenstream  termpositionvector  tfv
//no token info stored so fall back to analyzing raw content
if ts  null
ts gettokenstream reader docid field analyzer
return ts
public static tokenstream gettokenstream termpositionvector tpv
//assumes the worst and makes no assumptions about token position sequences.
return gettokenstream tpv false
/**
* low level api.
* returns a token stream or null if no offset info available in index.
* this can be used to feed the highlighter with a pre-parsed token stream
*
* in my tests the speeds to recreate 1000 token streams using this method are:
* - with termvector offset only data stored - 420  milliseconds
* - with termvector offset and position data stored - 271 milliseconds
*  (nb timings for termvector with position data are based on a tokenizer with contiguous
*  positions - no overlaps or gaps)
* the cost of not using termpositionvector to store
* pre-parsed content and using an analyzer to re-parse the original content:
* - reanalyzing the original content - 980 milliseconds
*
* the re-analyze timings will typically vary depending on -
* 	1) the complexity of the analyzer code (timings above were using a
* 	   stemmer/lowercaser/stopword combo)
*  2) the  number of other fields (lucene reads all fields off the disk
*     when accessing just one document field - can cost dear!)
*  3) use of compression on field storage - could be faster cos of compression (less disk io)
*     or slower (more cpu burn) depending on the content.
*
* @param tpv
* @param tokenpositionsguaranteedcontiguous true if the token position numbers have no overlaps or gaps. if looking
* to eek out the last drops of performance, set to true. if in doubt, set to false.
*/
public static tokenstream gettokenstream termpositionvector tpv  boolean tokenpositionsguaranteedcontiguous
//an object used to iterate across an array of tokens
class storedtokenstream extends tokenstream
token tokens
int currenttoken 0
storedtokenstream token tokens
this tokens tokens
public token next
if currenttoken> tokens length
return null
return tokens
//code to reconstruct the original sequence of tokens
string terms tpv getterms
int freq tpv gettermfrequencies
int totaltokens 0
for  int t   0  t < freq length  t
totaltokens  freq
token tokensinoriginalorder new token
arraylist unsortedtokens   null
for  int t   0  t < freq length  t
termvectoroffsetinfo offsets tpv getoffsets t
if offsets  null
return null
int pos null
if tokenpositionsguaranteedcontiguous
//try get the token position info to speed up assembly of tokens into sorted sequence
pos tpv gettermpositions t
if pos  null
//tokens not stored with positions or not guaranteed contiguous - must add to list and sort later
if unsortedtokens  null
unsortedtokens new arraylist
for  int tp   0  tp < offsets length  tp
unsortedtokens add new token terms
offsets getstartoffset
offsets getendoffset
else
//we have positions stored and a guarantee that the token position information is contiguous
// this may be fast but wont work if tokenizers used which create >1 token in same position or
// creates jumps in position numbers - this code would fail under those circumstances
//tokens stored with positions - can use this to index straight into sorted array
for  int tp   0  tp < pos length  tp
tokensinoriginalorder] new token terms
offsets getstartoffset
offsets getendoffset
//if the field has been stored without position data we must perform a sort
if unsortedtokens  null
tokensinoriginalorder  token  unsortedtokens toarray new token
arrays sort tokensinoriginalorder  new comparator
public int compare object o1  object o2
token t1  token  o1
token t2  token  o2
if t1 startoffset  >t2 startoffset
return 1
if t1 startoffset  <t2 startoffset
return  1
return 0
return new storedtokenstream tokensinoriginalorder
public static tokenstream gettokenstream indexreader reader int docid  string field  throws ioexception
termfreqvector tfv  termfreqvector  reader gettermfreqvector docid field
if tfv  null
throw new illegalargumentexception field   docid
if tfv instanceof termpositionvector
termpositionvector tpv  termpositionvector  reader gettermfreqvector docid field
return gettokenstream tpv
throw new illegalargumentexception field   docid
//convenience method
public static tokenstream gettokenstream indexreader reader int docid  string field analyzer analyzer  throws ioexception
document doc reader document docid
string contents doc get field
if contents  null
throw new illegalargumentexception   field    docid
return analyzer tokenstream field new stringreader contents