package org apache lucene analysis
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import org apache lucene index payload
import org apache lucene index termpositions
/** a token is an occurence of a term from the text of a field.  it consists of
a term's text, the start and end offset of the term in the text of the field,
and a type string.
<p>
the start and end offsets permit applications to re-associate a token with
its source text, e.g., to display highlighted query terms in a document
browser, or to show matching text fragments in a kwic (keyword in context)
display, etc.
<p>
the type is an interned string, assigned by a lexical analyzer
(a.k.a. tokenizer), naming the lexical or syntactic class that the token
belongs to.  for example an end of sentence marker token might be implemented
with type "eos".  the default token type is "word".
<p>
a token can optionally have metadata (a.k.a. payload) in the form of a variable
length byte array. use {@link termpositions#getpayloadlength()} and
{@link termpositions#getpayload(byte[], int)} to retrieve the payloads from the index.
<br><br>
<p><font color="#ff0000">
warning: the status of the <b>payloads</b> feature is experimental.
the apis introduced here might change in the future and will not be
supported anymore in such a case.</font>
<br><br>
<p><b>note:</b> as of 2.3, token stores the term text
internally as a malleable char[] termbuffer instead of
string termtext.  the indexing code and core tokenizers
have been changed re-use a single token instance, changing
its buffer and other fields in-place as the token is
processed.  this provides substantially better indexing
performance as it saves the gc cost of new'ing a token and
string for every term.  the apis that accept string
termtext are still available but a warning about the
associated performance cost has been added (below).  the
{@link #termtext()} method has been deprecated.</p>
<p>tokenizers and filters should try to re-use a token
instance when possible for best performance, by
implementing the {@link tokenstream#next(token)} api.
failing that, to create a new token you should first use
one of the constructors that starts with null text.  then
you should call either {@link #termbuffer()} or {@link
#resizetermbuffer(int)} to retrieve the token's
termbuffer.  fill in the characters of your term into this
buffer, and finally call {@link #settermlength(int)} to
set the length of the term text.  see <a target="_top"
href="https://issues.apache.org/jira/browse/lucene-969">lucene-969</a>
for details.</p>
@see org.apache.lucene.index.payload
*/
public class token implements cloneable
public static final string default_type
private static int min_buffer_size   10
/** @deprecated: we will remove this when we remove the
* deprecated apis */
private string termtext
char termbuffer                                  characters for the term text
int termlength                                     length of term text in buffer
int startoffset 				     start in source text
int endoffset 				     end in source text
string type   default_type                         lexical type
payload payload
int positionincrement   1
/** constructs a token will null text. */
public token
/** constructs a token with null text and start & end
*  offsets.
*  @param start start offset
*  @param end end offset */
public token int start  int end
startoffset   start
endoffset   end
/** constructs a token with null text and start & end
*  offsets plus the token type.
*  @param start start offset
*  @param end end offset */
public token int start  int end  string typ
startoffset   start
endoffset   end
type   typ
/** constructs a token with the given term text, and start
*  & end offsets.  the type defaults to "word."
*  <b>note:</b> for better indexing speed you should
*  instead use the char[] termbuffer methods to set the
*  term text.
*  @param text term text
*  @param start start offset
*  @param end end offset */
public token string text  int start  int end
termtext   text
startoffset   start
endoffset   end
/** constructs a token with the given text, start and end
*  offsets, & type.  <b>note:</b> for better indexing
*  speed you should instead use the char[] termbuffer
*  methods to set the term text.
*  @param text term text
*  @param start start offset
*  @param end end offset
*  @param typ token type */
public token string text  int start  int end  string typ
termtext   text
startoffset   start
endoffset   end
type   typ
/** set the position increment.  this determines the position of this token
* relative to the previous token in a {@link tokenstream}, used in phrase
* searching.
*
* <p>the default value is one.
*
* <p>some common uses for this are:<ul>
*
* <li>set it to zero to put multiple terms in the same position.  this is
* useful if, e.g., a word has multiple stems.  searches for phrases
* including either stem will match.  in this case, all but the first stem's
* increment should be set to zero: the increment of the first instance
* should be one.  repeating a token with an increment of zero can also be
* used to boost the scores of matches on that token.
*
* <li>set it to values greater than one to inhibit exact phrase matches.
* if, for example, one does not want phrases to match across removed stop
* words, then one could build a stop word filter that removes stop words and
* also sets the increment to the number of stop words removed before each
* non-stop word.  then exact phrase queries will only match when the terms
* occur with no intervening stop words.
*
* </ul>
* @see org.apache.lucene.index.termpositions
*/
public void setpositionincrement int positionincrement
if  positionincrement < 0
throw new illegalargumentexception
positionincrement
this positionincrement   positionincrement
/** returns the position increment of this token.
* @see #setpositionincrement
*/
public int getpositionincrement
return positionincrement
/** sets the token's term text.  <b>note:</b> for better
*  indexing speed you should instead use the char[]
*  termbuffer methods to set the term text. */
public void settermtext string text
termtext   text
termbuffer   null
/** returns the token's term text.
*
* @deprecated use {@link #termbuffer()} and {@link
* #termlength()} instead. */
public final string termtext
if  termtext    null    termbuffer    null
termtext   new string termbuffer  0  termlength
return termtext
/** copies the contents of buffer, starting at offset for
*  length characters, into the termbuffer
*  array. <b>note:</b> for better indexing speed you
*  should instead retrieve the termbuffer, using {@link
*  #termbuffer()} or {@link #resizetermbuffer(int)}, and
*  fill it in directly to set the term text.  this saves
*  an extra copy. */
public final void settermbuffer char buffer  int offset  int length
resizetermbuffer length
system arraycopy buffer  offset  termbuffer  0  length
termlength   length
/** returns the internal termbuffer character array which
*  you can then directly alter.  if the array is too
*  small for your token, use {@link
*  #resizetermbuffer(int)} to increase it.  after
*  altering the buffer be sure to call {@link
*  #settermlength} to record the number of valid
*  characters that were placed into the termbuffer. */
public final char termbuffer
inittermbuffer
return termbuffer
/** grows the termbuffer to at least size newsize.
*  @param newsize minimum size of the new termbuffer
*  @return newly created termbuffer with length >= newsize
*/
public char resizetermbuffer int newsize
inittermbuffer
if  newsize > termbuffer length
int size   termbuffer length
while size < newsize
size    2
char newbuffer   new char
system arraycopy termbuffer  0  newbuffer  0  termbuffer length
termbuffer   newbuffer
return termbuffer
// todo: once we remove the deprecated termtext() method
// and switch entirely to char[] termbuffer we don't need
// to use this method anymore
private void inittermbuffer
if  termbuffer    null
if  termtext    null
termbuffer   new char
termlength   0
else
int length   termtext length
if  length < min_buffer_size  length   min_buffer_size
termbuffer   new char
termlength   termtext length
termtext getchars 0  termtext length    termbuffer  0
termtext   null
else if  termtext    null
termtext   null
/** return number of valid characters (length of the term)
*  in the termbuffer array. */
public final int termlength
inittermbuffer
return termlength
/** set number of valid characters (length of the term) in
*  the termbuffer array. */
public final void settermlength int length
inittermbuffer
termlength   length
/** returns this token's starting offset, the position of the first character
corresponding to this token in the source text.
note that the difference between endoffset() and startoffset() may not be
equal to termtext.length(), as the term text may have been altered by a
stemmer or some other filter. */
public final int startoffset
return startoffset
/** set the starting offset.
@see #startoffset() */
public void setstartoffset int offset
this startoffset   offset
/** returns this token's ending offset, one greater than the position of the
last character corresponding to this token in the source text. */
public final int endoffset
return endoffset
/** set the ending offset.
@see #endoffset() */
public void setendoffset int offset
this endoffset   offset
/** returns this token's lexical type.  defaults to "word". */
public final string type
return type
/** set the lexical type.
@see #type() */
public final void settype string type
this type   type
/**
* returns this token's payload.
*/
public payload getpayload
return this payload
/**
* sets this token's payload.
*/
public void setpayload payload payload
this payload   payload
public string tostring
stringbuffer sb   new stringbuffer
sb append
inittermbuffer
if  termbuffer    null
sb append
else
sb append termbuffer  0  termlength
sb append    append startoffset  append    append endoffset
if   type equals
sb append    append type
if  positionincrement    1
sb append    append positionincrement
sb append
return sb tostring
/** resets the term text, payload, and positionincrement to default.
* other fields such as startoffset, endoffset and the token type are
* not reset since they are normally overwritten by the tokenizer. */
public void clear
payload   null
// leave termbuffer to allow re-use
termlength   0
termtext   null
positionincrement   1
// startoffset = endoffset = 0;
// type = default_type;
public object clone
try
token t    token super clone
if  termbuffer    null
t termbuffer   null
t settermbuffer termbuffer  0  termlength
if  payload    null
t setpayload  payload  payload clone
return t
catch  clonenotsupportedexception e
throw new runtimeexception e       shouldn't happen