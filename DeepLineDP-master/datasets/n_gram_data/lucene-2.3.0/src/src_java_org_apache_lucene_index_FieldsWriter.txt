package org apache lucene index
/**
* copyright 2004 the apache software foundation
*
* licensed under the apache license, version 2.0 (the "license"); you may not
* use this file except in compliance with the license. you may obtain a copy of
* the license at
*
* http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis, without
* warranties or conditions of any kind, either express or implied. see the
* license for the specific language governing permissions and limitations under
* the license.
*/
import java io bytearrayoutputstream
import java io ioexception
import java util iterator
import java util zip deflater
import org apache lucene document document
import org apache lucene document fieldable
import org apache lucene store directory
import org apache lucene store ramoutputstream
import org apache lucene store indexoutput
import org apache lucene store indexinput
final class fieldswriter
static final byte field_is_tokenized   0x1
static final byte field_is_binary   0x2
static final byte field_is_compressed   0x4
private fieldinfos fieldinfos
private indexoutput fieldsstream
private indexoutput indexstream
private boolean doclose
fieldswriter directory d  string segment  fieldinfos fn  throws ioexception
fieldinfos   fn
fieldsstream   d createoutput segment
indexstream   d createoutput segment
doclose   true
fieldswriter indexoutput fdx  indexoutput fdt  fieldinfos fn  throws ioexception
fieldinfos   fn
fieldsstream   fdt
indexstream   fdx
doclose   false
// writes the contents of buffer into the fields stream
// and adds a new entry for this document into the index
// stream.  this assumes the buffer was already written
// in the correct fields format.
void flushdocument int numstoredfields  ramoutputstream buffer  throws ioexception
indexstream writelong fieldsstream getfilepointer
fieldsstream writevint numstoredfields
buffer writeto fieldsstream
void flush   throws ioexception
indexstream flush
fieldsstream flush
final void close   throws ioexception
if  doclose
fieldsstream close
indexstream close
final void writefield fieldinfo fi  fieldable field  throws ioexception
// if the field as an instanceof fieldsreader.fieldformerge, we're in merge mode
// and field.binaryvalue() already returns the compressed value for a field
// with iscompressed()==true, so we disable compression in that case
boolean disablecompression    field instanceof fieldsreader fieldformerge
fieldsstream writevint fi number
byte bits   0
if  field istokenized
bits    fieldswriter field_is_tokenized
if  field isbinary
bits    fieldswriter field_is_binary
if  field iscompressed
bits    fieldswriter field_is_compressed
fieldsstream writebyte bits
if  field iscompressed
// compression is enabled for the current field
byte data   null
if  disablecompression
// optimized case for merging, the data
// is already compressed
data   field binaryvalue
else
// check if it is a binary field
if  field isbinary
data   compress field binaryvalue
else
data   compress field stringvalue   getbytes
final int len   data length
fieldsstream writevint len
fieldsstream writebytes data  len
else
// compression is disabled for the current field
if  field isbinary
byte data   field binaryvalue
final int len   data length
fieldsstream writevint len
fieldsstream writebytes data  len
else
fieldsstream writestring field stringvalue
/** bulk write a contiguous series of documents.  the
*  lengths array is the length (in bytes) of each raw
*  document.  the stream indexinput is the
*  fieldsstream from which we should bulk-copy all
*  bytes. */
final void addrawdocuments indexinput stream  int lengths  int numdocs  throws ioexception
long position   fieldsstream getfilepointer
long start   position
for int i 0 i<numdocs i
indexstream writelong position
position    lengths
fieldsstream copybytes stream  position start
assert fieldsstream getfilepointer      position
final void adddocument document doc  throws ioexception
indexstream writelong fieldsstream getfilepointer
int storedcount   0
iterator fielditerator   doc getfields   iterator
while  fielditerator hasnext
fieldable field    fieldable  fielditerator next
if  field isstored
storedcount
fieldsstream writevint storedcount
fielditerator   doc getfields   iterator
while  fielditerator hasnext
fieldable field    fieldable  fielditerator next
if  field isstored
writefield fieldinfos fieldinfo field name     field
private final byte compress  byte input
// create the compressor with highest level of compression
deflater compressor   new deflater
compressor setlevel deflater best_compression
// give the compressor the data to compress
compressor setinput input
compressor finish
/*
* create an expandable byte array to hold the compressed data.
* you cannot use an array that's the same size as the orginal because
* there is no guarantee that the compressed data will be smaller than
* the uncompressed data.
*/
bytearrayoutputstream bos   new bytearrayoutputstream input length
// compress the data
byte buf   new byte
while   compressor finished
int count   compressor deflate buf
bos write buf  0  count
compressor end
// get the compressed data
return bos tobytearray