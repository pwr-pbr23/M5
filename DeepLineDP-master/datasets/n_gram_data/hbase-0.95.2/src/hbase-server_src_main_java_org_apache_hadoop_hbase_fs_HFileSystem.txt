/*
* copyright the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase fs
import java io ioexception
import java lang reflect field
import java lang reflect invocationhandler
import java lang reflect invocationtargetexception
import java lang reflect method
import java lang reflect modifier
import java lang reflect proxy
import java lang reflect undeclaredthrowableexception
import java net uri
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filesystem
import org apache hadoop fs filterfilesystem
import org apache hadoop fs localfilesystem
import org apache hadoop fs path
import org apache hadoop hbase servername
import org apache hadoop hbase regionserver wal hlogutil
import org apache hadoop hdfs dfsclient
import org apache hadoop hdfs distributedfilesystem
import org apache hadoop hdfs protocol clientprotocol
import org apache hadoop hdfs protocol datanodeinfo
import org apache hadoop hdfs protocol locatedblock
import org apache hadoop hdfs protocol locatedblocks
import org apache hadoop io closeable
import org apache hadoop util progressable
import org apache hadoop util reflectionutils
/**
* an encapsulation for the filesystem object that hbase uses to access
* data. this class allows the flexibility of using
* separate filesystem objects for reading and writing hfiles and hlogs.
* in future, if we want to make hlogs be in a different filesystem,
* this is the place to make it happen.
*/
public class hfilesystem extends filterfilesystem
public static final log log   logfactory getlog hfilesystem class
private final filesystem nochecksumfs       read hfile data from storage
private final boolean usehbasechecksum
/**
* create a filesystem object for hbase regionservers.
* @param conf the configuration to be used for the filesystem
* @param usehbasechecksum if true, then use
*        checksum verfication in hbase, otherwise
*        delegate checksum verification to the filesystem.
*/
public hfilesystem configuration conf  boolean usehbasechecksum
throws ioexception
// create the default filesystem with checksum verification switched on.
// by default, any operation to this filterfilesystem occurs on
// the underlying filesystem that has checksums switched on.
this fs   filesystem get conf
this usehbasechecksum   usehbasechecksum
fs initialize getdefaulturi conf   conf
addlocationsorderinterceptor conf
// if hbase checksum verification is switched on, then create a new
// filesystem object that has cksum verification turned off.
// we will avoid verifying checksums in the fs client, instead do it
// inside of hbase.
// if this is the local file system hadoop has a bug where seeks
// do not go to the correct location if setverifychecksum(false) is called.
// this manifests itself in that incorrect data is read and hfileblocks won't be able to read
// their header magic numbers. see hbase-5885
if  usehbasechecksum      fs instanceof localfilesystem
conf   new configuration conf
conf setboolean    true
this nochecksumfs   newinstancefilesystem conf
this nochecksumfs setverifychecksum false
else
this nochecksumfs   fs
/**
* wrap a filesystem object within a hfilesystem. the nochecksumfs and
* writefs are both set to be the same specified fs.
* do not verify hbase-checksums while reading data from filesystem.
* @param fs set the nochecksumfs and writefs to this specified filesystem.
*/
public hfilesystem filesystem fs
this fs   fs
this nochecksumfs   fs
this usehbasechecksum   false
/**
* returns the filesystem that is specially setup for
* doing reads from storage. this object avoids doing
* checksum verifications for reads.
* @return the filesystem object that can be used to read data
*         from files.
*/
public filesystem getnochecksumfs
return nochecksumfs
/**
* returns the underlying filesystem
* @return the underlying filesystem for this filterfilesystem object.
*/
public filesystem getbackingfs   throws ioexception
return fs
/**
* are we verifying checksums in hbase?
* @return true, if hbase is configured to verify checksums,
*         otherwise false.
*/
public boolean usehbasechecksum
return usehbasechecksum
/**
* close this filesystem object
*/
@override
public void close   throws ioexception
super close
if  this nochecksumfs    fs
this nochecksumfs close
/**
* returns a brand new instance of the filesystem. it does not use
* the filesystem.cache. in newer versions of hdfs, we can directly
* invoke filesystem.newinstance(configuration).
*
* @param conf configuration
* @return a new instance of the filesystem
*/
private static filesystem newinstancefilesystem configuration conf
throws ioexception
uri uri   filesystem getdefaulturi conf
filesystem fs   null
class<?> clazz   conf getclass     uri getscheme        null
if  clazz    null
// this will be true for hadoop 1.0, or 0.20.
fs    filesystem reflectionutils newinstance clazz  conf
fs initialize uri  conf
else
// for hadoop 2.0, we have to go through filesystem for the filesystem
// implementation to be loaded by the service loader in case it has not
// been loaded yet.
configuration clone   new configuration conf
clone setboolean     uri getscheme        true
fs   filesystem get uri  clone
if  fs    null
throw new ioexception     uri getscheme
return fs
public static boolean addlocationsorderinterceptor configuration conf  throws ioexception
return addlocationsorderinterceptor conf  new reorderwalblocks
/**
* add an interceptor on the calls to the namenode#getblocklocations from the dfsclient
* linked to this filesystem. see hbase-6435 for the background.
* <p/>
* there should be no reason, except testing, to create a specific reorderblocks.
*
* @return true if the interceptor was added, false otherwise.
*/
static boolean addlocationsorderinterceptor configuration conf  final reorderblocks lrb
if   conf getboolean    true         activated by default
log debug
return false
filesystem fs
try
fs   filesystem get conf
catch  ioexception e
log warn    e
return false
if    fs instanceof distributedfilesystem
log debug
return false
distributedfilesystem dfs    distributedfilesystem  fs
dfsclient dfsc   dfs getclient
if  dfsc    null
log warn
return false
try
field nf   dfsclient class getdeclaredfield
nf setaccessible true
field modifiersfield   field class getdeclaredfield
modifiersfield setaccessible true
modifiersfield setint nf  nf getmodifiers     ~modifier final
clientprotocol namenode    clientprotocol  nf get dfsc
if  namenode    null
log warn
return false
clientprotocol cp1   createreorderingproxy namenode  lrb  conf
nf set dfsc  cp1
log info
lrb getclass
catch  nosuchfieldexception e
log warn    e
return false
catch  illegalaccessexception e
log warn    e
return false
return true
private static clientprotocol createreorderingproxy final clientprotocol cp
final reorderblocks lrb  final configuration conf
return  clientprotocol  proxy newproxyinstance
cp getclass   getclassloader
new class clientprotocol class  closeable class
new invocationhandler
public object invoke object proxy  method method
object args  throws throwable
try
object res   method invoke cp  args
if  res    null    args    null    args length    3
equals method getname
res instanceof locatedblocks
args instanceof string
args    null
lrb reorderblocks conf   locatedblocks  res   string  args
return res
catch   invocationtargetexception ite
// we will have this for all the exception, checked on not, sent
//  by any layer, including the functional exception
throwable cause   ite getcause
if  cause    null
throw new runtimeexception
ite
if  cause instanceof undeclaredthrowableexception
throwable causecause   cause getcause
if  causecause    null
throw new runtimeexception
cause   cause getcause
throw cause
/**
* interface to implement to add a specific reordering logic in hdfs.
*/
interface reorderblocks
/**
*
* @param conf - the conf to use
* @param lbs - the locatedblocks to reorder
* @param src - the file name currently read
* @throws ioexception - if something went wrong
*/
void reorderblocks configuration conf  locatedblocks lbs  string src  throws ioexception
/**
* we're putting at lowest priority the hlog files blocks that are on the same datanode
* as the original regionserver which created these files. this because we fear that the
* datanode is actually dead, so if we use it it will timeout.
*/
static class reorderwalblocks implements reorderblocks
public void reorderblocks configuration conf  locatedblocks lbs  string src
throws ioexception
servername sn   hlogutil getservernamefromhlogdirectoryname conf  src
if  sn    null
// it's not an hlog
return
// ok, so it's an hlog
string hostname   sn gethostname
log debug src       hostname
// just check for all blocks
for  locatedblock lb   lbs getlocatedblocks
datanodeinfo dnis   lb getlocations
if  dnis    null    dnis length > 1
boolean found   false
for  int i   0  i < dnis length   1     found  i
if  hostname equals dnis gethostname
// advance the other locations by one and put this one at the last place.
datanodeinfo tolast   dnis
system arraycopy dnis  i   1  dnis  i  dnis length   i   1
dnis   tolast
found   true
/**
* create a new hfilesystem object, similar to filesystem.get().
* this returns a filesystem object that avoids checksum
* verification in the filesystem for hfileblock-reads.
* for these blocks, checksum verification is done by hbase.
*/
static public filesystem get configuration conf  throws ioexception
return new hfilesystem conf  true
/**
* wrap a localfilesystem within a hfilesystem.
*/
static public filesystem getlocalfs configuration conf  throws ioexception
return new hfilesystem filesystem getlocal conf
/**
* the org.apache.hadoop.fs.filterfilesystem does not yet support
* createnonrecursive. this is a hadoop bug and when it is fixed in hadoop,
* this definition will go away.
*/
public fsdataoutputstream createnonrecursive path f
boolean overwrite
int buffersize  short replication  long blocksize
progressable progress  throws ioexception
return fs createnonrecursive f  overwrite  buffersize  replication
blocksize  progress