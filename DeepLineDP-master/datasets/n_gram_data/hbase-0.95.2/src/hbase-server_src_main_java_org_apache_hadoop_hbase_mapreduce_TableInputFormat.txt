/**
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io ioexception
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop classification interfaceaudience
import org apache hadoop classification interfacestability
import org apache hadoop conf configurable
import org apache hadoop conf configuration
import org apache hadoop hbase keyvalue
import org apache hadoop hbase client htable
import org apache hadoop hbase client scan
import org apache hadoop hbase util bytes
import org apache hadoop util stringutils
/**
* convert hbase tabular data into a format that is consumable by map/reduce.
*/
@interfaceaudience public
@interfacestability stable
public class tableinputformat extends tableinputformatbase
implements configurable
private final log log   logfactory getlog tableinputformat class
/** job parameter that specifies the input table. */
public static final string input_table
/** base-64 encoded scanner. all other scan_ confs are ignored if this is specified.
* see {@link tablemapreduceutil#convertscantostring(scan)} for more details.
*/
public static final string scan
/** scan start row */
public static final string scan_row_start
/** scan stop row */
public static final string scan_row_stop
/** column family to scan */
public static final string scan_column_family
/** space delimited list of columns to scan. */
public static final string scan_columns
/** the timestamp used to filter columns with a specific timestamp. */
public static final string scan_timestamp
/** the starting timestamp used to filter columns with a specific range of versions. */
public static final string scan_timerange_start
/** the ending timestamp used to filter columns with a specific range of versions. */
public static final string scan_timerange_end
/** the maximum number of version to return. */
public static final string scan_maxversions
/** set to false to disable server-side caching of blocks for this scan. */
public static final string scan_cacheblocks
/** the number of rows for caching that will be passed to scanners. */
public static final string scan_cachedrows
/** the configuration. */
private configuration conf   null
/**
* returns the current configuration.
*
* @return the current configuration.
* @see org.apache.hadoop.conf.configurable#getconf()
*/
@override
public configuration getconf
return conf
/**
* sets the configuration. this is used to set the details for the table to
* be scanned.
*
* @param configuration  the configuration to set.
* @see org.apache.hadoop.conf.configurable#setconf(
*   org.apache.hadoop.conf.configuration)
*/
@override
public void setconf configuration configuration
this conf   configuration
string tablename   conf get input_table
try
sethtable new htable new configuration conf   tablename
catch  exception e
log error stringutils stringifyexception e
scan scan   null
if  conf get scan     null
try
scan   tablemapreduceutil convertstringtoscan conf get scan
catch  ioexception e
log error    e
else
try
scan   new scan
if  conf get scan_row_start     null
scan setstartrow bytes tobytes conf get scan_row_start
if  conf get scan_row_stop     null
scan setstoprow bytes tobytes conf get scan_row_stop
if  conf get scan_columns     null
addcolumns scan  conf get scan_columns
if  conf get scan_column_family     null
scan addfamily bytes tobytes conf get scan_column_family
if  conf get scan_timestamp     null
scan settimestamp long parselong conf get scan_timestamp
if  conf get scan_timerange_start     null    conf get scan_timerange_end     null
scan settimerange
long parselong conf get scan_timerange_start
long parselong conf get scan_timerange_end
if  conf get scan_maxversions     null
scan setmaxversions integer parseint conf get scan_maxversions
if  conf get scan_cachedrows     null
scan setcaching integer parseint conf get scan_cachedrows
// false by default, full table scans generate too much bc churn
scan setcacheblocks  conf getboolean scan_cacheblocks  false
catch  exception e
log error stringutils stringifyexception e
setscan scan
/**
* parses a combined family and qualifier and adds either both or just the
* family in case there is not qualifier. this assumes the older colon
* divided notation, e.g. "data:contents" or "meta:".
* <p>
* note: it will through an error when the colon is missing.
*
* @param familyandqualifier family and qualifier
* @return a reference to this instance.
* @throws illegalargumentexception when the colon is missing.
*/
private static void addcolumn scan scan  byte familyandqualifier
byte  fq   keyvalue parsecolumn familyandqualifier
if  fq length > 1    fq    null    fq length > 0
scan addcolumn fq  fq
else
scan addfamily fq
/**
* adds an array of columns specified using old format, family:qualifier.
* <p>
* overrides previous calls to addfamily for any families in the input.
*
* @param columns array of columns, formatted as <pre>family:qualifier</pre>
*/
public static void addcolumns scan scan  byte  columns
for  byte column   columns
addcolumn scan  column
/**
* convenience method to help parse old style (or rather user entry on the
* command line) column definitions, e.g. "data:contents mime:". the columns
* must be space delimited and always have a colon (":") to denote family
* and qualifier.
*
* @param columns  the columns to parse.
* @return a reference to this instance.
*/
private static void addcolumns scan scan  string columns
string cols   columns split
for  string col   cols
addcolumn scan  bytes tobytes col