/**
* copyright 2010 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase regionserver wal
import java io ioexception
import java io outputstream
import java lang reflect field
import java lang reflect invocationtargetexception
import java lang reflect method
import java util treemap
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hbase hconstants
import org apache hadoop io sequencefile
import org apache hadoop io text
import org apache hadoop io sequencefile compressiontype
import org apache hadoop io sequencefile metadata
import org apache hadoop io compress compressioncodec
import org apache hadoop io compress defaultcodec
/**
* implementation of {@link hlog.writer} that delegates to
* sequencefile.writer.
*/
public class sequencefilelogwriter implements hlog writer
static final text wal_version_key   new text
// let the version be 1.  let absence of a version meta tag be old, version 0.
// set this version '1' to be the version that introduces compression,
// the compression_version.
private static final int compression_version   1
static final int version   compression_version
static final text wal_version   new text     version
static final text wal_compression_type_key   new text
static final text dictionary_compression_type   new text
private final log log   logfactory getlog this getclass
// the sequence file we delegate to.
private sequencefile writer writer
// this is the fsdataoutputstream instance that is the 'out' instance
// in the sequencefile.writer 'writer' instance above.
private fsdataoutputstream writer_out
private class<? extends hlogkey> keyclass
/**
* context used by our wal dictionary compressor.  null if we're not to do
* our custom dictionary compression.  this custom wal compression is distinct
* from sequencefile native compression.
*/
private compressioncontext compressioncontext
private method syncfs   null
private method hflush   null
/**
* default constructor.
*/
public sequencefilelogwriter
super
/**
* this constructor allows a specific hlogkey implementation to override that
* which would otherwise be chosen via configuration property.
*
* @param keyclass
*/
public sequencefilelogwriter class<? extends hlogkey> keyclass
this keyclass   keyclass
/**
* create sequence file metadata for our wal file with version and compression
* type (if any).
* @param conf
* @param compress
* @return metadata instance.
*/
private static metadata createmetadata final configuration conf
final boolean compress
treemap<text  text> metamap   new treemap<text  text>
metamap put wal_version_key  wal_version
if  compress
// currently we only do one compression type.
metamap put wal_compression_type_key  dictionary_compression_type
return new metadata metamap
/**
* call this method after init() has been executed
*
* @return whether wal compression is enabled
*/
static boolean iswalcompressionenabled final metadata metadata
// check version is >= version?
text txt   metadata get wal_version_key
if  txt    null    integer parseint txt tostring    < compression_version
return false
// now check that compression type is present.  currently only one value.
txt   metadata get wal_compression_type_key
return txt    null    txt equals dictionary_compression_type
@override
public void init filesystem fs  path path  configuration conf
throws ioexception
// should we do our custom wal compression?
boolean compress   conf getboolean hconstants enable_wal_compression  false
if  compress
try
if  this compressioncontext    null
this compressioncontext   new compressioncontext lrudictionary class
else
this compressioncontext clear
catch  exception e
throw new ioexception    e
if  null    keyclass
keyclass   hlog getkeyclass conf
// create a sf.writer instance.
try
// reflection for a version of sequencefile.createwriter that doesn't
// automatically create the parent directory (see hbase-2312)
this writer    sequencefile writer  sequencefile class
getmethod    new class  filesystem class
configuration class  path class  class class  class class
integer type  short type  long type  boolean type
compressiontype class  compressioncodec class  metadata class
invoke null  new object  fs  conf  path  hlog getkeyclass conf
waledit class
integer valueof fs getconf   getint    4096
short valueof  short
conf getint
fs getdefaultreplication
long valueof conf getlong
fs getdefaultblocksize
boolean valueof false    createparent
sequencefile compressiontype none  new defaultcodec
createmetadata conf  compress
catch  invocationtargetexception ite
// function was properly called, but threw it's own exception
throw new ioexception ite getcause
catch  exception e
// ignore all other exceptions. related to reflection failure
// if reflection failed, use the old createwriter
if  this writer    null
log debug
this writer   sequencefile createwriter fs  conf  path
hlog getkeyclass conf   waledit class
fs getconf   getint    4096
short  conf getint
fs getdefaultreplication
conf getlong
fs getdefaultblocksize
sequencefile compressiontype none
new defaultcodec

createmetadata conf  compress
else
log debug
this writer_out   getsequencefileprivatefsdataoutputstreamaccessible
this syncfs   getsyncfs
this hflush   gethflush
string msg       path
this syncfs    null
this hflush    null
compress
if  this syncfs    null    this hflush    null
log debug msg
else
log warn     msg
/**
* now do dirty work to see if syncfs is available on the backing this.writer.
* it will be available in branch-0.20-append and in cdh3.
* @return the syncfs method or null if not available.
* @throws ioexception
*/
private method getsyncfs
throws ioexception
method m   null
try
// function pointer to writer.syncfs() method; present when sync is hdfs-200.
m   this writer getclass   getmethod    new class<?>
catch  securityexception e
throw new ioexception    e
catch  nosuchmethodexception e
// not available
return m
/**
* see if hflush (0.21 and 0.22 hadoop) is available.
* @return the hflush method or null if not available.
* @throws ioexception
*/
private method gethflush
throws ioexception
method m   null
try
class<? extends outputstream> c   getwriterfsdataoutputstream   getclass
m   c getmethod    new class<?>
catch  securityexception e
throw new ioexception    e
catch  nosuchmethodexception e
// ignore
return m
// get at the private fsdataoutputstream inside in sequencefile so we can
// call sync on it.  make it accessible.
private fsdataoutputstream getsequencefileprivatefsdataoutputstreamaccessible
throws ioexception
fsdataoutputstream out   null
final field fields    this writer getclass   getdeclaredfields
final string fieldname
for  int i   0  i < fields length    i
if  fieldname equals fields getname
try
// make the 'out' field up in sf.writer accessible.
fields setaccessible true
out    fsdataoutputstream fields get this writer
break
catch  illegalaccessexception ex
throw new ioexception     fieldname  ex
catch  securityexception e
// todo auto-generated catch block
e printstacktrace
return out
@override
public void append hlog entry entry  throws ioexception
entry setcompressioncontext compressioncontext
try
this writer append entry getkey    entry getedit
catch  nullpointerexception npe
// concurrent close...
throw new ioexception npe
@override
public void close   throws ioexception
if  this writer    null
try
this writer close
catch  nullpointerexception npe
// can get a npe coming up from down in dfsclient$dfsoutputstream#close
log warn npe
this writer   null
@override
public void sync   throws ioexception
if  this syncfs    null
try
this syncfs invoke this writer  hlog no_args
catch  exception e
throw new ioexception    e
else if  this hflush    null
try
this hflush invoke getwriterfsdataoutputstream    hlog no_args
catch  exception e
throw new ioexception    e
@override
public long getlength   throws ioexception
try
return this writer getlength
catch  nullpointerexception npe
// concurrent close...
throw new ioexception npe
/**
* @return the dfsclient out stream up inside sf.writer made accessible, or
* null if not available.
*/
public fsdataoutputstream getwriterfsdataoutputstream
return this writer_out