/**
* copyright 2009 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io ioexception
import org apache hadoop conf configuration
import org apache hadoop fs path
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase client result
import org apache hadoop hbase client scan
import org apache hadoop hbase filter prefixfilter
import org apache hadoop hbase filter rowfilter
import org apache hadoop hbase filter regexstringcomparator
import org apache hadoop hbase filter comparefilter compareop
import org apache hadoop hbase filter filter
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase util bytes
import org apache hadoop mapreduce job
import org apache hadoop mapreduce lib output fileoutputformat
import org apache hadoop mapreduce lib output sequencefileoutputformat
import org apache hadoop util genericoptionsparser
import org apache commons logging log
import org apache commons logging logfactory
/**
* export an hbase table.
* writes content to sequence files up in hdfs.  use {@link import} to read it
* back in again.
*/
public class export
private static final log log   logfactory getlog export class
final static string name
final static string raw_scan
/**
* mapper.
*/
static class exporter
extends tablemapper<immutablebyteswritable  result>
/**
* @param row  the current table row key.
* @param value  the columns.
* @param context  the current context.
* @throws ioexception when something is broken with the data.
* @see org.apache.hadoop.mapreduce.mapper#map(keyin, valuein,
*   org.apache.hadoop.mapreduce.mapper.context)
*/
@override
public void map immutablebyteswritable row  result value
context context
throws ioexception
try
context write row  value
catch  interruptedexception e
e printstacktrace
/**
* sets up the actual job.
*
* @param conf  the current configuration.
* @param args  the command line parameters.
* @return the newly created job.
* @throws ioexception when setting up the job fails.
*/
public static job createsubmittablejob configuration conf  string args
throws ioexception
string tablename   args
path outputdir   new path args
job job   new job conf  name       tablename
job setjobname name       tablename
job setjarbyclass exporter class
// set optional scan parameters
scan s   getconfiguredscanforjob conf  args
tablemapreduceutil inittablemapperjob tablename  s  exporter class  null
null  job
// no reducers.  just write straight to output files.
job setnumreducetasks 0
job setoutputformatclass sequencefileoutputformat class
job setoutputkeyclass immutablebyteswritable class
job setoutputvalueclass result class
fileoutputformat setoutputpath job  outputdir
return job
private static scan getconfiguredscanforjob configuration conf  string args  throws ioexception
scan s   new scan
// optional arguments.
// set scan versions
int versions   args length > 2? integer parseint args   1
s setmaxversions versions
// set scan range
long starttime   args length > 3? long parselong args   0l
long endtime   args length > 4? long parselong args   long max_value
s settimerange starttime  endtime
// set cache blocks
s setcacheblocks false
// set scan column family
boolean raw   boolean parseboolean conf get raw_scan
if  raw
s setraw raw
if  conf get tableinputformat scan_column_family     null
s addfamily bytes tobytes conf get tableinputformat scan_column_family
// set rowfilter or prefix filter if applicable.
filter exportfilter   getexportfilter args
if  exportfilter   null
log info
s setfilter exportfilter
log info     versions       starttime
endtime       raw
return s
private static filter getexportfilter string args
filter exportfilter   null
string filtercriteria    args length > 5  ? args  null
if  filtercriteria    null  return null
if  filtercriteria startswith
string regexpattern   filtercriteria substring 1  filtercriteria length
exportfilter   new rowfilter compareop equal  new regexstringcomparator regexpattern
else
exportfilter   new prefixfilter bytes tobytes filtercriteria
return exportfilter
/*
* @param errormsg error message.  can be null.
*/
private static void usage final string errormsg
if  errormsg    null    errormsg length   > 0
system err println     errormsg
system err println
system err println
system err println
system err println
system err println
system err println
system err println
system err println
system err println     tableinputformat scan_column_family
system err println     raw_scan
system err println
/**
* main entry point.
*
* @param args  the command line parameters.
* @throws exception when running the job fails.
*/
public static void main string args  throws exception
configuration conf   hbaseconfiguration create
string otherargs   new genericoptionsparser conf  args  getremainingargs
if  otherargs length < 2
usage     otherargs length
system exit  1
job job   createsubmittablejob conf  otherargs
system exit job waitforcompletion true ? 0   1