/**
* copyright 2009 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io ioexception
import org apache hadoop conf configuration
import org apache hadoop fs path
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase client put
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase mapreduce tablemapreduceutil
import org apache hadoop hbase util bytes
import org apache hadoop io longwritable
import org apache hadoop io text
import org apache hadoop mapreduce job
import org apache hadoop mapreduce mapper
import org apache hadoop mapreduce lib input fileinputformat
import org apache hadoop mapreduce lib input sequencefileinputformat
import org apache hadoop util genericoptionsparser
/**
* sample uploader mapreduce
* <p>
* this is example code.  you will need to change it to work for your context.
* <p>
* uses {@link tablereducer} to put the data into hbase. change the inputformat
* to suit your data.  in this example, we are importing a csv file.
* <p>
* <pre>row,family,qualifier,value</pre>
* <p>
* the table and columnfamily we're to insert into must preexist.
* <p>
* there is no reducer in this example as it is not necessary and adds
* significant overhead.  if you need to do any massaging of data before
* inserting into hbase, you can do this in the map as well.
* <p>do the following to start the mr job:
* <pre>
* ./bin/hadoop org.apache.hadoop.hbase.mapreduce.sampleuploader /tmp/input.csv table_name
* </pre>
* <p>
* this code was written against hbase 0.21 trunk.
*/
public class sampleuploader
private static final string name
static class uploader
extends mapper<longwritable  text  immutablebyteswritable  put>
private long checkpoint   100
private long count   0
@override
public void map longwritable key  text line  context context
throws ioexception
// input is a csv file
// each map() is a single line, where the key is the line number
// each line is comma-delimited; row,family,qualifier,value
// split csv line
string  values   line tostring   split
if values length    4
return
// extract each value
byte  row   bytes tobytes values
byte  family   bytes tobytes values
byte  qualifier   bytes tobytes values
byte  value   bytes tobytes values
// create put
put put   new put row
put add family  qualifier  value
// uncomment below to disable wal. this will improve performance but means
// you will experience data loss in the case of a regionserver crash.
// put.setwritetowal(false);
try
context write new immutablebyteswritable row   put
catch  interruptedexception e
e printstacktrace
// set status every checkpoint lines
if   count % checkpoint    0
context setstatus     count
/**
* job configuration.
*/
public static job configurejob configuration conf  string  args
throws ioexception
path inputpath   new path args
string tablename   args
job job   new job conf  name       tablename
job setjarbyclass uploader class
fileinputformat setinputpaths job  inputpath
job setinputformatclass sequencefileinputformat class
job setmapperclass uploader class
// no reducers.  just write straight to table.  call inittablereducerjob
// because it sets up the tableoutputformat.
tablemapreduceutil inittablereducerjob tablename  null  job
job setnumreducetasks 0
return job
/**
* main entry point.
*
* @param args  the command line parameters.
* @throws exception when running the job fails.
*/
public static void main string args  throws exception
configuration conf   hbaseconfiguration create
string otherargs   new genericoptionsparser conf  args  getremainingargs
if otherargs length    2
system err println     otherargs length
system err println     name
system exit  1
job job   configurejob conf  otherargs
system exit job waitforcompletion true  ? 0   1