/**
* copyright 2009 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io ioexception
import java util map
import java util treemap
import org apache hadoop conf configuration
import org apache hadoop fs path
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase keyvalue
import org apache hadoop hbase client delete
import org apache hadoop hbase client htable
import org apache hadoop hbase client mutation
import org apache hadoop hbase client put
import org apache hadoop hbase client result
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase util bytes
import org apache hadoop mapreduce job
import org apache hadoop mapreduce lib input fileinputformat
import org apache hadoop mapreduce lib input sequencefileinputformat
import org apache hadoop mapreduce lib output fileoutputformat
import org apache hadoop util genericoptionsparser
/**
* import data written by {@link export}.
*/
public class import
final static string name
final static string cf_rename_prop
final static string bulk_output_conf_key
/**
* a mapper that just writes out keyvalues.
*/
static class keyvalueimporter
extends tablemapper<immutablebyteswritable  keyvalue>
private map<byte  byte> cfrenamemap
/**
* @param row  the current table row key.
* @param value  the columns.
* @param context  the current context.
* @throws ioexception when something is broken with the data.
* @see org.apache.hadoop.mapreduce.mapper#map(keyin, valuein,
*   org.apache.hadoop.mapreduce.mapper.context)
*/
@override
public void map immutablebyteswritable row  result value
context context
throws ioexception
try
for  keyvalue kv   value raw
context write row  convertkv kv  cfrenamemap
catch  interruptedexception e
e printstacktrace
@override
public void setup context context
cfrenamemap   createcfrenamemap context getconfiguration
/**
* write table content out to files in hdfs.
*/
static class importer
extends tablemapper<immutablebyteswritable  mutation>
private map<byte  byte> cfrenamemap
/**
* @param row  the current table row key.
* @param value  the columns.
* @param context  the current context.
* @throws ioexception when something is broken with the data.
* @see org.apache.hadoop.mapreduce.mapper#map(keyin, valuein,
*   org.apache.hadoop.mapreduce.mapper.context)
*/
@override
public void map immutablebyteswritable row  result value
context context
throws ioexception
try
writeresult row  value  context
catch  interruptedexception e
e printstacktrace
private void writeresult immutablebyteswritable key  result result  context context
throws ioexception  interruptedexception
put put   null
delete delete   null
for  keyvalue kv   result raw
kv   convertkv kv  cfrenamemap
// deletes and puts are gathered and written when finished
if  kv isdelete
if  delete    null
delete   new delete key get
delete adddeletemarker kv
else
if  put    null
put   new put key get
put add kv
if  put    null
context write key  put
if  delete    null
context write key  delete
@override
public void setup context context
cfrenamemap   createcfrenamemap context getconfiguration
// helper: create a new keyvalue based on cf rename map
private static keyvalue convertkv keyvalue kv  map<byte  byte> cfrenamemap
if cfrenamemap    null
// if there's a rename mapping for this cf, create a new keyvalue
byte newcfname   cfrenamemap get kv getfamily
if newcfname    null
kv   new keyvalue kv getbuffer       row buffer
kv getrowoffset              row offset
kv getrowlength              row length
newcfname                    cf buffer
0                            cf offset
newcfname length             cf length
kv getbuffer                 qualifier buffer
kv getqualifieroffset        qualifier offset
kv getqualifierlength        qualifier length
kv gettimestamp              timestamp
keyvalue type codetotype kv gettype        kv type
kv getbuffer                 value buffer
kv getvalueoffset            value offset
kv getvaluelength            value length
return kv
// helper: make a map from sourcecfname to destcfname by parsing a config key
private static map<byte  byte> createcfrenamemap configuration conf
map<byte  byte> cfrenamemap   null
string allmappingspropval   conf get cf_rename_prop
if allmappingspropval    null
// the conf value format should be sourcecf1:destcf1,sourcecf2:destcf2,...
string allmappings   allmappingspropval split
for  string mapping  allmappings
if cfrenamemap    null
cfrenamemap   new treemap<byte byte> bytes bytes_comparator
string  srcanddest   mapping split
if srcanddest length    2
continue
cfrenamemap put srcanddest getbytes    srcanddest getbytes
return cfrenamemap
/**
* <p>sets a configuration property with key {@link #cf_rename_prop} in conf that tells
* the mapper how to rename column families.
*
* <p>alternately, instead of calling this function, you could set the configuration key
* {@link #cf_rename_prop} yourself. the value should look like
* <pre>srccf1:destcf1,srccf2:destcf2,....</pre>. this would have the same effect on
* the mapper behavior.
*
* @param conf the configuration in which the {@link #cf_rename_prop} key will be
*  set
* @param renamemap a mapping from source cf names to destination cf names
*/
static public void configurecfrenaming configuration conf
map<string  string> renamemap
stringbuilder sb   new stringbuilder
for map entry<string string> entry  renamemap entryset
string sourcecf   entry getkey
string destcf   entry getvalue
if sourcecf contains       sourcecf contains
destcf contains       destcf contains
throw new illegalargumentexception
sourcecf       destcf
if sb length      0
sb append
sb append sourcecf       destcf
conf set cf_rename_prop  sb tostring
/**
* sets up the actual job.
*
* @param conf  the current configuration.
* @param args  the command line parameters.
* @return the newly created job.
* @throws ioexception when setting up the job fails.
*/
public static job createsubmittablejob configuration conf  string args
throws ioexception
string tablename   args
path inputdir   new path args
job job   new job conf  name       tablename
job setjarbyclass importer class
fileinputformat setinputpaths job  inputdir
job setinputformatclass sequencefileinputformat class
string hfileoutpath   conf get bulk_output_conf_key
if  hfileoutpath    null
job setmapperclass keyvalueimporter class
htable table   new htable conf  tablename
job setreducerclass keyvaluesortreducer class
path outputdir   new path hfileoutpath
fileoutputformat setoutputpath job  outputdir
job setmapoutputkeyclass immutablebyteswritable class
job setmapoutputvalueclass keyvalue class
hfileoutputformat configureincrementalload job  table
tablemapreduceutil adddependencyjars job getconfiguration
com google common base preconditions class
else
// no reducers.  just write straight to table.  call inittablereducerjob
// because it sets up the tableoutputformat.
job setmapperclass importer class
tablemapreduceutil inittablereducerjob tablename  null  job
job setnumreducetasks 0
return job
/*
* @param errormsg error message.  can be null.
*/
private static void usage final string errormsg
if  errormsg    null    errormsg length   > 0
system err println     errormsg
system err println
system err println
system err println
system err println     bulk_output_conf_key
system err println
/**
* main entry point.
*
* @param args  the command line parameters.
* @throws exception when running the job fails.
*/
public static void main string args  throws exception
configuration conf   hbaseconfiguration create
string otherargs   new genericoptionsparser conf  args  getremainingargs
if  otherargs length < 2
usage     otherargs length
system exit  1
job job   createsubmittablejob conf  otherargs
system exit job waitforcompletion true  ? 0   1