/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce hadoopbackport
import java io ioexception
import java lang reflect array
import java util arraylist
import java util arrays
import org apache hadoop conf configurable
import org apache hadoop conf configuration
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop io binarycomparable
import org apache hadoop io nullwritable
import org apache hadoop io sequencefile
import org apache hadoop io rawcomparator
import org apache hadoop io writablecomparable
import org apache hadoop mapreduce job
import org apache hadoop mapreduce partitioner
import org apache hadoop util reflectionutils
/**
* partitioner effecting a total order by reading split points from
* an externally generated source.
*
* this is an identical copy of o.a.h.mapreduce.lib.partition.totalorderpartitioner
* from hadoop trunk at r910774.
*/
public class totalorderpartitioner<k extends writablecomparable<?> v>
extends partitioner<k v> implements configurable
private node partitions
public static final string default_path
public static final string partitioner_path
public static final string max_trie_depth
public static final string natural_order
configuration conf
public totalorderpartitioner
/**
* read in the partition file and build indexing data structures.
* if the keytype is {@link org.apache.hadoop.io.binarycomparable} and
* <tt>total.order.partitioner.natural.order</tt> is not false, a trie
* of the first <tt>total.order.partitioner.max.trie.depth</tt>(2) + 1 bytes
* will be built. otherwise, keys will be located using a binary search of
* the partition keyset using the {@link org.apache.hadoop.io.rawcomparator}
* defined for this job. the input file must be sorted with the same
* comparator and contain {@link job#getnumreducetasks()} - 1 keys.
*/
@suppresswarnings       keytype from conf not static
public void setconf configuration conf
try
this conf   conf
string parts   getpartitionfile conf
final path partfile   new path parts
final filesystem fs    default_path equals parts
? filesystem getlocal conf         assume in distributedcache
partfile getfilesystem conf
job job   new job conf
class<k> keyclass    class<k> job getmapoutputkeyclass
k splitpoints   readpartitions fs  partfile  keyclass  conf
if  splitpoints length    job getnumreducetasks     1
throw new ioexception
splitpoints length
rawcomparator<k> comparator
rawcomparator<k>  job getsortcomparator
for  int i   0  i < splitpoints length   1    i
if  comparator compare splitpoints  splitpoints  >  0
throw new ioexception
boolean natorder
conf getboolean natural_order  true
if  natorder    binarycomparable class isassignablefrom keyclass
partitions   buildtrie  binarycomparable splitpoints  0
splitpoints length  new byte
// now that blocks of identical splitless trie nodes are
// represented reentrantly, and we develop a leaf for any trie
// node with only one split point, the only reason for a depth
// limit is to refute stack overflow or bloat in the pathological
// case where the split points are long and mostly look like bytes
// iii...iixii...iii   .  therefore, we make the default depth
// limit large but not huge.
conf getint max_trie_depth  200
else
partitions   new binarysearchnode splitpoints  comparator
catch  ioexception e
throw new illegalargumentexception    e
public configuration getconf
return conf
// by construction, we know if our keytype
@suppresswarnings       is memcmp able and uses the trie
public int getpartition k key  v value  int numpartitions
return partitions findpartition key
/**
* set the path to the sequencefile storing the sorted partition keyset.
* it must be the case that for <tt>r</tt> reduces, there are <tt>r-1</tt>
* keys in the sequencefile.
*/
public static void setpartitionfile configuration conf  path p
conf set partitioner_path  p tostring
/**
* get the path to the sequencefile storing the sorted partition keyset.
* @see #setpartitionfile(configuration, path)
*/
public static string getpartitionfile configuration conf
return conf get partitioner_path  default_path
/**
* interface to the partitioner to locate a key in the partition keyset.
*/
interface node<t>
/**
* locate partition in keyset k, st [ki..ki+1) defines a partition,
* with implicit k0 = -inf, kn = +inf, and |k| = #partitions - 1.
*/
int findpartition t key
/**
* base class for trie nodes. if the keytype is memcomp-able, this builds
* tries of the first <tt>total.order.partitioner.max.trie.depth</tt>
* bytes.
*/
static abstract class trienode implements node<binarycomparable>
private final int level
trienode int level
this level   level
int getlevel
return level
/**
* for types that are not {@link org.apache.hadoop.io.binarycomparable} or
* where disabled by <tt>total.order.partitioner.natural.order</tt>,
* search the partition keyset with a binary search.
*/
class binarysearchnode implements node<k>
private final k splitpoints
private final rawcomparator<k> comparator
binarysearchnode k splitpoints  rawcomparator<k> comparator
this splitpoints   splitpoints
this comparator   comparator
public int findpartition k key
final int pos   arrays binarysearch splitpoints  key  comparator    1
return  pos < 0  ?  pos   pos
/**
* an inner trie node that contains 256 children based on the next
* character.
*/
class innertrienode extends trienode
private trienode child   new trienode
innertrienode int level
super level
public int findpartition binarycomparable key
int level   getlevel
if  key getlength   <  level
return child findpartition key
return child] findpartition key
/**
* @param level        the tree depth at this node
* @param splitpoints  the full split point vector, which holds
*                     the split point or points this leaf node
*                     should contain
* @param lower        first included element of splitpoints
* @param upper        first excluded element of splitpoints
* @return  a leaf node.  they come in three kinds: no split points
*          [and the findparttion returns a canned index], one split
*          point [and we compare with a single comparand], or more
*          than one [and we do a binary search].  the last case is
*          rare.
*/
private trienode leaftrienodefactory
int level  binarycomparable splitpoints  int lower  int upper
switch  upper   lower
case 0
return new unsplittrienode level  lower
case 1
return new singlysplittrienode level  splitpoints  lower
default
return new leaftrienode level  splitpoints  lower  upper
/**
* a leaf trie node that scans for the key between lower..upper.
*
* we don't generate many of these now, since we usually continue trie-ing
* when more than one split point remains at this level. and we make different
* objects for nodes with 0 or 1 split point.
*/
private class leaftrienode extends trienode
final int lower
final int upper
final binarycomparable splitpoints
leaftrienode int level  binarycomparable splitpoints  int lower  int upper
super level
this lower   lower
this upper   upper
this splitpoints   splitpoints
public int findpartition binarycomparable key
final int pos   arrays binarysearch splitpoints  lower  upper  key    1
return  pos < 0  ?  pos   pos
private class unsplittrienode extends trienode
final int result
unsplittrienode int level  int value
super level
this result   value
public int findpartition binarycomparable key
return result
private class singlysplittrienode extends trienode
final int               lower
final binarycomparable  mysplitpoint
singlysplittrienode int level  binarycomparable splitpoints  int lower
super level
this lower   lower
this mysplitpoint   splitpoints
public int findpartition binarycomparable key
return lower    key compareto mysplitpoint  < 0 ? 0   1
/**
* read the cut points from the given ifile.
* @param fs the file system
* @param p the path to read
* @param keyclass the map output key class
* @param job the job config
* @throws ioexception
*/
// matching key types enforced by passing in
@suppresswarnings       map output key class
private k readpartitions filesystem fs  path p  class<k> keyclass
configuration conf  throws ioexception
sequencefile reader reader   new sequencefile reader fs  p  conf
arraylist<k> parts   new arraylist<k>
k key   reflectionutils newinstance keyclass  conf
nullwritable value   nullwritable get
while  reader next key  value
parts add key
key   reflectionutils newinstance keyclass  conf
reader close
return parts toarray  k array newinstance keyclass  parts size
/**
*
* this object contains a trienoderef if there is such a thing that
* can be repeated.  two adjacent trie node slots that contain no
* split points can be filled with the same trie node, even if they
* are not on the same level.  see buildtreerec, below.
*
*/
private class carriedtrienoderef
trienode   content
carriedtrienoderef
content   null
/**
* given a sorted set of cut points, build a trie that will find the correct
* partition quickly.
* @param splits the list of cut points
* @param lower the lower bound of partitions 0..numpartitions-1
* @param upper the upper bound of partitions 0..numpartitions-1
* @param prefix the prefix that we have already checked against
* @param maxdepth the maximum depth we will build a trie for
* @return the trie node that will divide the splits correctly
*/
private trienode buildtrie binarycomparable splits  int lower
int upper  byte prefix  int maxdepth
return buildtrierec
splits  lower  upper  prefix  maxdepth  new carriedtrienoderef
/**
* this is the core of buildtrie.  the interface, and stub, above, just adds
* an empty carriedtrienoderef.
*
* we build trie nodes in depth first order, which is also in key space
* order.  every leaf node is referenced as a slot in a parent internal
* node.  if two adjacent slots [in the dfo] hold leaf nodes that have
* no split point, then they are not separated by a split point either,
* because there's no place in key space for that split point to exist.
*
* when that happens, the leaf nodes would be semantically identical, and
* we reuse the object.  a single carriedtrienoderef "ref" lives for the
* duration of the tree-walk.  ref carries a potentially reusable, unsplit
* leaf node for such reuse until a leaf node with a split arises, which
* breaks the chain until we need to make a new unsplit leaf node.
*
* note that this use of carriedtrienoderef means that for internal nodes,
* for internal nodes if this code is modified in any way we still need
* to make or fill in the subnodes in key space order.
*/
private trienode buildtrierec binarycomparable splits  int lower
int upper  byte prefix  int maxdepth  carriedtrienoderef ref
final int depth   prefix length
// we generate leaves for a single split point as well as for
// no split points.
if  depth >  maxdepth    lower >  upper   1
// if we have two consecutive requests for an unsplit trie node, we
// can deliver the same one the second time.
if  lower    upper    ref content    null
return ref content
trienode  result   leaftrienodefactory depth  splits  lower  upper
ref content   lower    upper ? result   null
return result
innertrienode result   new innertrienode depth
byte trial   arrays copyof prefix  prefix length   1
// append an extra byte on to the prefix
int         currentbound   lower
for int ch   0  ch < 0xff    ch
trial    byte   ch   1
lower   currentbound
while  currentbound < upper
if  splits compareto trial  0  trial length  >  0
break
currentbound    1
trial    byte  ch
result child
buildtrierec splits  lower  currentbound  trial  maxdepth  ref
// pick up the rest
trial    byte 0xff
result child
buildtrierec splits  lower  currentbound  trial  maxdepth  ref
return result