/**
* copyright 2009 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io ioexception
import java util hashmap
import org apache hadoop conf configuration
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase client put
import org apache hadoop hbase client result
import org apache hadoop hbase client scan
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase util bytes
import org apache hadoop io writable
import org apache hadoop mapreduce job
import org apache hadoop mapreduce mapper
import org apache hadoop util genericoptionsparser
/**
* example map/reduce job to construct index tables that can be used to quickly
* find a row based on the value of a column. it demonstrates:
* <ul>
* <li>using tableinputformat and tablemapreduceutil to use an htable as input
* to a map/reduce job.</li>
* <li>passing values from main method to children via the configuration.</li>
* <li>using multitableoutputformat to output to multiple tables from a
* map/reduce job.</li>
* <li>a real use case of building a secondary index over a table.</li>
* </ul>
*
* <h3>usage</h3>
*
* <p>
* modify ${hadoop_home}/conf/hadoop-env.sh to include the hbase jar, the
* zookeeper jar, the examples output directory, and the hbase conf directory in
* hadoop_classpath, and then run
* <tt><strong>bin/hadoop org.apache.hadoop.hbase.mapreduce.indexbuilder table_name column_family attr [attr ...]</strong></tt>
* </p>
*
* <p>
* to run with the sample data provided in index-builder-setup.rb, use the
* arguments <strong><tt>people attributes name email phone</tt></strong>.
* </p>
*
* <p>
* this code was written against hbase 0.21 trunk.
* </p>
*/
public class indexbuilder
/** the column family containing the indexed row key */
public static final byte index_column   bytes tobytes
/** the qualifier containing the indexed row key */
public static final byte index_qualifier   bytes tobytes
/**
* internal mapper to be run by hadoop.
*/
public static class map extends
mapper<immutablebyteswritable  result  immutablebyteswritable  writable>
private byte family
private hashmap<byte  immutablebyteswritable> indexes
@override
protected void map immutablebyteswritable rowkey  result result  context context
throws ioexception  interruptedexception
for java util map entry<byte  immutablebyteswritable> index   indexes entryset
byte qualifier   index getkey
immutablebyteswritable tablename   index getvalue
byte value   result getvalue family  qualifier
if  value    null
// original: row 123 attribute:phone 555-1212
// index: row 555-1212 index:row 123
put put   new put value
put add index_column  index_qualifier  rowkey get
context write tablename  put
@override
protected void setup context context  throws ioexception
interruptedexception
configuration configuration   context getconfiguration
string tablename   configuration get
string fields   configuration getstrings
string familyname   configuration get
family   bytes tobytes familyname
indexes   new hashmap<byte  immutablebyteswritable>
for string field   fields
// if the table is "people" and the field to index is "email", then the
// index table will be called "people-email"
indexes put bytes tobytes field
new immutablebyteswritable bytes tobytes tablename       field
/**
* job configuration.
*/
public static job configurejob configuration conf  string  args
throws ioexception
string tablename   args
string columnfamily   args
system out println     tablename
conf set tableinputformat scan  tablemapreduceutil convertscantostring new scan
conf set tableinputformat input_table  tablename
conf set    tablename
conf set    columnfamily
string fields   new string
for int i   0  i < fields length  i
fields   args
conf setstrings    fields
conf set
job job   new job conf  tablename
job setjarbyclass indexbuilder class
job setmapperclass map class
job setnumreducetasks 0
job setinputformatclass tableinputformat class
job setoutputformatclass multitableoutputformat class
return job
public static void main string args  throws exception
configuration conf   hbaseconfiguration create
string otherargs   new genericoptionsparser conf  args  getremainingargs
if otherargs length < 3
system err println     otherargs length
system err println
system exit  1
job job   configurejob conf  otherargs
system exit job waitforcompletion true  ? 0   1