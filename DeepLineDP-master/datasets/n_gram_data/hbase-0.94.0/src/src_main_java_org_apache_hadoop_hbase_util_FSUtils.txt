/**
* copyright 2010 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase util
import java io datainputstream
import java io eofexception
import java io filenotfoundexception
import java io ioexception
import java net uri
import java net urisyntaxexception
import java util arraylist
import java util hashmap
import java util list
import java util map
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs blocklocation
import org apache hadoop fs fsdatainputstream
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop fs permission fspermission
import org apache hadoop hbase hconstants
import org apache hadoop hbase hdfsblocksdistribution
import org apache hadoop hbase hregioninfo
import org apache hadoop hbase remoteexceptionhandler
import org apache hadoop hbase master hmaster
import org apache hadoop hbase regionserver hregion
import org apache hadoop hdfs distributedfilesystem
import org apache hadoop io sequencefile
import org apache hadoop util reflectionutils
import org apache hadoop util stringutils
/**
* utility methods for interacting with the underlying file system.
*/
public abstract class fsutils
private static final log log   logfactory getlog fsutils class
/** full access permissions (starting point for a umask) */
private static final string full_rwx_permissions
protected fsutils
super
public static fsutils getinstance filesystem fs  configuration conf
string scheme   fs geturi   getscheme
if  scheme    null
log warn
fs geturi
scheme
class<?> fsutilsclass   conf getclass
scheme      fshdfsutils class      default to hdfs impl
fsutils fsutils    fsutils reflectionutils newinstance fsutilsclass  conf
return fsutils
/**
* delete if exists.
* @param fs filesystem object
* @param dir directory to delete
* @return true if deleted <code>dir</code>
* @throws ioexception e
*/
public static boolean deletedirectory final filesystem fs  final path dir
throws ioexception
return fs exists dir     fs delete dir  true
/**
* check if directory exists.  if it does not, create it.
* @param fs filesystem object
* @param dir path to check
* @return path
* @throws ioexception e
*/
public path checkdir final filesystem fs  final path dir  throws ioexception
if   fs exists dir
fs mkdirs dir
return dir
/**
* create the specified file on the filesystem. by default, this will:
* <ol>
* <li>overwrite the file if it exists</li>
* <li>apply the umask in the configuration (if it is enabled)</li>
* <li>use the fs configured buffer size (or {@value default_buffer_size} if
* not set)</li>
* <li>use the default replication</li>
* <li>use the default block size</li>
* <li>not track progress</li>
* </ol>
*
* @param fs {@link filesystem} on which to write the file
* @param path {@link path} to the file to write
* @return output stream to the created file
* @throws ioexception if the file cannot be created
*/
public static fsdataoutputstream create filesystem fs  path path
fspermission perm  throws ioexception
return create fs  path  perm  true
/**
* create the specified file on the filesystem. by default, this will:
* <ol>
* <li>apply the umask in the configuration (if it is enabled)</li>
* <li>use the fs configured buffer size (or {@value default_buffer_size} if
* not set)</li>
* <li>use the default replication</li>
* <li>use the default block size</li>
* <li>not track progress</li>
* </ol>
*
* @param fs {@link filesystem} on which to write the file
* @param path {@link path} to the file to write
* @param perm
* @param overwrite whether or not the created file should be overwritten.
* @return output stream to the created file
* @throws ioexception if the file cannot be created
*/
public static fsdataoutputstream create filesystem fs  path path
fspermission perm  boolean overwrite  throws ioexception
log debug     path       perm
return fs create path  perm  overwrite
fs getconf   getint    4096
fs getdefaultreplication    fs getdefaultblocksize    null
/**
* get the file permissions specified in the configuration, if they are
* enabled.
*
* @param fs filesystem that the file will be created on.
* @param conf configuration to read for determining if permissions are
*          enabled and which to use
* @param permssionconfkey property key in the configuration to use when
*          finding the permission
* @return the permission to use when creating a new file on the fs. if
*         special permissions are not specified in the configuration, then
*         the default permissions on the the fs will be returned.
*/
public static fspermission getfilepermissions final filesystem fs
final configuration conf  final string permssionconfkey
boolean enablepermissions   conf getboolean
hconstants enable_data_file_umask  false
if  enablepermissions
try
fspermission perm   new fspermission full_rwx_permissions
// make sure that we have a mask, if not, go default.
string mask   conf get permssionconfkey
if  mask    null
return fspermission getdefault
// appy the umask
fspermission umask   new fspermission mask
return perm applyumask umask
catch  illegalargumentexception e
log warn
conf get permssionconfkey
e
return fspermission getdefault
return fspermission getdefault
/**
* checks to see if the specified file system is available
*
* @param fs filesystem
* @throws ioexception e
*/
public static void checkfilesystemavailable final filesystem fs
throws ioexception
if    fs instanceof distributedfilesystem
return
ioexception exception   null
distributedfilesystem dfs    distributedfilesystem  fs
try
if  dfs exists new path
return
catch  ioexception e
exception   remoteexceptionhandler checkioexception e
try
fs close
catch  exception e
log error    e
ioexception io   new ioexception
io initcause exception
throw io
/**
* check whether dfs is in safemode.
* @param conf
* @throws ioexception
*/
public static void checkdfssafemode final configuration conf
throws ioexception
boolean isinsafemode   false
filesystem fs   filesystem get conf
if  fs instanceof distributedfilesystem
distributedfilesystem dfs    distributedfilesystem fs
// check whether dfs is on safemode.
isinsafemode   dfs setsafemode org apache hadoop hdfs protocol fsconstants safemodeaction safemode_get
if  isinsafemode
throw new ioexception
/**
* verifies current version of file system
*
* @param fs filesystem object
* @param rootdir root hbase directory
* @return null if no version file exists, version string otherwise.
* @throws ioexception e
*/
public static string getversion filesystem fs  path rootdir
throws ioexception
path versionfile   new path rootdir  hconstants version_file_name
string version   null
if  fs exists versionfile
fsdatainputstream s
fs open versionfile
try
version   datainputstream readutf s
catch  eofexception eof
log warn
finally
s close
return version
/**
* verifies current version of file system
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @param message if true, issues a message on system.out
*
* @throws ioexception e
*/
public static void checkversion filesystem fs  path rootdir
boolean message  throws ioexception
checkversion fs  rootdir  message  0
hconstants default_version_file_write_attempts
/**
* verifies current version of file system
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @param message if true, issues a message on system.out
* @param wait wait interval
* @param retries number of times to retry
*
* @throws ioexception e
*/
public static void checkversion filesystem fs  path rootdir
boolean message  int wait  int retries  throws ioexception
string version   getversion fs  rootdir
if  version    null
if   rootregionexists fs  rootdir
// rootdir is empty (no version file and no root region)
// just create new version file (hbase-1195)
fsutils setversion fs  rootdir  wait  retries
return
else if  version compareto hconstants file_system_version     0
return
// version is deprecated require migration
// output on stdout so user sees it in terminal.
string msg
version
hconstants file_system_version
if  message
system out println     msg
throw new filesystemversionexception msg
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir
throws ioexception
setversion fs  rootdir  hconstants file_system_version  0
hconstants default_version_file_write_attempts
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root
* @param wait time to wait for retry
* @param retries number of times to retry before failing
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir  int wait  int retries
throws ioexception
setversion fs  rootdir  hconstants file_system_version  wait  retries
/**
* sets version of file system
*
* @param fs filesystem object
* @param rootdir hbase root directory
* @param version version to set
* @param wait time to wait for retry
* @param retries number of times to retry before throwing an ioexception
* @throws ioexception e
*/
public static void setversion filesystem fs  path rootdir  string version
int wait  int retries  throws ioexception
path versionfile   new path rootdir  hconstants version_file_name
while  true
try
fsdataoutputstream s   fs create versionfile
s writeutf version
log debug     rootdir tostring
version
s close
return
catch  ioexception e
if  retries > 0
log warn     rootdir tostring
e getmessage
fs delete versionfile  false
try
if  wait > 0
thread sleep wait
catch  interruptedexception ex
// ignore
retries
else
throw e
/**
* checks that a cluster id file exists in the hbase root directory
* @param fs the root directory filesystem
* @param rootdir the hbase root directory in hdfs
* @param wait how long to wait between retries
* @return <code>true</code> if the file exists, otherwise <code>false</code>
* @throws ioexception if checking the filesystem fails
*/
public static boolean checkclusteridexists filesystem fs  path rootdir
int wait  throws ioexception
while  true
try
path filepath   new path rootdir  hconstants cluster_id_file_name
return fs exists filepath
catch  ioexception ioe
if  wait > 0
log warn     rootdir tostring
wait   stringutils stringifyexception ioe
try
thread sleep wait
catch  interruptedexception ie
thread interrupted
break
else
throw ioe
return false
/**
* returns the value of the unique cluster id stored for this hbase instance.
* @param fs the root directory filesystem
* @param rootdir the path to the hbase root directory
* @return the unique cluster identifier
* @throws ioexception if reading the cluster id file fails
*/
public static string getclusterid filesystem fs  path rootdir
throws ioexception
path idpath   new path rootdir  hconstants cluster_id_file_name
string clusterid   null
if  fs exists idpath
fsdatainputstream in   fs open idpath
try
clusterid   in readutf
catch  eofexception eof
log warn   idpath tostring
finally
in close
else
log warn     idpath tostring
return clusterid
/**
* writes a new unique identifier for this cluster to the "hbase.id" file
* in the hbase root directory
* @param fs the root directory filesystem
* @param rootdir the path to the hbase root directory
* @param clusterid the unique identifier to store
* @param wait how long (in milliseconds) to wait between retries
* @throws ioexception if writing to the filesystem fails and no wait value
*/
public static void setclusterid filesystem fs  path rootdir  string clusterid
int wait  throws ioexception
while  true
try
path filepath   new path rootdir  hconstants cluster_id_file_name
fsdataoutputstream s   fs create filepath
s writeutf clusterid
s close
if  log isdebugenabled
log debug     filepath tostring
clusterid
return
catch  ioexception ioe
if  wait > 0
log warn     rootdir tostring
wait   stringutils stringifyexception ioe
try
thread sleep wait
catch  interruptedexception ie
thread interrupted
break
else
throw ioe
/**
* verifies root directory path is a valid uri with a scheme
*
* @param root root directory path
* @return passed <code>root</code> argument.
* @throws ioexception if not a valid uri with a scheme
*/
public static path validaterootpath path root  throws ioexception
try
uri rooturi   new uri root tostring
string scheme   rooturi getscheme
if  scheme    null
throw new ioexception
return root
catch  urisyntaxexception e
ioexception io   new ioexception
hconstants hbase_dir
io initcause e
throw io
/**
* if dfs, check safe mode and if so, wait until we clear it.
* @param conf configuration
* @param wait sleep between retries
* @throws ioexception e
*/
public static void waitonsafemode final configuration conf
final long wait
throws ioexception
filesystem fs   filesystem get conf
if    fs instanceof distributedfilesystem   return
distributedfilesystem dfs    distributedfilesystem fs
// make sure dfs is not in safe mode
while  dfs setsafemode org apache hadoop hdfs protocol fsconstants safemodeaction safemode_get
log info
try
thread sleep wait
catch  interruptedexception e
//continue
/**
* return the 'path' component of a path.  in hadoop, path is an uri.  this
* method returns the 'path' component of a path's uri: e.g. if a path is
* <code>hdfs://example.org:9000/hbase_trunk/testtable/compaction.dir</code>,
* this method returns <code>/hbase_trunk/testtable/compaction.dir</code>.
* this method is useful if you want to print out a path without qualifying
* filesystem instance.
* @param p filesystem path whose 'path' component we are to return.
* @return path portion of the filesystem
*/
public static string getpath path p
return p touri   getpath
/**
* @param c configuration
* @return path to hbase root directory: i.e. <code>hbase.rootdir</code> from
* configuration as a qualified path.
* @throws ioexception e
*/
public static path getrootdir final configuration c  throws ioexception
path p   new path c get hconstants hbase_dir
filesystem fs   p getfilesystem c
return p makequalified fs
/**
* checks if root region exists
*
* @param fs file system
* @param rootdir root directory of hbase installation
* @return true if exists
* @throws ioexception e
*/
public static boolean rootregionexists filesystem fs  path rootdir
throws ioexception
path rootregiondir
hregion getregiondir rootdir  hregioninfo root_regioninfo
return fs exists rootregiondir
/**
* compute hdfs blocks distribution of a given file, or a portion of the file
* @param fs file system
* @param status file status of the file
* @param start start position of the portion
* @param length length of the portion
* @return the hdfs blocks distribution
*/
static public hdfsblocksdistribution computehdfsblocksdistribution
final filesystem fs  filestatus status  long start  long length
throws ioexception
hdfsblocksdistribution blocksdistribution   new hdfsblocksdistribution
blocklocation  blocklocations
fs getfileblocklocations status  start  length
for blocklocation bl   blocklocations
string  hosts   bl gethosts
long len   bl getlength
blocksdistribution addhostsandblockweight hosts  len
return blocksdistribution
/**
* runs through the hbase rootdir and checks all stores have only
* one file in them -- that is, they've been major compacted.  looks
* at root and meta tables too.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this hbase install is major compacted.
* @throws ioexception e
*/
public static boolean ismajorcompacted final filesystem fs
final path hbaserootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table.
filestatus  tabledirs   fs liststatus hbaserootdir  new dirfilter fs
for  filestatus tabledir   tabledirs
// skip the .log directory.  all others should be tables.  inside a table,
// there are compaction.dir directories to skip.  otherwise, all else
// should be regions.  then in each region, should only be family
// directories.  under each of these, should be one file only.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
filestatus regiondirs   fs liststatus d  new dirfilter fs
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name.  now look in region for families.
filestatus familydirs   fs liststatus dd  new dirfilter fs
for  filestatus familydir   familydirs
path family   familydir getpath
// now in family make sure only one file.
filestatus familystatus   fs liststatus family
if  familystatus length > 1
log debug family tostring         familystatus length
return false
return true
// todo move this method out of fsutils. no dependencies to hmaster
/**
* returns the total overall fragmentation percentage. includes .meta. and
* -root- as well.
*
* @param master  the master defining the hbase root and file system.
* @return a map for each table and its percentage.
* @throws ioexception when scanning the directory fails.
*/
public static int gettotaltablefragmentation final hmaster master
throws ioexception
map<string  integer> map   gettablefragmentation master
return map    null    map size   > 0 ? map get       1
/**
* runs through the hbase rootdir and checks how many stores for each table
* have more than one file in them. checks -root- and .meta. too. the total
* percentage across all tables is stored under the special key "-total-".
*
* @param master  the master defining the hbase root and file system.
* @return a map for each table and its percentage.
* @throws ioexception when scanning the directory fails.
*/
public static map<string  integer> gettablefragmentation
final hmaster master
throws ioexception
path path   getrootdir master getconfiguration
// since hmaster.getfilesystem() is package private
filesystem fs   path getfilesystem master getconfiguration
return gettablefragmentation fs  path
/**
* runs through the hbase rootdir and checks how many stores for each table
* have more than one file in them. checks -root- and .meta. too. the total
* percentage across all tables is stored under the special key "-total-".
*
* @param fs  the file system to use.
* @param hbaserootdir  the root directory to scan.
* @return a map for each table and its percentage.
* @throws ioexception when scanning the directory fails.
*/
public static map<string  integer> gettablefragmentation
final filesystem fs  final path hbaserootdir
throws ioexception
map<string  integer> frags   new hashmap<string  integer>
int cfcounttotal   0
int cffragtotal   0
dirfilter df   new dirfilter fs
// presumes any directory under hbase.rootdir is a table
filestatus  tabledirs   fs liststatus hbaserootdir  df
for  filestatus tabledir   tabledirs
// skip the .log directory.  all others should be tables.  inside a table,
// there are compaction.dir directories to skip.  otherwise, all else
// should be regions.  then in each region, should only be family
// directories.  under each of these, should be one file only.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
int cfcount   0
int cffrag   0
filestatus regiondirs   fs liststatus d  df
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name, now look in region for families
filestatus familydirs   fs liststatus dd  df
for  filestatus familydir   familydirs
cfcount
cfcounttotal
path family   familydir getpath
// now in family make sure only one file
filestatus familystatus   fs liststatus family
if  familystatus length > 1
cffrag
cffragtotal
// compute percentage per table and store in result list
frags put d getname    math round  float  cffrag   cfcount   100
// set overall percentage for all tables
frags put    math round  float  cffragtotal   cfcounttotal   100
return frags
/**
* expects to find -root- directory.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this a pre020 layout.
* @throws ioexception e
*/
public static boolean ispre020filelayout final filesystem fs
final path hbaserootdir
throws ioexception
path mapfiles   new path new path new path new path hbaserootdir
return fs exists mapfiles
/**
* runs through the hbase rootdir and checks all stores have only
* one file in them -- that is, they've been major compacted.  looks
* at root and meta tables too.  this version differs from
* {@link #ismajorcompacted(filesystem, path)} in that it expects a
* pre-0.20.0 hbase layout on the filesystem.  used migrating.
* @param fs filesystem
* @param hbaserootdir hbase root directory
* @return true if this hbase install is major compacted.
* @throws ioexception e
*/
public static boolean ismajorcompactedpre020 final filesystem fs
final path hbaserootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table.
filestatus  tabledirs   fs liststatus hbaserootdir  new dirfilter fs
for  filestatus tabledir   tabledirs
// inside a table, there are compaction.dir directories to skip.
// otherwise, all else should be regions.  then in each region, should
// only be family directories.  under each of these, should be a mapfile
// and info directory and in these only one file.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
filestatus regiondirs   fs liststatus d  new dirfilter fs
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name.  now look in region for families.
filestatus familydirs   fs liststatus dd  new dirfilter fs
for  filestatus familydir   familydirs
path family   familydir getpath
filestatus infoandmapfile   fs liststatus family
// assert that only info and mapfile in family dir.
if  infoandmapfile length    0    infoandmapfile length    2
log debug family tostring
infoandmapfile length
return false
// make sure directory named info or mapfile.
for  int ll   0  ll < 2  ll
if  infoandmapfile getpath   getname   equals
infoandmapfile getpath   getname   equals
continue
log debug
infoandmapfile getpath
return false
// now in family, there are 'mapfile' and 'info' subdirs.  just
// look in the 'mapfile' subdir.
filestatus familystatus
fs liststatus new path family
if  familystatus length > 1
log debug family tostring         familystatus length
return false
return true
/**
* a {@link pathfilter} that returns directories.
*/
public static class dirfilter implements pathfilter
private final filesystem fs
public dirfilter final filesystem fs
this fs   fs
public boolean accept path p
boolean isvalid   false
try
if  hconstants hbase_non_user_table_dirs contains p
isvalid   false
else
isvalid   this fs getfilestatus p  isdir
catch  ioexception e
e printstacktrace
return isvalid
/**
* heuristic to determine whether is safe or not to open a file for append
* looks both for dfs.support.append and use reflection to search
* for sequencefile.writer.syncfs() or fsdataoutputstream.hflush()
* @param conf
* @return true if append support
*/
public static boolean isappendsupported final configuration conf
boolean append   conf getboolean    false
if  append
try
// todo: the implementation that comes back when we do a createwriter
// may not be using sequencefile so the below is not a definitive test.
// will do for now (hdfs-200).
sequencefile writer class getmethod    new class<?>
append   true
catch  securityexception e
catch  nosuchmethodexception e
append   false
if   append
// look for the 0.21, 0.22, new-style append evidence.
try
fsdataoutputstream class getmethod    new class<?>
append   true
catch  nosuchmethodexception e
append   false
return append
/**
* @param conf
* @return true if this filesystem whose scheme is 'hdfs'.
* @throws ioexception
*/
public static boolean ishdfs final configuration conf  throws ioexception
filesystem fs   filesystem get conf
string scheme   fs geturi   getscheme
return scheme equalsignorecase
/**
* recover file lease. used when a file might be suspect
* to be had been left open by another process.
* @param fs filesystem handle
* @param p path of file to recover lease
* @param conf configuration handle
* @throws ioexception
*/
public abstract void recoverfilelease final filesystem fs  final path p
configuration conf  throws ioexception
/**
* @param fs
* @param rootdir
* @return all the table directories under <code>rootdir</code>. ignore non table hbase folders such as
* .logs, .oldlogs, .corrupt, .meta., and -root- folders.
* @throws ioexception
*/
public static list<path> gettabledirs final filesystem fs  final path rootdir
throws ioexception
// presumes any directory under hbase.rootdir is a table
filestatus  dirs   fs liststatus rootdir  new dirfilter fs
list<path> tabledirs   new arraylist<path> dirs length
for  filestatus dir  dirs
path p   dir getpath
string tablename   p getname
if   hconstants hbase_non_user_table_dirs contains tablename
tabledirs add p
return tabledirs
public static path gettablepath path rootdir  byte  tablename
return gettablepath rootdir  bytes tostring tablename
public static path gettablepath path rootdir  final string tablename
return new path rootdir  tablename
/**
* @param conf
* @return returns the filesystem of the hbase rootdir.
* @throws ioexception
*/
public static filesystem getcurrentfilesystem configuration conf
throws ioexception
return getrootdir conf  getfilesystem conf
/**
* runs through the hbase rootdir and creates a reverse lookup map for
* table storefile names to the full path.
* <br>
* example...<br>
* key = 3944417774205889744  <br>
* value = hdfs://localhost:51169/user/userid/-root-/70236052/info/3944417774205889744
*
* @param fs  the file system to use.
* @param hbaserootdir  the root directory to scan.
* @return map keyed by storefile name with a value of the full path.
* @throws ioexception when scanning the directory fails.
*/
public static map<string  path> gettablestorefilepathmap
final filesystem fs  final path hbaserootdir
throws ioexception
map<string  path> map   new hashmap<string  path>
// if this method looks similar to 'gettablefragmentation' that is because
// it was borrowed from it.
dirfilter df   new dirfilter fs
// presumes any directory under hbase.rootdir is a table
filestatus  tabledirs   fs liststatus hbaserootdir  df
for  filestatus tabledir   tabledirs
// skip the .log directory.  all others should be tables.  inside a table,
// there are compaction.dir directories to skip.  otherwise, all else
// should be regions.
path d   tabledir getpath
if  d getname   equals hconstants hregion_logdir_name
continue
filestatus regiondirs   fs liststatus d  df
for  filestatus regiondir   regiondirs
path dd   regiondir getpath
if  dd getname   equals hconstants hregion_compactiondir_name
continue
// else its a region name, now look in region for families
filestatus familydirs   fs liststatus dd  df
for  filestatus familydir   familydirs
path family   familydir getpath
// now in family, iterate over the storefiles and
// put in map
filestatus familystatus   fs liststatus family
for  filestatus sfstatus   familystatus
path sf   sfstatus getpath
map put  sf getname    sf
return map
/**
* calls fs.liststatus() and treats filenotfoundexception as non-fatal
* this would accommodate difference in various hadoop versions
*
* @param fs file system
* @param dir directory
* @param filter path filter
* @return null if tabledir doesn't exist, otherwise filestatus array
*/
public static filestatus  liststatus final filesystem fs
final path dir  final pathfilter filter  throws ioexception
filestatus  status   null
try
status   filter    null ? fs liststatus dir    fs liststatus dir  filter
catch  filenotfoundexception fnfe
// if directory doesn't exist, return null
log info dir
if  status    null    status length < 1  return null
return status
/**
* calls fs.delete() and returns the value returned by the fs.delete()
*
* @param fs
* @param path
* @param recursive
* @return
* @throws ioexception
*/
public static boolean delete final filesystem fs  final path path  final boolean recursive
throws ioexception
return fs delete path  recursive
/**
* calls fs.exists(). checks if the specified path exists
*
* @param fs
* @param path
* @return
* @throws ioexception
*/
public static boolean isexists final filesystem fs  final path path  throws ioexception
return fs exists path