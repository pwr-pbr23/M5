/**
* copyright 2008 the apache software foundation
*
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hbase mapreduce
import java io bytearrayinputstream
import java io bytearrayoutputstream
import java io datainputstream
import java io dataoutputstream
import java io ioexception
import java lang reflect method
import java lang reflect invocationtargetexception
import java net url
import java net urldecoder
import java util enumeration
import java util hashset
import java util set
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hbase hconstants
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase client htable
import org apache hadoop hbase client scan
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase security user
import org apache hadoop hbase util base64
import org apache hadoop hbase util bytes
import org apache hadoop hbase zookeeper zkutil
import org apache hadoop io text
import org apache hadoop io writable
import org apache hadoop io writablecomparable
import org apache hadoop mapreduce inputformat
import org apache hadoop mapreduce job
import org apache hadoop util stringutils
/**
* utility for {@link tablemapper} and {@link tablereducer}
*/
@suppresswarnings
public class tablemapreduceutil
static log log   logfactory getlog tablemapreduceutil class
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table  the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob string table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
throws ioexception
inittablemapperjob table  scan  mapper  outputkeyclass  outputvalueclass
job  true
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table binary representation of the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob byte table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
throws ioexception
inittablemapperjob bytes tostring table   scan  mapper  outputkeyclass  outputvalueclass
job  true
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table  the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param adddependencyjars upload hbase jars and jars for any of the configured
*           job classes via the distributed cache (tmpjars).
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob string table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
boolean adddependencyjars  class<? extends inputformat> inputformatclass
throws ioexception
job setinputformatclass inputformatclass
if  outputvalueclass    null  job setmapoutputvalueclass outputvalueclass
if  outputkeyclass    null  job setmapoutputkeyclass outputkeyclass
job setmapperclass mapper
configuration conf   job getconfiguration
hbaseconfiguration merge conf  hbaseconfiguration create conf
conf set tableinputformat input_table  table
conf set tableinputformat scan  convertscantostring scan
if  adddependencyjars
adddependencyjars job
initcredentials job
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table binary representation of the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param adddependencyjars upload hbase jars and jars for any of the configured
*           job classes via the distributed cache (tmpjars).
* @param inputformatclass the class of the input format
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob byte table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
boolean adddependencyjars  class<? extends inputformat> inputformatclass
throws ioexception
inittablemapperjob bytes tostring table   scan  mapper  outputkeyclass
outputvalueclass  job  adddependencyjars  inputformatclass
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table binary representation of the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param adddependencyjars upload hbase jars and jars for any of the configured
*           job classes via the distributed cache (tmpjars).
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob byte table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
boolean adddependencyjars
throws ioexception
inittablemapperjob bytes tostring table   scan  mapper  outputkeyclass
outputvalueclass  job  adddependencyjars  tableinputformat class
/**
* use this before submitting a tablemap job. it will appropriately set up
* the job.
*
* @param table the table name to read from.
* @param scan  the scan instance with the columns, time range etc.
* @param mapper  the mapper class to use.
* @param outputkeyclass  the class of the output key.
* @param outputvalueclass  the class of the output value.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param adddependencyjars upload hbase jars and jars for any of the configured
*           job classes via the distributed cache (tmpjars).
* @throws ioexception when setting up the details fails.
*/
public static void inittablemapperjob string table  scan scan
class<? extends tablemapper> mapper
class<? extends writablecomparable> outputkeyclass
class<? extends writable> outputvalueclass  job job
boolean adddependencyjars
throws ioexception
inittablemapperjob table  scan  mapper  outputkeyclass
outputvalueclass  job  adddependencyjars  tableinputformat class
public static void initcredentials job job  throws ioexception
if  user ishbasesecurityenabled job getconfiguration
try
user getcurrent   obtainauthtokenforjob job getconfiguration    job
catch  interruptedexception ie
log info
thread interrupted
/**
* writes the given scan into a base64 encoded string.
*
* @param scan  the scan to write out.
* @return the scan saved in a base64 encoded string.
* @throws ioexception when writing the scan fails.
*/
static string convertscantostring scan scan  throws ioexception
bytearrayoutputstream out   new bytearrayoutputstream
dataoutputstream dos   new dataoutputstream out
scan write dos
return base64 encodebytes out tobytearray
/**
* converts the given base64 string back into a scan instance.
*
* @param base64  the scan details.
* @return the newly created scan instance.
* @throws ioexception when reading the scan instance fails.
*/
static scan convertstringtoscan string base64  throws ioexception
bytearrayinputstream bis   new bytearrayinputstream base64 decode base64
datainputstream dis   new datainputstream bis
scan scan   new scan
scan readfields dis
return scan
/**
* use this before submitting a tablereduce job. it will
* appropriately set up the jobconf.
*
* @param table  the output table.
* @param reducer  the reducer class to use.
* @param job  the current job to adjust.
* @throws ioexception when determining the region count fails.
*/
public static void inittablereducerjob string table
class<? extends tablereducer> reducer  job job
throws ioexception
inittablereducerjob table  reducer  job  null
/**
* use this before submitting a tablereduce job. it will
* appropriately set up the jobconf.
*
* @param table  the output table.
* @param reducer  the reducer class to use.
* @param job  the current job to adjust.
* @param partitioner  partitioner to use. pass <code>null</code> to use
* default partitioner.
* @throws ioexception when determining the region count fails.
*/
public static void inittablereducerjob string table
class<? extends tablereducer> reducer  job job
class partitioner  throws ioexception
inittablereducerjob table  reducer  job  partitioner  null  null  null
/**
* use this before submitting a tablereduce job. it will
* appropriately set up the jobconf.
*
* @param table  the output table.
* @param reducer  the reducer class to use.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param partitioner  partitioner to use. pass <code>null</code> to use
* default partitioner.
* @param quorumaddress distant cluster to write to; default is null for
* output to the cluster that is designated in <code>hbase-site.xml</code>.
* set this string to the zookeeper ensemble of an alternate remote cluster
* when you would have the reduce write a cluster that is other than the
* default; e.g. copying tables between clusters, the source would be
* designated by <code>hbase-site.xml</code> and this param would have the
* ensemble address of the remote cluster.  the format to pass is particular.
* pass <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
* </code> such as <code>server,server2,server3:2181:/hbase</code>.
* @param serverclass redefined hbase.regionserver.class
* @param serverimpl redefined hbase.regionserver.impl
* @throws ioexception when determining the region count fails.
*/
public static void inittablereducerjob string table
class<? extends tablereducer> reducer  job job
class partitioner  string quorumaddress  string serverclass
string serverimpl  throws ioexception
inittablereducerjob table  reducer  job  partitioner  quorumaddress
serverclass  serverimpl  true
/**
* use this before submitting a tablereduce job. it will
* appropriately set up the jobconf.
*
* @param table  the output table.
* @param reducer  the reducer class to use.
* @param job  the current job to adjust.  make sure the passed job is
* carrying all necessary hbase configuration.
* @param partitioner  partitioner to use. pass <code>null</code> to use
* default partitioner.
* @param quorumaddress distant cluster to write to; default is null for
* output to the cluster that is designated in <code>hbase-site.xml</code>.
* set this string to the zookeeper ensemble of an alternate remote cluster
* when you would have the reduce write a cluster that is other than the
* default; e.g. copying tables between clusters, the source would be
* designated by <code>hbase-site.xml</code> and this param would have the
* ensemble address of the remote cluster.  the format to pass is particular.
* pass <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
* </code> such as <code>server,server2,server3:2181:/hbase</code>.
* @param serverclass redefined hbase.regionserver.class
* @param serverimpl redefined hbase.regionserver.impl
* @param adddependencyjars upload hbase jars and jars for any of the configured
*           job classes via the distributed cache (tmpjars).
* @throws ioexception when determining the region count fails.
*/
public static void inittablereducerjob string table
class<? extends tablereducer> reducer  job job
class partitioner  string quorumaddress  string serverclass
string serverimpl  boolean adddependencyjars  throws ioexception
configuration conf   job getconfiguration
hbaseconfiguration merge conf  hbaseconfiguration create conf
job setoutputformatclass tableoutputformat class
if  reducer    null  job setreducerclass reducer
conf set tableoutputformat output_table  table
// if passed a quorum/ensemble address, pass it on to tableoutputformat.
if  quorumaddress    null
// calling this will validate the format
zkutil transformclusterkey quorumaddress
conf set tableoutputformat quorum_address quorumaddress
if  serverclass    null    serverimpl    null
conf set tableoutputformat region_server_class  serverclass
conf set tableoutputformat region_server_impl  serverimpl
job setoutputkeyclass immutablebyteswritable class
job setoutputvalueclass writable class
if  partitioner    hregionpartitioner class
job setpartitionerclass hregionpartitioner class
htable outputtable   new htable conf  table
int regions   outputtable getregionsinfo   size
if  job getnumreducetasks   > regions
job setnumreducetasks outputtable getregionsinfo   size
else if  partitioner    null
job setpartitionerclass partitioner
if  adddependencyjars
adddependencyjars job
initcredentials job
/**
* ensures that the given number of reduce tasks for the given job
* configuration does not exceed the number of regions for the given table.
*
* @param table  the table to get the region count for.
* @param job  the current job to adjust.
* @throws ioexception when retrieving the table details fails.
*/
public static void limitnumreducetasks string table  job job
throws ioexception
htable outputtable   new htable job getconfiguration    table
int regions   outputtable getregionsinfo   size
if  job getnumreducetasks   > regions
job setnumreducetasks regions
/**
* sets the number of reduce tasks for the given job configuration to the
* number of regions the given table has.
*
* @param table  the table to get the region count for.
* @param job  the current job to adjust.
* @throws ioexception when retrieving the table details fails.
*/
public static void setnumreducetasks string table  job job
throws ioexception
htable outputtable   new htable job getconfiguration    table
int regions   outputtable getregionsinfo   size
job setnumreducetasks regions
/**
* sets the number of rows to return and cache with each scanner iteration.
* higher caching values will enable faster mapreduce jobs at the expense of
* requiring more heap to contain the cached rows.
*
* @param job the current job to adjust.
* @param batchsize the number of rows to return in batch with each scanner
* iteration.
*/
public static void setscannercaching job job  int batchsize
job getconfiguration   setint    batchsize
/**
* add the hbase dependency jars as well as jars for any of the configured
* job classes to the job configuration, so that jobclient will ship them
* to the cluster and add them to the distributedcache.
*/
public static void adddependencyjars job job  throws ioexception
try
adddependencyjars job getconfiguration
org apache zookeeper zookeeper class
com google protobuf message class
job getmapoutputkeyclass
job getmapoutputvalueclass
job getinputformatclass
job getoutputkeyclass
job getoutputvalueclass
job getoutputformatclass
job getpartitionerclass
job getcombinerclass
catch  classnotfoundexception e
throw new ioexception e
/**
* add the jars containing the given classes to the job's configuration
* such that jobclient will ship them to the cluster and add them to
* the distributedcache.
*/
public static void adddependencyjars configuration conf
class    classes  throws ioexception
filesystem localfs   filesystem getlocal conf
set<string> jars   new hashset<string>
// add jars that are already in the tmpjars variable
jars addall  conf getstringcollection
// add jars containing the specified classes
for  class clazz   classes
if  clazz    null  continue
string pathstr   findorcreatejar clazz
if  pathstr    null
log warn     clazz
continue
path path   new path pathstr
if   localfs exists path
log warn     path
clazz
continue
jars add path makequalified localfs  tostring
if  jars isempty    return
conf set
stringutils arraytostring jars toarray new string
/**
* if org.apache.hadoop.util.jarfinder is available (0.23+ hadoop),
* finds the jar for a class or creates it if it doesn't exist. if
* the class is in a directory in the classpath, it creates a jar
* on the fly with the contents of the directory and returns the path
* to that jar. if a jar is created, it is created in
* the system temporary directory.
*
* otherwise, returns an existing jar that contains a class of the
* same name.
*
* @param my_class the class to find.
* @return a jar file that contains the class, or null.
* @throws ioexception
*/
private static string findorcreatejar class my_class
throws ioexception
try
class<?> jarfinder   class forname
// hadoop-0.23 has a jarfinder class that will create the jar
// if it doesn't exist.  note that this is needed to run the mapreduce
// unit tests post-0.23, because mapreduce v2 requires the relevant jars
// to be in the mr cluster to do output, split, etc.  at unit test time,
// the hbase jars do not exist, so we need to create some.  note that we
// can safely fall back to findcontainingjars for pre-0.23 mapreduce.
method m   jarfinder getmethod    class class
return  string m invoke null my_class
catch  invocationtargetexception ite
// function was properly called, but threw it's own exception
throw new ioexception ite getcause
catch  exception e
// ignore all other exceptions. related to reflection failure
log debug
return findcontainingjar my_class
/**
* find a jar that contains a class of the same name, if any.
* it will return a jar file, even if that is not the first thing
* on the class path that has a class with the same name.
*
* this is shamelessly copied from jobconf
*
* @param my_class the class to find.
* @return a jar file that contains the class, or null.
* @throws ioexception
*/
private static string findcontainingjar class my_class
classloader loader   my_class getclassloader
string class_file   my_class getname   replaceall
try
for enumeration itr   loader getresources class_file
itr hasmoreelements
url url    url  itr nextelement
if    equals url getprotocol
string toreturn   url getpath
if  toreturn startswith
toreturn   toreturn substring   length
// urldecoder is a misnamed class, since it actually decodes
// x-www-form-urlencoded mime type rather than actual
// url encoding (which the file path has). therefore it would
// decode +s to ' 's which is incorrect (spaces are actually
// either unencoded or encoded as "%20"). replace +s first, so
// that they are kept sacred during the decoding process.
toreturn   toreturn replaceall
toreturn   urldecoder decode toreturn
return toreturn replaceall
catch  ioexception e
throw new runtimeexception e
return null