/*
derby - class org.apache.derby.iapi.store.access.conglomerate.scanmanager
licensed to the apache software foundation (asf) under one or more
contributor license agreements.  see the notice file distributed with
this work for additional information regarding copyright ownership.
the asf licenses this file to you under the apache license, version 2.0
(the "license"); you may not use this file except in compliance with
the license.  you may obtain a copy of the license at
http://www.apache.org/licenses/license-2.0
unless required by applicable law or agreed to in writing, software
distributed under the license is distributed on an "as is" basis,
without warranties or conditions of any kind, either express or implied.
see the license for the specific language governing permissions and
limitations under the license.
*/
package org apache derby iapi store access conglomerate
import org apache derby iapi store access groupfetchscancontroller
import org apache derby iapi store access scancontroller
import org apache derby iapi store raw page
import org apache derby iapi error standardexception
import org apache derby iapi store access backingstorehashtable
/**
the scanmanager interface contains those methods private to access method
implementors necessary to implement scans on conglomerates.  client of scans
use the scancontroller to interact with the scan.
<p>
@see scancontroller
**/
public interface scanmanager extends scancontroller  groupfetchscancontroller
/**
* close scan as part of terminating a transaction.
* <p>
* use this call to close the scan resources as part of committing or
* aborting a transaction.  the normal close() routine may do some cleanup
* that is either unnecessary, or not correct due to the unknown condition
* of the scan following a transaction ending error.  use this call when
* closing all scans as part of an abort of a transaction.
*
* @param closeheldscan     if true, means to close scan even if it has been
*                          opened to be kept opened across commit.  this is
*                          used to close these scans on abort.
*
* @return boolean indicating that the close has resulted in a real close
*                 of the scan.  a held scan will return false if called
*                 by closeforendtransaction(false), otherwise it will
*                 return true.  a non-held scan will always return true.
*
* @exception  standardexception  standard exception policy.
**/
boolean closeforendtransaction boolean closeheldscan
throws standardexception
/**
* insert all rows that qualify for the current scan into the input
* hash table.
* <p>
* this routine scans executes the entire scan as described in the
* openscan call.  for every qualifying unique row value an entry is
* placed into the hashtable. for unique row values the entry in the
* hashtable has a key value of the object stored in
* row[key_column_number], and the value of the data is row.  for row
* values with duplicates, the key value is also row[key_column_number],
* but the value of the data is a vector of
* rows.  the caller will have to call "instanceof" on the data value
* object if duplicates are expected, to determine if the data value
* of the hashtable entry is a row or is a vector of rows.
* <p>
* note, that for this routine to work efficiently the caller must
* ensure that the object in row[key_column_number] implements
* the hashcode and equals method as appropriate for it's datatype.
* <p>
* it is expected that this call will be the first and only call made in
* an openscan.  qualifiers and stop position of the openscan are applied
* just as in a normal scan.  this call is logically equivalent to the
* caller performing the following:
*
* import java.util.hashtable;
*
* hash_table = new hashtable();
*
* while (next())
* {
*     row = create_new_row();
*     fetch(row);
*     if ((duplicate_value =
*         hash_table.put(row[key_column_number], row)) != null)
*     {
*         vector row_vec;
*
*         // inserted a duplicate
*         if ((duplicate_value instanceof vector))
*         {
*             row_vec = (vector) duplicate_value;
*         }
*         else
*         {
*             // allocate vector to hold duplicates
*             row_vec = new vector(2);
*
*             // insert original row into vector
*             row_vec.addelement(duplicate_value);
*
*             // put the vector as the data rather than the row
*             hash_table.put(row[key_column_number], row_vec);
*         }
*
*         // insert new row into vector
*         row_vec.addelement(row);
*     }
* }
* <p>
* the columns of the row will be the standard columns returned as
* part of a scan, as described by the validcolumns - see openscan for
* description.
* resolve - is this ok?  or should i hard code somehow the row to
*           be the first column and the row location?
* <p>
* no overflow to external storage is provided, so calling this routine
* on a 1 gigabyte conglomerate will incur at least 1 gigabyte of memory
* (probably failing with a java out of memory condition).  if this
* routine gets an out of memory condition, or if "max_rowcnt" is
* exceeded then then the routine will give up, empty the hashtable,
* and return "false."
* <p>
* on exit from this routine, whether the fetchset() succeeded or not
* the scan is complete, it is positioned just the same as if the scan
* had been drained by calling "next()" until it returns false (ie.
* fetchnext() and next() calls will return false).
* reopenscan() can be called to restart the scan.
* <p>
*
* resolve - until we get row counts what should we do for sizing the
*           the size, capasity, and load factor of the hash table.
*           for now it is up to the caller to create the hashtable,
*           access does not reset any parameters.
* <p>
* resolve - i am not sure if access should be in charge of allocating
*           the new row objects.  i know that i can do this in the
*           case of btree's, but i don't think i can do this in heaps.
*           maybe this is solved by work to be done on the sort
*           interface.
*
*
* @param max_rowcnt        the maximum number of rows to insert into the
*                          hash table.  pass in -1 if there is no maximum.
* @param key_column_numbers the column numbers of the columns in the
*                          scan result row to be the key to the hashtable.
*                          "0" is the first column in the scan result
*                          row (which may be different than the first
*                          row in the table of the scan).
*
* @exception  standardexception  standard exception policy.
**/
void fetchset
long                    max_rowcnt
int                   key_column_numbers
backingstorehashtable   hash_table
throws standardexception
/**
* do work necessary to maintain the current position in the scan.
* <p>
* the latched page in the conglomerate "congomid" is changing, do
* whatever is necessary to maintain the current position of the scan.
* for some conglomerates this may be a no-op.
* <p>
*
* @param conglom   conglomerate object of the conglomerate being changed.
* @param page      page in the conglomerate being changed.
*
* @exception  standardexception  standard exception policy.
**/
public void saveposition conglomerate conglom  page page
throws standardexception