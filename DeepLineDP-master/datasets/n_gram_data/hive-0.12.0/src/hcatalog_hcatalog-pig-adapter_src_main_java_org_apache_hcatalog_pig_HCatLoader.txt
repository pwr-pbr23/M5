/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing,
* software distributed under the license is distributed on an
* "as is" basis, without warranties or conditions of any
* kind, either express or implied.  see the license for the
* specific language governing permissions and limitations
* under the license.
*/
package org apache hcatalog pig
import java io ioexception
import java util enumeration
import java util hashmap
import java util list
import java util map
import java util map entry
import java util properties
import org apache hadoop fs path
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive ql metadata table
import org apache hadoop mapreduce inputformat
import org apache hadoop mapreduce job
import org apache hadoop security credentials
import org apache hcatalog common hcatconstants
import org apache hcatalog common hcatcontext
import org apache hcatalog common hcatutil
import org apache hcatalog data pair
import org apache hcatalog data schema hcatschema
import org apache hcatalog mapreduce hcatinputformat
import org apache hcatalog mapreduce inputjobinfo
import org apache pig expression
import org apache pig expression binaryexpression
import org apache pig pigexception
import org apache pig resourceschema
import org apache pig resourcestatistics
import org apache pig impl util udfcontext
/**
* pig {@link org.apache.pig.loadfunc} to read data from hcat
* @deprecated use/modify {@link org.apache.hive.hcatalog.pig.hcatloader} instead
*/
public class hcatloader extends hcatbaseloader
private static final string partition_filter         for future use
private hcatinputformat hcatinputformat   null
private string dbname
private string tablename
private string hcatserveruri
private string partitionfilterstring
private final pighcatutil phutil   new pighcatutil
// signature for wrapped loader, see comments in loadfuncbasedinputdriver.initialize
final public static string inner_signature
final public static string inner_signature_prefix
// a hash map which stores job credentials. the key is a signature passed by pig, which is
//unique to the load func and input file name (table, in our case).
private static map<string  credentials> jobcredentials   new hashmap<string  credentials>
@override
public inputformat<?  ?> getinputformat   throws ioexception
if  hcatinputformat    null
hcatinputformat   new hcatinputformat
return hcatinputformat
@override
public string relativetoabsolutepath string location  path curdir  throws ioexception
return location
@override
public void setlocation string location  job job  throws ioexception
hcatcontext instance setconf job getconfiguration    getconf   get
setboolean hcatconstants hcat_data_tiny_small_int_promotion  true
udfcontext udfcontext   udfcontext getudfcontext
properties udfprops   udfcontext getudfproperties this getclass
new string signature
job getconfiguration   set inner_signature  inner_signature_prefix       signature
pair<string  string> dbtablepair   pighcatutil getdbtablenames location
dbname   dbtablepair first
tablename   dbtablepair second
requiredfieldlist requiredfieldsinfo    requiredfieldlist  udfprops
get prune_projection_info
// get partitionfilterstring stored in the udfcontext - it would have
// been stored there by an earlier call to setpartitionfilter
// call setinput on hcatinputformat only in the frontend because internally
// it makes calls to the hcat server - we don't want these to happen in
// the backend
// in the hadoop front end mapred.task.id property will not be set in
// the configuration
if  udfprops containskey hcatconstants hcat_pig_loader_location_set
for  enumeration<object> emr   udfprops keys    emr hasmoreelements
pighcatutil getconfigfromudfproperties udfprops
job getconfiguration    emr nextelement   tostring
if   hcatutil checkjobcontextifrunningfrombackend job
//combine credentials and credentials from job takes precedence for freshness
credentials crd   jobcredentials get inner_signature_prefix       signature
crd addall job getcredentials
job getcredentials   addall crd
else
job clone   new job job getconfiguration
hcatinputformat setinput job  dbname  tablename  setfilter getpartitionfilterstring
// we will store all the new /changed properties in the job in the
// udf context, so the the hcatinputformat.setinput method need not
//be called many times.
for  entry<string  string> keyvalue   job getconfiguration
string oldvalue   clone getconfiguration   getraw keyvalue getkey
if   oldvalue    null      keyvalue getvalue   equals oldvalue     false
udfprops put keyvalue getkey    keyvalue getvalue
udfprops put hcatconstants hcat_pig_loader_location_set  true
//store credentials in a private hash map and not the udf context to
// make sure they are not public.
credentials crd   new credentials
crd addall job getcredentials
jobcredentials put inner_signature_prefix       signature  crd
// need to also push projections by calling setoutputschema on
// hcatinputformat - we have to get the requiredfields information
// from the udfcontext, translate it to an schema and then pass it
// the reason we do this here is because setlocation() is called by
// pig runtime at inputformat.getsplits() and
// inputformat.createrecordreader() time - we are not sure when
// hcatinputformat needs to know about pruned projections - so doing it
// here will ensure we communicate to hcatinputformat about pruned
// projections at getsplits() and createrecordreader() time
if  requiredfieldsinfo    null
// convert to hcatschema and pass to hcatinputformat
try
outputschema   phutil gethcatschema requiredfieldsinfo getfields    signature  this getclass
hcatinputformat setoutputschema job  outputschema
catch  exception e
throw new ioexception e
else
// else - this means pig's optimizer never invoked the pushprojection
// method - so we need all fields and hence we should not call the
// setoutputschema on hcatinputformat
if  hcatutil checkjobcontextifrunningfrombackend job
try
hcatschema hcattableschema    hcatschema  udfprops get hcatconstants hcat_table_schema
outputschema   hcattableschema
hcatinputformat setoutputschema job  outputschema
catch  exception e
throw new ioexception e
@override
public string getpartitionkeys string location  job job
throws ioexception
table table   phutil gettable location
hcatserveruri    null ? hcatserveruri   pighcatutil gethcatserveruri job
pighcatutil gethcatserverprincipal job
list<fieldschema> tablepartitionkeys   table getpartitionkeys
string partitionkeys   new string
for  int i   0  i < tablepartitionkeys size    i
partitionkeys   tablepartitionkeys get i  getname
return partitionkeys
@override
public resourceschema getschema string location  job job  throws ioexception
hcatcontext instance setconf job getconfiguration    getconf   get
setboolean hcatconstants hcat_data_tiny_small_int_promotion  true
table table   phutil gettable location
hcatserveruri    null ? hcatserveruri   pighcatutil gethcatserveruri job
pighcatutil gethcatserverprincipal job
hcatschema hcattableschema   hcatutil gettableschemawithptncols table
try
pighcatutil validatehcattableschemafollowspigrules hcattableschema
catch  ioexception e
throw new pigexception
e getmessage
hcattableschema tostring
pighcatutil pig_exception_code  e
storeinudfcontext signature  hcatconstants hcat_table_schema  hcattableschema
outputschema   hcattableschema
return pighcatutil getresourceschema hcattableschema
@override
public void setpartitionfilter expression partitionfilter  throws ioexception
// convert the partition filter expression into a string expected by
// hcat and pass it in setlocation()
partitionfilterstring   gethcatcomparisonstring partitionfilter
// store this in the udf context so we can get it later
storeinudfcontext signature
partition_filter  partitionfilterstring
/**
* get statistics about the data to be loaded. only input data size is implemented at this time.
*/
@override
public resourcestatistics getstatistics string location  job job  throws ioexception
try
resourcestatistics stats   new resourcestatistics
inputjobinfo inputjobinfo    inputjobinfo  hcatutil deserialize
job getconfiguration   get hcatconstants hcat_key_job_info
stats setmbytes getsizeinbytes inputjobinfo    1024   1024
return stats
catch  exception e
throw new ioexception e
private string getpartitionfilterstring
if  partitionfilterstring    null
properties props   udfcontext getudfcontext   getudfproperties
this getclass    new string signature
partitionfilterstring   props getproperty partition_filter
return partitionfilterstring
private string gethcatcomparisonstring expression expr
if  expr instanceof binaryexpression
// call gethcatcomparisonstring on lhs and rhs, and and join the
// results with optype string
// we can just use optype.tostring() on all expression types except
// equal, notequalt since equal has '==' in tostring() and
// we need '='
string opstr   null
switch  expr getoptype
case op_eq
opstr
break
default
opstr   expr getoptype   tostring
binaryexpression be    binaryexpression  expr
return     gethcatcomparisonstring be getlhs
opstr
gethcatcomparisonstring be getrhs
else
// should be a constant or column
return expr tostring