/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql metadata
import java io serializable
import java net uri
import java util arraylist
import java util arrays
import java util hashmap
import java util linkedhashmap
import java util list
import java util map
import java util properties
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hive common javautils
import org apache hadoop hive metastore metastoreutils
import org apache hadoop hive metastore protectmode
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive metastore api order
import org apache hadoop hive metastore api storagedescriptor
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql io hivefileformatutils
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive ql io hivesequencefileoutputformat
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 serdeutils
import org apache hadoop mapred inputformat
import org apache thrift texception
import org apache thrift protocol tbinaryprotocol
import org apache thrift transport tmemorybuffer
/**
* a hive table partition: is a fundamental storage unit within a table.
*
* please note that the ql code should always go through methods of this class to access the
* metadata, instead of directly accessing org.apache.hadoop.hive.metastore.api.partition.
* this helps to isolate the metastore code and the ql code.
*/
public class partition implements serializable
@suppresswarnings
static final private log log   logfactory
getlog
private table table
private org apache hadoop hive metastore api partition tpartition
/**
* these fields are cached. the information comes from tpartition.
*/
private deserializer deserializer
private class<? extends hiveoutputformat> outputformatclass
private class<? extends inputformat> inputformatclass
private uri uri
/**
* @return the values of the partition
* @see org.apache.hadoop.hive.metastore.api.partition#getvalues()
*/
public list<string> getvalues
return tpartition getvalues
}
/**
* used only for serialization.
*/
public partition
}
/**
* create an empty partition.
* semanticanalyzer code requires that an empty partition when the table is not partitioned.
*/
public partition table tbl  throws hiveexception
org apache hadoop hive metastore api partition tpart
new org apache hadoop hive metastore api partition
if   tbl isview
tpart setsd tbl getttable   getsd        todo  get a copy
}
initialize tbl  tpart
}
public partition table tbl  org apache hadoop hive metastore api partition tp
throws hiveexception
initialize tbl  tp
}
/**
* create partition object with the given info.
*
* @param tbl
*          table the partition will be in.
* @param partspec
*          partition specifications.
* @param location
*          location of the partition, relative to the table.
* @throws hiveexception
*           thrown if we could not create the partition.
*/
public partition table tbl  map<string  string> partspec  path location
throws hiveexception
list<string> pvals   new arraylist<string>
for  fieldschema field   tbl getpartcols
string val   partspec get field getname
if  val    null
throw new hiveexception
}
pvals add val
}
org apache hadoop hive metastore api partition tpart   new org apache hadoop hive metastore api partition
tpart setdbname tbl getdbname
tpart settablename tbl gettablename
tpart setvalues pvals
if  tbl isview
initialize tbl  tpart
return
}
storagedescriptor sd   new storagedescriptor
try
// replace with thrift-138
tmemorybuffer buffer   new tmemorybuffer 1024
tbinaryprotocol prot   new tbinaryprotocol buffer
tbl getttable   getsd   write prot
sd read prot
catch  texception e
log error
throw new hiveexception   e
}
tpart setsd sd
if  location    null
tpart getsd   setlocation location tostring
else
tpart getsd   setlocation null
}
initialize tbl  tpart
}
/**
* initializes this object with the given variables
*
* @param table
*          table the partition belongs to
* @param tpartition
*          thrift partition object
* @throws hiveexception
*           thrown if we cannot initialize the partition
*/
private void initialize table table
org apache hadoop hive metastore api partition tpartition  throws hiveexception
this table   table
this tpartition   tpartition
if  table isview
return
}
string partname
if  table ispartitioned
try
partname   warehouse makepartname table getpartcols    tpartition getvalues
if  tpartition getsd   getlocation      null
// set default if location is not set and this is a physical
// table partition (not a view partition)
if  table getdatalocation      null
path partpath   new path
table getdatalocation   tostring    partname
tpartition getsd   setlocation partpath tostring
}
}
// set default if columns are not set
if  tpartition getsd   getcols      null
if  table getcols      null
tpartition getsd   setcols table getcols
}
}
catch  metaexception e
throw new hiveexception     table gettablename
e
}
}
// this will set up field: inputformatclass
getinputformatclass
// this will set up field: outputformatclass
getoutputformatclass
getdeserializer
}
public string getname
try
return warehouse makepartname table getpartcols    tpartition getvalues
catch  metaexception e
throw new runtimeexception e
}
}
public path getpath
path ret   new path getpartitionpath
return ret
}
public path getpartitionpath
if  table ispartitioned
return new path tpartition getsd   getlocation
else
return new path table getttable   getsd   getlocation
}
}
final public uri getdatalocation
if  uri    null
uri   getpartitionpath   touri
}
return uri
}
final public deserializer getdeserializer
if  deserializer    null
try
deserializer   metastoreutils getdeserializer hive get   getconf
tpartition  table getttable
catch  hiveexception e
throw new runtimeexception e
catch  metaexception e
throw new runtimeexception e
}
}
return deserializer
}
final public deserializer getdeserializer properties props
if  deserializer    null
try
deserializer   metastoreutils getdeserializer hive get   getconf    props
catch  hiveexception e
throw new runtimeexception e
catch  metaexception e
throw new runtimeexception e
}
}
return deserializer
}
public properties getschema
return metastoreutils getschema tpartition  table getttable
}
public properties getmetadatafrompartitionschema
return metastoreutils getpartitionmetadata tpartition  table getttable
}
public properties getschemafromtableschema properties tblschema
return metastoreutils getpartschemafromtableschema tpartition getsd    table getttable   getsd
tpartition getparameters    table getdbname    table gettablename    table getpartitionkeys
tblschema
}
/**
* @param inputformatclass
*/
public void setinputformatclass class<? extends inputformat> inputformatclass
this inputformatclass   inputformatclass
tpartition getsd   setinputformat inputformatclass getname
}
/**
* @param outputformatclass
*/
public void setoutputformatclass class<? extends hiveoutputformat> outputformatclass
this outputformatclass   outputformatclass
tpartition getsd   setoutputformat hivefileformatutils
getoutputformatsubstitute outputformatclass  false  tostring
}
final public class<? extends inputformat> getinputformatclass
throws hiveexception
if  inputformatclass    null
string clsname   null
if  tpartition    null    tpartition getsd      null
clsname   tpartition getsd   getinputformat
}
if  clsname    null
clsname   org apache hadoop mapred sequencefileinputformat class getname
}
try
inputformatclass     class<? extends inputformat>  class forname clsname  true
javautils getclassloader
catch  classnotfoundexception e
throw new hiveexception     clsname  e
}
}
return inputformatclass
}
final public class<? extends hiveoutputformat> getoutputformatclass
throws hiveexception
if  outputformatclass    null
string clsname   null
if  tpartition    null    tpartition getsd      null
clsname   tpartition getsd   getoutputformat
}
if  clsname    null
clsname   hivesequencefileoutputformat class getname
}
try
class<?> c    class forname clsname  true
javautils getclassloader
// replace fileoutputformat for backward compatibility
if   hiveoutputformat class isassignablefrom c
outputformatclass   hivefileformatutils getoutputformatsubstitute c false
else
outputformatclass    class<? extends hiveoutputformat> c
}
catch  classnotfoundexception e
throw new hiveexception     clsname  e
}
}
return outputformatclass
}
public int getbucketcount
return tpartition getsd   getnumbuckets
/*
* todo: keeping this code around for later use when we will support
* sampling on tables which are not created with clustered into clause
*
* // read from table meta data int numbuckets = this.table.getnumbuckets();
* if (numbuckets == -1) { // table meta data does not have bucket
* information // check if file system has multiple buckets(files) in this
* partition string pathpattern = this.partpath.tostring() + "/*"; try {
* filesystem fs = filesystem.get(this.table.getdatalocation(),
* hive.get().getconf()); filestatus srcs[] = fs.globstatus(new
* path(pathpattern)); numbuckets = srcs.length; } catch (exception e) {
* throw new runtimeexception("cannot get bucket count for table " +
* this.table.getname(), e); } } return numbuckets;
*/
}
public void setbucketcount int newbucketnum
tpartition getsd   setnumbuckets newbucketnum
}
public list<string> getbucketcols
return tpartition getsd   getbucketcols
}
public list<order> getsortcols
return tpartition getsd   getsortcols
}
public list<string> getsortcolnames
return utilities getcolumnnamesfromsortcols getsortcols
}
/**
* get all paths for this partition in a sorted manner
*/
@suppresswarnings
public filestatus getsortedpaths
try
// previously, this got the filesystem of the table, which could be
// different from the filesystem of the partition.
filesystem fs   filesystem get getpartitionpath   touri    hive get
getconf
string pathpattern   getpartitionpath   tostring
if  getbucketcount   > 0
pathpattern   pathpattern
}
log.info("path pattern = " + pathpattern);
filestatus srcs[] = fs.globstatus(new path(pathpattern));
arrays.sort(srcs);
for (filestatus src : srcs) {
log.info("got file: " + src.getpath());
}
if (srcs.length == 0) {
return null;
}
return srcs;
} catch (exception e) {
throw new runtimeexception("cannot get path ", e);
}
}
/**
* mapping from bucket number to bucket path
*/
// todo: add test case and clean it up
@suppresswarnings
public path getbucketpath int bucketnum
filestatus srcs   getsortedpaths
if  srcs    null
return null;
}
return srcs getpath
}
@suppresswarnings
public path getpath sample s  throws hiveexception
if  s    null
return getpath
else
int bcount   getbucketcount
if  bcount    0
return getpath
}
dimension d   s getsampledimension
if   d getdimensionid   equals table getbucketingdimensionid
// if the bucket dimension is not the same as the sampling dimension
// we must scan all the data
return getpath
}
int scount   s getsamplefraction
arraylist<path> ret   new arraylist<path>
if  bcount    scount
ret add getbucketpath s getsamplenum     1
else if  bcount < scount
if   scount   bcount    bcount    scount
throw new hiveexception     scount
bcount
table gettablename
}
// undersampling a bucket
ret add getbucketpath  s getsamplenum     1  % bcount
else if  bcount > scount
if   bcount   scount    scount    bcount
throw new hiveexception     scount
bcount
table gettablename
}
// sampling multiple buckets
for  int i   0  i < bcount   scount  i
ret add getbucketpath i   scount    s getsamplenum     1
}
}
return  ret toarray new path
}
}
public linkedhashmap<string  string> getspec
return table createspec tpartition
}
@suppresswarnings
@override
public string tostring
string pn
try
pn   warehouse makepartname getspec    false
catch  metaexception e
// ignore as we most probably in an exception path already otherwise this
// error wouldn't occur
}
return table tostring         pn
}
public table gettable
return table
}
/**
* should be only used by serialization.
*/
public void settable table table
this table   table
}
/**
* should be only used by serialization.
*/
public org apache hadoop hive metastore api partition gettpartition
return tpartition
}
/**
* should be only used by serialization.
*/
public void settpartition
org apache hadoop hive metastore api partition partition
tpartition   partition
}
public map<string  string> getparameters
return tpartition getparameters
}
public list<fieldschema> getcols
if   serdeutils shouldgetcolsfromserde
tpartition getsd   getserdeinfo   getserializationlib
return tpartition getsd   getcols
}
try
return hive getfieldsfromdeserializer table gettablename    getdeserializer
catch  hiveexception e
log error
tpartition getsd   getserdeinfo   getserializationlib    e
}
return new arraylist<fieldschema>
}
public string getlocation
if  tpartition getsd      null
return null;
else
return tpartition getsd   getlocation
}
}
public void setlocation string location
tpartition getsd   setlocation location
}
/**
* set partition's values
*
* @param partspec
*          partition specifications.
* @throws hiveexception
*           thrown if we could not create the partition.
*/
public void setvalues map<string  string> partspec
throws hiveexception
list<string> pvals   new arraylist<string>
for  fieldschema field   table getpartcols
string val   partspec get field getname
if  val    null
throw new hiveexception
}
pvals add val
}
tpartition setvalues pvals
}
/**
* @param protectmode
*/
public void setprotectmode protectmode protectmode
map<string  string> parameters   tpartition getparameters
string pm   protectmode tostring
if  pm    null
parameters put protectmode parameter_name  pm
else
parameters remove protectmode parameter_name
}
tpartition setparameters parameters
}
/**
* @return protect mode
*/
public protectmode getprotectmode
map<string  string> parameters   tpartition getparameters
if  parameters    null
return null;
}
if   parameters containskey protectmode parameter_name
return new protectmode
else
return protectmode getprotectmodefromstring
parameters get protectmode parameter_name
}
}
/**
* @return true protect mode indicates the partition if offline.
*/
public boolean isoffline
protectmode pm   getprotectmode
if  pm    null
return false
else
return pm offline
}
}
/**
* @return true if protect mode attribute of the partition indicate
* that it is ok to drop the table
*/
public boolean candrop
protectmode mode   getprotectmode
protectmode parentmode   table getprotectmode
return   mode nodrop     mode offline     mode readonly     parentmode nodropcascade
}
/**
* @return true if protect mode attribute of the partition indicate
* that it is ok to write to the table
*/
public boolean canwrite
protectmode mode   getprotectmode
return   mode offline     mode readonly
}
/**
* @return include the db name
*/
public string getcompletename
return gettable   getcompletename         getname
}
public int getlastaccesstime
return tpartition getlastaccesstime
}
public void setlastaccesstime int lastaccesstime
tpartition setlastaccesstime lastaccesstime
}
public boolean isstoredassubdirectories
return tpartition getsd   isstoredassubdirectories
}
public list<list<string>> getskewedcolvalues
return tpartition getsd   getskewedinfo   getskewedcolvalues
}
public list<string> getskewedcolnames
return tpartition getsd   getskewedinfo   getskewedcolnames
}
public void setskewedvaluelocationmap list<string> vallist  string dirname
throws hiveexception
map<list<string>  string> mappings   tpartition getsd   getskewedinfo
getskewedcolvaluelocationmaps
if  null    mappings
mappings   new hashmap<list<string>  string>
tpartition getsd   getskewedinfo   setskewedcolvaluelocationmaps mappings
}
// add or update new mapping
mappings put vallist  dirname
}
public map<list<string>  string> getskewedcolvaluelocationmaps
return tpartition getsd   getskewedinfo   getskewedcolvaluelocationmaps
}
}