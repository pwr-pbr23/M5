/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql optimizer physical
import java io bytearrayinputstream
import java io serializable
import java io unsupportedencodingexception
import java util arraylist
import java util hashmap
import java util linkedhashmap
import java util list
import java util map
import org apache hadoop fs path
import org apache hadoop hive conf hiveconf
import org apache hadoop hive ql exec conditionaltask
import org apache hadoop hive ql exec joinoperator
import org apache hadoop hive ql exec mapjoinoperator
import org apache hadoop hive ql exec operator
import org apache hadoop hive ql exec operatorfactory
import org apache hadoop hive ql exec rowschema
import org apache hadoop hive ql exec tablescanoperator
import org apache hadoop hive ql exec task
import org apache hadoop hive ql exec taskfactory
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql io hiveinputformat
import org apache hadoop hive ql parse parsecontext
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql plan conditionalresolverskewjoin
import org apache hadoop hive ql plan conditionalwork
import org apache hadoop hive ql plan exprnodecolumndesc
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan fetchwork
import org apache hadoop hive ql plan joindesc
import org apache hadoop hive ql plan mapjoindesc
import org apache hadoop hive ql plan mapwork
import org apache hadoop hive ql plan mapredlocalwork
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan planutils
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive serde2 typeinfo typeinfofactory
/**
* genmrskewjoinprocessor.
*
*/
public final class genmrskewjoinprocessor
private genmrskewjoinprocessor
// prevent instantiation
/**
* create tasks for processing skew joins. the idea is (hive-964) to use
* separated jobs and map-joins to handle skew joins.
* <p>
* <ul>
* <li>
* number of mr jobs to handle skew keys is the number of table minus 1 (we
* can stream the last table, so big keys in the last table will not be a
* problem).
* <li>
* at runtime in join, we output big keys in one table into one corresponding
* directories, and all same keys in other tables into different dirs(one for
* each table). the directories will look like:
* <ul>
* <li>
* dir-t1-bigkeys(containing big keys in t1), dir-t2-keys(containing keys
* which is big in t1),dir-t3-keys(containing keys which is big in t1), ...
* <li>
* dir-t1-keys(containing keys which is big in t2), dir-t2-bigkeys(containing
* big keys in t2),dir-t3-keys(containing keys which is big in t2), ...
* <li>
* dir-t1-keys(containing keys which is big in t3), dir-t2-keys(containing big
* keys in t3),dir-t3-bigkeys(containing keys which is big in t3), ... .....
* </ul>
* </ul>
* for each table, we launch one mapjoin job, taking the directory containing
* big keys in this table and corresponding dirs in other tables as input.
* (actally one job for one row in the above.)
*
* <p>
* for more discussions, please check
* https://issues.apache.org/jira/browse/hive-964.
*
*/
@suppresswarnings
public static void processskewjoin joinoperator joinop
task<? extends serializable> currtask  parsecontext parsectx
throws semanticexception
// we are trying to adding map joins to handle skew keys, and map join right
// now does not work with outer joins
if   genmrskewjoinprocessor skewjoinenabled parsectx getconf    joinop
return
string basetmpdir   parsectx getcontext   getmrtmpfileuri
joindesc joindescriptor   joinop getconf
map<byte  list<exprnodedesc>> joinvalues   joindescriptor getexprs
int numaliases   joinvalues size
map<byte  string> bigkeysdirmap   new hashmap<byte  string>
map<byte  map<byte  string>> smallkeysdirmap   new hashmap<byte  map<byte  string>>
map<byte  string> skewjoinjobresultsdir   new hashmap<byte  string>
byte tags   joindescriptor gettagorder
for  int i   0  i < numaliases  i
byte alias   tags
string bigkeysdir   getbigkeysdir basetmpdir  alias
bigkeysdirmap put alias  bigkeysdir
map<byte  string> smallkeysmap   new hashmap<byte  string>
smallkeysdirmap put alias  smallkeysmap
for  byte src2   tags
if   src2 equals alias
smallkeysmap put src2  getsmallkeysdir basetmpdir  alias  src2
skewjoinjobresultsdir put alias  getbigkeysskewjoinresultdir basetmpdir
alias
joindescriptor sethandleskewjoin true
joindescriptor setbigkeysdirmap bigkeysdirmap
joindescriptor setsmallkeysdirmap smallkeysdirmap
joindescriptor setskewkeydefinition hiveconf getintvar parsectx getconf
hiveconf confvars hiveskewjoinkey
hashmap<string  task<? extends serializable>> bigkeysdirtotaskmap
new hashmap<string  task<? extends serializable>>
list<serializable> listworks   new arraylist<serializable>
list<task<? extends serializable>> listtasks   new arraylist<task<? extends serializable>>
mapredwork currplan    mapredwork  currtask getwork
tabledesc keytbldesc    tabledesc  currplan getreducework   getkeydesc   clone
list<string> joinkeys   utilities
getcolumnnames keytbldesc getproperties
list<string> joinkeytypes   utilities getcolumntypes keytbldesc
getproperties
map<byte  tabledesc> tabledesclist   new hashmap<byte  tabledesc>
map<byte  list<exprnodedesc>> newjoinvalues   new hashmap<byte  list<exprnodedesc>>
map<byte  list<exprnodedesc>> newjoinkeys   new hashmap<byte  list<exprnodedesc>>
// used for create mapjoindesc, should be in order
list<tabledesc> newjoinvaluetbldesc   new arraylist<tabledesc>
for  byte tag   tags
newjoinvaluetbldesc add null
for  int i   0  i < numaliases  i
byte alias   tags
list<exprnodedesc> valuecols   joinvalues get alias
string colnames
string coltypes
int columnsize   valuecols size
list<exprnodedesc> newvalueexpr   new arraylist<exprnodedesc>
list<exprnodedesc> newkeyexpr   new arraylist<exprnodedesc>
boolean first   true
for  int k   0  k < columnsize  k
typeinfo type   valuecols get k  gettypeinfo
string newcolname   i       k     any name  it does not matter
newvalueexpr
add new exprnodecolumndesc type  newcolname      i  false
if   first
colnames   colnames
coltypes   coltypes
first   false
colnames   colnames   newcolname
coltypes   coltypes   valuecols get k  gettypestring
// we are putting join keys at last part of the spilled table
for  int k   0  k < joinkeys size    k
if   first
colnames   colnames
coltypes   coltypes
first   false
colnames   colnames   joinkeys get k
coltypes   coltypes   joinkeytypes get k
newkeyexpr add new exprnodecolumndesc typeinfofactory
getprimitivetypeinfo joinkeytypes get k    joinkeys get k
i  false
newjoinvalues put alias  newvalueexpr
newjoinkeys put alias  newkeyexpr
tabledesclist put alias  utilities gettabledesc colnames  coltypes
// construct value table desc
string valuecolnames
string valuecoltypes
first   true
for  int k   0  k < columnsize  k
string newcolname   i       k     any name  it does not matter
if   first
valuecolnames   valuecolnames
valuecoltypes   valuecoltypes
valuecolnames   valuecolnames   newcolname
valuecoltypes   valuecoltypes   valuecols get k  gettypestring
first   false
newjoinvaluetbldesc set byte valueof  byte  i   utilities gettabledesc
valuecolnames  valuecoltypes
joindescriptor setskewkeysvaluestables tabledesclist
joindescriptor setkeytabledesc keytbldesc
for  int i   0  i < numaliases   1  i
byte src   tags
mapwork newplan   planutils getmapredwork   getmapwork
// this code has been only added for testing
boolean mappercannotspanpartns
parsectx getconf   getboolvar
hiveconf confvars hive_mapper_cannot_span_multiple_partitions
newplan setmappercannotspanpartns mappercannotspanpartns
mapredwork cloneplan   utilities cloneplan currplan
operator<? extends operatordesc> parentops   new tablescanoperator
for  int k   0  k < tags length  k
operator<? extends operatordesc> ts   operatorfactory get
tablescandesc class   rowschema  null
tablescanoperator ts  settabledesc tabledesclist get  byte k
parentops   ts
operator<? extends operatordesc> tblscan_op   parentops
arraylist<string> aliases   new arraylist<string>
string alias   src tostring
aliases add alias
string bigkeydirpath   bigkeysdirmap get src
newplan getpathtoaliases   put bigkeydirpath  aliases
newplan getaliastowork   put alias  tblscan_op
partitiondesc part   new partitiondesc tabledesclist get src   null
newplan getpathtopartitioninfo   put bigkeydirpath  part
newplan getaliastopartninfo   put alias  part
operator<? extends operatordesc> reducer   cloneplan getreducework   getreducer
assert reducer instanceof joinoperator
joinoperator clonejoinop    joinoperator  reducer
string dumpfileprefix     planutils getcountformapjoindumpfileprefix
mapjoindesc mapjoindescriptor   new mapjoindesc newjoinkeys  keytbldesc
newjoinvalues  newjoinvaluetbldesc  newjoinvaluetbldesc joindescriptor
getoutputcolumnnames    i  joindescriptor getconds
joindescriptor getfilters    joindescriptor getnoouterjoin    dumpfileprefix
mapjoindescriptor settagorder tags
mapjoindescriptor sethandleskewjoin false
mapjoindescriptor setnullsafes joindescriptor getnullsafes
mapredlocalwork localplan   new mapredlocalwork
new linkedhashmap<string  operator<? extends operatordesc>>
new linkedhashmap<string  fetchwork>
map<byte  string> smalltbldirs   smallkeysdirmap get src
for  int j   0  j < numaliases  j
if  j    i
continue
byte small_alias   tags
operator<? extends operatordesc> tblscan_op2   parentops
localplan getaliastowork   put small_alias tostring    tblscan_op2
path tbldir   new path smalltbldirs get small_alias
localplan getaliastofetchwork   put small_alias tostring
new fetchwork tbldir tostring    tabledesclist get small_alias
newplan setmaplocalwork localplan
// construct a map join and set it as the child operator of tblscan_op
mapjoinoperator mapjoinop    mapjoinoperator  operatorfactory
getandmakechild mapjoindescriptor   rowschema  null  parentops
// change the children of the original join operator to point to the map
// join operator
list<operator<? extends operatordesc>> childops   clonejoinop
getchildoperators
for  operator<? extends operatordesc> childop   childops
childop replaceparent clonejoinop  mapjoinop
mapjoinop setchildoperators childops
hiveconf jc   new hiveconf parsectx getconf
genmrskewjoinprocessor class
newplan setnummaptasks hiveconf
getintvar jc  hiveconf confvars hiveskewjoinmapjoinnummaptask
newplan
setminsplitsize hiveconf getlongvar jc  hiveconf confvars hiveskewjoinmapjoinminsplit
newplan setinputformat hiveinputformat class getname
mapredwork w   new mapredwork
w setmapwork newplan
task<? extends serializable> skewjoinmapjointask   taskfactory get w  jc
bigkeysdirtotaskmap put bigkeydirpath  skewjoinmapjointask
listworks add skewjoinmapjointask getwork
listtasks add skewjoinmapjointask
conditionalwork cndwork   new conditionalwork listworks
conditionaltask cndtsk    conditionaltask  taskfactory get cndwork  parsectx getconf
cndtsk setlisttasks listtasks
cndtsk setresolver new conditionalresolverskewjoin
cndtsk
setresolverctx new conditionalresolverskewjoin conditionalresolverskewjoinctx
bigkeysdirtotaskmap
list<task<? extends serializable>> oldchildtasks   currtask getchildtasks
currtask setchildtasks new arraylist<task<? extends serializable>>
currtask adddependenttask cndtsk
if  oldchildtasks    null
for  task<? extends serializable> tsk   cndtsk getlisttasks
for  task<? extends serializable> oldchild   oldchildtasks
tsk adddependenttask oldchild
return
public static boolean skewjoinenabled hiveconf conf  joinoperator joinop
if  conf    null     conf getboolvar hiveconf confvars hiveskewjoin
return false
if   joinop getconf   isnoouterjoin
return false
byte pos   0
for  byte tag   joinop getconf   gettagorder
if  tag    pos
return false
pos
return true
private static string skewjoinprefix
private static string underline
private static string bigkeys
private static string smallkeys
private static string results
static string getbigkeysdir string basedir  byte srctbl
return basedir   path separator   skewjoinprefix   underline   bigkeys
underline   srctbl
static string getbigkeysskewjoinresultdir string basedir  byte srctbl
return basedir   path separator   skewjoinprefix   underline   bigkeys
underline   results   underline   srctbl
static string getsmallkeysdir string basedir  byte srctblbigtbl
byte srctblsmalltbl
return basedir   path separator   skewjoinprefix   underline   smallkeys
underline   srctblbigtbl   underline   srctblsmalltbl