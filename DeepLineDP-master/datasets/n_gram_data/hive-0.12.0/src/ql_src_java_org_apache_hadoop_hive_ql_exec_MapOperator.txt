/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java io serializable
import java util arraylist
import java util arrays
import java util hashmap
import java util hashset
import java util list
import java util map
import java util map entry
import java util properties
import java util set
import org apache hadoop conf configuration
import org apache hadoop fs path
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore api hive_metastoreconstants
import org apache hadoop hive ql exec mr execmappercontext
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata virtualcolumn
import org apache hadoop hive ql plan mapwork
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive ql plan api operatortype
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 serdestats
import org apache hadoop hive serde2 serdeutils
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspectorconverters
import org apache hadoop hive serde2 objectinspector objectinspectorconverters converter
import org apache hadoop hive serde2 objectinspector objectinspectorfactory
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop hive serde2 objectinspector primitive primitiveobjectinspectorfactory
import org apache hadoop io longwritable
import org apache hadoop io text
import org apache hadoop io writable
import org apache hadoop util stringutils
/**
* map operator. this triggers overall map side processing. this is a little
* different from regular operators in that it starts off by processing a
* writable data structure from a table (instead of a hive object).
**/
public class mapoperator extends operator<mapwork> implements serializable  cloneable
private static final long serialversionuid   1l
/**
* counter.
*
*/
public static enum counter
deserialize_errors
private final transient longwritable deserialize_error_count   new longwritable
private final map<mapinputpath  mapopctx> opctxmap   new hashmap<mapinputpath  mapopctx>
private final map<operator<? extends operatordesc>  mapopctx> childrenoptoopctxmap
new hashmap<operator<? extends operatordesc>  mapopctx>
private transient mapopctx current
private transient list<operator<? extends operatordesc>> extrachildrentoclose   null
private static class mapinputpath
string path
string alias
operator<?> op
partitiondesc partdesc
/**
* @param path
* @param alias
* @param op
*/
public mapinputpath string path  string alias  operator<?> op  partitiondesc partdesc
this path   path
this alias   alias
this op   op
this partdesc   partdesc
@override
public boolean equals object o
if  o instanceof mapinputpath
mapinputpath mobj    mapinputpath  o
return path equals mobj path     alias equals mobj alias
op equals mobj op
return false
@override
public int hashcode
int ret    path    null  ? 0   path hashcode
ret     alias    null  ? 0   alias hashcode
ret     op    null  ? 0   op hashcode
return ret
private static class mapopctx
structobjectinspector tblrawrowobjectinspector      columns
structobjectinspector partobjectinspector        partition columns
structobjectinspector vcsobjectinspector         virtual columns
structobjectinspector rowobjectinspector
converter parttblobjectinspectorconverter
object rowwithpart
object rowwithpartandvc
deserializer deserializer
string tablename
string partname
list<virtualcolumn> vcs
writable vcvalues
private boolean ispartitioned
return partobjectinspector    null
private boolean hasvc
return vcsobjectinspector    null
private object readrow writable value  throws serdeexception
return parttblobjectinspectorconverter convert deserializer deserialize value
/**
* initializes this map op as the root of the tree. it sets jobconf &
* mapredwork and starts initialization of the operator tree rooted at this
* op.
*
* @param hconf
* @param mrwork
* @throws hiveexception
*/
public void initializeasroot configuration hconf  mapwork mapwork
throws hiveexception
setconf mapwork
setchildren hconf
initialize hconf  null
private mapopctx initobjectinspector configuration hconf  mapinputpath ctx
map<tabledesc  structobjectinspector> convertedoi  throws exception
partitiondesc pd   ctx partdesc
tabledesc td   pd gettabledesc
mapopctx opctx   new mapopctx
// use table properties in case of unpartitioned tables,
// and the union of table properties and partition properties, with partition
// taking precedence
properties partprops   ispartitioned pd  ?
pd getoverlayedproperties     pd gettabledesc   getproperties
map<string  string> partspec   pd getpartspec
opctx tablename   string valueof partprops getproperty
opctx partname   string valueof partspec
class serdeclass   pd getdeserializerclass
if  serdeclass    null
string classname   checkserdeclassname pd getserdeclassname    opctx tablename
serdeclass   hconf getclassbyname classname
opctx deserializer    deserializer  serdeclass newinstance
opctx deserializer initialize hconf  partprops
structobjectinspector partrawrowobjectinspector
structobjectinspector  opctx deserializer getobjectinspector
opctx tblrawrowobjectinspector   convertedoi get td
opctx parttblobjectinspectorconverter   objectinspectorconverters getconverter
partrawrowobjectinspector  opctx tblrawrowobjectinspector
// next check if this table has partitions and if so
// get the list of partition names as well as allocate
// the serdes for the partition columns
string pcols   partprops getproperty hive_metastoreconstants meta_table_partition_columns
// log log = logfactory.getlog(mapoperator.class.getname());
if  pcols    null    pcols length   > 0
string partkeys   pcols trim   split
list<string> partnames   new arraylist<string> partkeys length
object partvalues   new object
list<objectinspector> partobjectinspectors   new arraylist<objectinspector> partkeys length
for  int i   0  i < partkeys length  i
string key   partkeys
partnames add key
// partitions do not exist for this table
if  partspec    null
// for partitionless table, initialize partvalue to null
partvalues   null
else
partvalues   new text partspec get key
partobjectinspectors add primitiveobjectinspectorfactory writablestringobjectinspector
opctx rowwithpart   new object  null  partvalues
opctx partobjectinspector   objectinspectorfactory
getstandardstructobjectinspector partnames  partobjectinspectors
// the op may not be a tablescan for mapjoins
// consider the query: select /*+mapjoin(a)*/ count(*) from t1 a join t2 b on a.key = b.key;
// in that case, it will be a select, but the rowoi need not be ammended
if  ctx op instanceof tablescanoperator
tablescanoperator tsop    tablescanoperator  ctx op
tablescandesc tsdesc   tsop getconf
if  tsdesc    null    tsdesc hasvirtualcols
opctx vcs   tsdesc getvirtualcols
opctx vcvalues   new writable
opctx vcsobjectinspector   virtualcolumn getvcsobjectinspector opctx vcs
if  opctx ispartitioned
opctx rowwithpartandvc   arrays copyofrange opctx rowwithpart  0  3
else
opctx rowwithpartandvc   new object
if   opctx hasvc       opctx ispartitioned
opctx rowobjectinspector   opctx tblrawrowobjectinspector
return opctx
list<structobjectinspector> inspectors   new arraylist<structobjectinspector>
inspectors add opctx tblrawrowobjectinspector
if  opctx ispartitioned
inspectors add opctx partobjectinspector
if  opctx hasvc
inspectors add opctx vcsobjectinspector
opctx rowobjectinspector   objectinspectorfactory getunionstructobjectinspector inspectors
return opctx
// return the mapping for table descriptor to the expected table oi
/**
* traverse all the partitions for a table, and get the oi for the table.
* note that a conversion is required if any of the partition oi is different
* from the table oi. for eg. if the query references table t (partitions p1, p2),
* and p1's schema is same as t, whereas p2's scheme is different from t, conversion
* might be needed for both p1 and p2, since settableoi might be needed for t
*/
private map<tabledesc  structobjectinspector> getconvertedoi configuration hconf
throws hiveexception
map<tabledesc  structobjectinspector> tabledescoi
new hashmap<tabledesc  structobjectinspector>
set<tabledesc> identityconvertertabledesc   new hashset<tabledesc>
try
for  string onefile   conf getpathtoaliases   keyset
partitiondesc pd   conf getpathtopartitioninfo   get onefile
tabledesc tabledesc   pd gettabledesc
properties tblprops   tabledesc getproperties
// if the partition does not exist, use table properties
properties partprops   ispartitioned pd  ? pd getoverlayedproperties     tblprops
class sdclass   pd getdeserializerclass
if  sdclass    null
string classname   checkserdeclassname pd getserdeclassname
pd getproperties   getproperty
sdclass   hconf getclassbyname classname
deserializer partdeserializer    deserializer  sdclass newinstance
partdeserializer initialize hconf  partprops
structobjectinspector partrawrowobjectinspector    structobjectinspector  partdeserializer
getobjectinspector
structobjectinspector tblrawrowobjectinspector   tabledescoi get tabledesc
if   tblrawrowobjectinspector    null
identityconvertertabledesc contains tabledesc
sdclass   tabledesc getdeserializerclass
if  sdclass    null
string classname   checkserdeclassname tabledesc getserdeclassname
tabledesc getproperties   getproperty
sdclass   hconf getclassbyname classname
deserializer tbldeserializer    deserializer  sdclass newinstance
tbldeserializer initialize hconf  tblprops
tblrawrowobjectinspector
structobjectinspector  objectinspectorconverters getconvertedoi
partrawrowobjectinspector
tbldeserializer getobjectinspector    true
if  identityconvertertabledesc contains tabledesc
if   partrawrowobjectinspector equals tblrawrowobjectinspector
identityconvertertabledesc remove tabledesc
else if  partrawrowobjectinspector equals tblrawrowobjectinspector
identityconvertertabledesc add tabledesc
tabledescoi put tabledesc  tblrawrowobjectinspector
catch  exception e
throw new hiveexception e
return tabledescoi
private boolean ispartitioned partitiondesc pd
return pd getpartspec      null     pd getpartspec   isempty
private string checkserdeclassname string classname  string tablename  throws hiveexception
if  classname    null    classname isempty
throw new hiveexception
tablename
return classname
public void setchildren configuration hconf  throws hiveexception
path fpath   new path hiveconf getvar hconf
hiveconf confvars hadoopmapfilename
boolean schemeless   fpath touri   getscheme      null
list<operator<? extends operatordesc>> children
new arraylist<operator<? extends operatordesc>>
map<tabledesc  structobjectinspector> convertedoi   getconvertedoi hconf
try
for  map entry<string  arraylist<string>> entry   conf getpathtoaliases   entryset
string onefile   entry getkey
list<string> aliases   entry getvalue
path onepath   new path onefile
if  schemeless
onepath   new path onepath touri   getpath
partitiondesc partdesc   conf getpathtopartitioninfo   get onefile
for  string onealias   aliases
operator<? extends operatordesc> op   conf getaliastowork   get onealias
log info     onealias
onefile
mapinputpath inp   new mapinputpath onefile  onealias  op  partdesc
if  opctxmap containskey inp
continue
mapopctx opctx   initobjectinspector hconf  inp  convertedoi
opctxmap put inp  opctx
op setparentoperators new arraylist<operator<? extends operatordesc>>
op getparentoperators   add this
// check for the operators who will process rows coming to this map
// operator
if   onepath touri   relativize fpath touri    equals fpath touri
children add op
childrenoptoopctxmap put op  opctx
log info     op getname
opctxmap get inp  rowobjectinspector gettypename
current   opctx      just need for testoperators testmapoperator
if  children size      0
// didn't find match for input file path in configuration!
// serious problem ..
log error
fpath touri
throw new hiveexception
// we found all the operators that we are supposed to process.
setchildoperators children
catch  exception e
throw new hiveexception e
@override
public void initializeop configuration hconf  throws hiveexception
// set that parent initialization is done and call initialize on children
state   state init
statsmap put counter deserialize_errors  deserialize_error_count
list<operator<? extends operatordesc>> children   getchildoperators
for  entry<operator<? extends operatordesc>  mapopctx> entry   childrenoptoopctxmap
entryset
operator<? extends operatordesc> child   entry getkey
mapopctx mapopctx   entry getvalue
// add alias, table name, and partitions to hadoop conf so that their
// children will inherit these
hiveconf setvar hconf  hiveconf confvars hivetablename  mapopctx tablename
hiveconf setvar hconf  hiveconf confvars hivepartitionname  mapopctx partname
child initialize hconf  new objectinspector  mapopctx rowobjectinspector
for  entry<mapinputpath  mapopctx> entry   opctxmap entryset
mapinputpath input   entry getkey
mapopctx mapopctx   entry getvalue
// add alias, table name, and partitions to hadoop conf so that their
// children will inherit these
hiveconf setvar hconf  hiveconf confvars hivetablename  mapopctx tablename
hiveconf setvar hconf  hiveconf confvars hivepartitionname  mapopctx partname
operator<? extends operatordesc> op   input op
if  children indexof op      1
// op is not in the children list, so need to remember it and close it afterwards
if  extrachildrentoclose    null
extrachildrentoclose   new arraylist<operator<? extends operatordesc>>
extrachildrentoclose add op
op initialize hconf  new objectinspector  entry getvalue   rowobjectinspector
/**
* close extra child operators that are initialized but are not executed.
*/
@override
public void closeop boolean abort  throws hiveexception
if  extrachildrentoclose    null
for  operator<? extends operatordesc> op   extrachildrentoclose
op close abort
// find context for current input file
@override
public void cleanupinputfilechangedop   throws hiveexception
path fpath   normalizepath getexeccontext   getcurrentinputfile
for  string onefile   conf getpathtoaliases   keyset
path onepath   normalizepath onefile
// check for the operators who will process rows coming to this map
// operator
if  onepath touri   relativize fpath touri    equals fpath touri
// not from this
continue
partitiondesc partdesc   conf getpathtopartitioninfo   get onefile
for  string onealias   conf getpathtoaliases   get onefile
operator<? extends operatordesc> op   conf getaliastowork   get onealias
mapinputpath inp   new mapinputpath onefile  onealias  op  partdesc
mapopctx context   opctxmap get inp
if  context    null
current   context
log info     onealias       onefile
return
throw new illegalstateexception     fpath
private path normalizepath string onefile
return new path onefile
public void process writable value  throws hiveexception
// a mapper can span multiple files/partitions.
// the serializers need to be reset if the input file changed
execmappercontext context   getexeccontext
if  context    null    context inputfilechanged
// the child operators cleanup if input file has changed
cleanupinputfilechanged
object row
try
row   current readrow value
if  current hasvc
current rowwithpartandvc   row
if  context    null
populatevirtualcolumnvalues context  current vcs  current vcvalues  current deserializer
int vcpos   current ispartitioned   ? 2   1
current rowwithpartandvc   current vcvalues
row   current rowwithpartandvc
else if  current ispartitioned
current rowwithpart   row
row   current rowwithpart
catch  exception e
// serialize the row and output.
string rawrowstring
try
rawrowstring   value tostring
catch  exception e2
rawrowstring
stringutils stringifyexception e2
// todo: policy on deserialization errors
deserialize_error_count set deserialize_error_count get     1
throw new hiveexception     rawrowstring  e
// the row has been converted to comply with table schema, irrespective of partition schema.
// so, use tbloi (and not partoi) for forwarding
try
forward row  current rowobjectinspector
catch  exception e
// serialize the row and output the error message.
string rowstring
try
rowstring   serdeutils getjsonstring row  current rowobjectinspector
catch  exception e2
rowstring
stringutils stringifyexception e2
throw new hiveexception     rowstring  e
public static writable populatevirtualcolumnvalues execmappercontext ctx
list<virtualcolumn> vcs  writable vcvalues  deserializer deserializer
if  vcs    null
return vcvalues
if  vcvalues    null
vcvalues   new writable
for  int i   0  i < vcs size    i
virtualcolumn vc   vcs get i
if  vc equals virtualcolumn filename
if  ctx inputfilechanged
vcvalues   new text ctx getcurrentinputfile
else if  vc equals virtualcolumn blockoffset
long current   ctx getiocxt   getcurrentblockstart
longwritable old    longwritable  vcvalues
if  old    null
old   new longwritable current
vcvalues   old
continue
if  current    old get
old set current
else if  vc equals virtualcolumn rowoffset
long current   ctx getiocxt   getcurrentrow
longwritable old    longwritable  vcvalues
if  old    null
old   new longwritable current
vcvalues   old
continue
if  current    old get
old set current
else if  vc equals virtualcolumn rawdatasize
long current   0l
serdestats stats   deserializer getserdestats
if stats    null
current   stats getrawdatasize
longwritable old    longwritable  vcvalues
if  old    null
old   new longwritable current
vcvalues   old
continue
if  current    old get
old set current
return vcvalues
@override
public void processop object row  int tag  throws hiveexception
throw new hiveexception
@override
public string getname
return getoperatorname
static public string getoperatorname
return
@override
public operatortype gettype
return null