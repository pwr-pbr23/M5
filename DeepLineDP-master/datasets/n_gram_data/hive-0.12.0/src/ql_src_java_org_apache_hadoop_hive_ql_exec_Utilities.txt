/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java beans defaultpersistencedelegate
import java beans encoder
import java beans exceptionlistener
import java beans expression
import java beans persistencedelegate
import java beans statement
import java beans xmldecoder
import java beans xmlencoder
import java io bufferedreader
import java io bytearrayinputstream
import java io bytearrayoutputstream
import java io datainput
import java io eofexception
import java io file
import java io fileinputstream
import java io filenotfoundexception
import java io ioexception
import java io inputstream
import java io inputstreamreader
import java io outputstream
import java io printstream
import java io serializable
import java io unsupportedencodingexception
import java net uri
import java net url
import java net urlclassloader
import java security messagedigest
import java security nosuchalgorithmexception
import java sql connection
import java sql drivermanager
import java sql preparedstatement
import java sql sqlexception
import java sql sqltransientexception
import java sql timestamp
import java text simpledateformat
import java util arraylist
import java util arrays
import java util calendar
import java util collection
import java util collections
import java util date
import java util hashmap
import java util hashset
import java util iterator
import java util linkedhashmap
import java util linkedlist
import java util list
import java util map
import java util properties
import java util random
import java util set
import java util uuid
import java util concurrent concurrenthashmap
import java util concurrent executionexception
import java util concurrent future
import java util concurrent linkedblockingqueue
import java util concurrent threadpoolexecutor
import java util concurrent timeunit
import java util regex matcher
import java util regex pattern
import org antlr runtime commontoken
import org apache commons lang stringutils
import org apache commons lang wordutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop filecache distributedcache
import org apache hadoop fs contentsummary
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs pathfilter
import org apache hadoop hive common hiveinterruptcallback
import org apache hadoop hive common hiveinterruptutils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api order
import org apache hadoop hive ql context
import org apache hadoop hive ql errormsg
import org apache hadoop hive ql queryplan
import org apache hadoop hive ql exec filesinkoperator recordwriter
import org apache hadoop hive ql exec mr execdriver
import org apache hadoop hive ql exec mr mapredtask
import org apache hadoop hive ql io contentsummaryinputformat
import org apache hadoop hive ql io hivefileformatutils
import org apache hadoop hive ql io hiveignorekeytextoutputformat
import org apache hadoop hive ql io hiveinputformat
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive ql io hivesequencefileoutputformat
import org apache hadoop hive ql io onenullrowinputformat
import org apache hadoop hive ql io rcfile
import org apache hadoop hive ql io reworkmapredinputformat
import org apache hadoop hive ql log perflogger
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql plan basework
import org apache hadoop hive ql plan dynamicpartitionctx
import org apache hadoop hive ql plan exprnodecolumndesc
import org apache hadoop hive ql plan exprnodeconstantdesc
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan exprnodegenericfuncdesc
import org apache hadoop hive ql plan filesinkdesc
import org apache hadoop hive ql plan groupbydesc
import org apache hadoop hive ql plan mapwork
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan planutils
import org apache hadoop hive ql plan planutils expressiontypes
import org apache hadoop hive ql plan reducework
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan api adjacency
import org apache hadoop hive ql plan api graph
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive ql stats statsfactory
import org apache hadoop hive ql stats statspublisher
import org apache hadoop hive ql udf generic genericudf
import org apache hadoop hive ql udf generic genericudfopand
import org apache hadoop hive ql udf generic genericudfopequal
import org apache hadoop hive ql udf generic genericudfopnotequal
import org apache hadoop hive ql udf generic genericudfopor
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 serializer
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive shims shimloader
import org apache hadoop io ioutils
import org apache hadoop io sequencefile
import org apache hadoop io sequencefile compressiontype
import org apache hadoop io text
import org apache hadoop io writable
import org apache hadoop io compress compressioncodec
import org apache hadoop io compress defaultcodec
import org apache hadoop mapred fileinputformat
import org apache hadoop mapred fileoutputformat
import org apache hadoop mapred inputformat
import org apache hadoop mapred jobconf
import org apache hadoop mapred reporter
import org apache hadoop mapred sequencefileinputformat
import org apache hadoop mapred sequencefileoutputformat
import org apache hadoop util reflectionutils
import org apache hadoop util shell
/**
* utilities.
*
*/
@suppresswarnings
public final class utilities
/**
* the object in the reducer are composed of these top level fields.
*/
public static string hadoop_local_fs
public static string map_plan_name
public static string reduce_plan_name
/**
* reducefield:
* key: record key
* value: record value
*/
public static enum reducefield
key  value
public static list<string> reducefieldnamelist
static
reducefieldnamelist   new arraylist<string>
for  reducefield r   reducefield values
reducefieldnamelist add r tostring
}
}
private utilities
// prevent instantiation
}
private static map<path  basework> gworkmap   collections
synchronizedmap new hashmap<path  basework>
private static final log log   logfactory getlog utilities class getname
public static void clearwork configuration conf
path mappath   getplanpath conf  map_plan_name
path reducepath   getplanpath conf  reduce_plan_name
// if the plan path hasn't been initialized just return, nothing to clean.
if  mappath    null    reducepath    null
return
}
try
filesystem fs   mappath getfilesystem conf
if  fs exists mappath
fs delete mappath  true
}
if  fs exists reducepath
fs delete reducepath  true
}
catch  exception e
log warn    e
finally
// where a single process works with multiple plans - we must clear
// the cache before working with the next plan.
if  mappath    null
gworkmap remove mappath
}
if  reducepath    null
gworkmap remove reducepath
}
}
}
public static mapredwork getmapredwork configuration conf
mapredwork w   new mapredwork
w setmapwork getmapwork conf
w setreducework getreducework conf
return w
}
public static mapwork getmapwork configuration conf
return  mapwork  getbasework conf  map_plan_name
}
public static reducework getreducework configuration conf
return  reducework  getbasework conf  reduce_plan_name
}
public static basework getbasework configuration conf  string name
basework gwork   null
path path   null
try
path   getplanpath conf  name
assert path    null
gwork   gworkmap get path
if  gwork    null
path localpath
if  shimloader gethadoopshims   islocalmode conf
localpath   path
} else {
localpath   new path name
}
inputstream in   new fileinputstream localpath touri   getpath
basework ret   deserializeplan in
gwork   ret
gworkmap put path  gwork
}
return gwork
catch  filenotfoundexception fnf
// happens. e.g.: no reduce work.
log debug   path
return null
catch  exception e
e printstacktrace
log error   path  e
throw new runtimeexception e
}
}
public static void setworkflowadjacencies configuration conf  queryplan plan
try
graph stagegraph   plan getqueryplan   getstagegraph
if  stagegraph    null
return
}
list<adjacency> adjlist   stagegraph getadjacencylist
if  adjlist    null
return
}
for  adjacency adj   adjlist
list<string> children   adj getchildren
if  children    null    children isempty
return
}
conf setstrings   adj getnode
children toarray new string
}
catch  ioexception e
}
}
public static list<string> getfieldschemastring list<fieldschema> fl
if  fl    null
return null
}
arraylist<string> ret   new arraylist<string>
for  fieldschema f   fl
ret add f getname         f gettype
f getcomment      null ?      f getcomment
}
return ret
}
/**
* java 1.5 workaround. from http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5015403
*/
public static class enumdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
return new expression enum class     new object  oldinstance getclass
enum<?>  oldinstance  name
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return oldinstance    newinstance
}
}
public static class mapdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
map oldmap    map  oldinstance
hashmap newmap   new hashmap oldmap
return new expression newmap  hashmap class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
public static class setdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
set oldset    set  oldinstance
hashset newset   new hashset oldset
return new expression newset  hashset class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
public static class listdelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
list oldlist    list  oldinstance
arraylist newlist   new arraylist oldlist
return new expression newlist  arraylist class     new object
}
@override
protected boolean mutatesto object oldinstance  object newinstance
return false
}
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
java util collection oldo    java util collection  oldinstance
java util collection newo    java util collection  newinstance
if  newo size      0
out writestatement new statement oldinstance     new object
}
for  iterator i   oldo iterator    i hasnext
out writestatement new statement oldinstance     new object  i next
}
}
}
/**
* datepersistencedelegate. needed to serialize java.util.date
* since it is not serialization friendly.
* also works for java.sql.date since it derives from java.util.date.
*/
public static class datepersistencedelegate extends persistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
date dateval    date oldinstance
object args     dateval gettime
return new expression dateval  dateval getclass       args
}
@override
protected boolean mutatesto object oldinstance  object newinstance
if  oldinstance    null    newinstance    null
return false
}
return oldinstance getclass      newinstance getclass
}
}
/**
* timestamppersistencedelegate. needed to serialize java.sql.timestamp since
* it is not serialization friendly.
*/
public static class timestamppersistencedelegate extends datepersistencedelegate
@override
protected void initialize class<?> type  object oldinstance  object newinstance  encoder out
timestamp ts    timestamp oldinstance
object args     ts getnanos
statement stmt   new statement oldinstance     args
out writestatement stmt
}
}
/**
* need to serialize org.antlr.runtime.commontoken
*/
public static class commontokendelegate extends persistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
commontoken ct    commontoken oldinstance
object args    ct gettype    ct gettext
return new expression ct  ct getclass       args
}
}
public static void setmapredwork configuration conf  mapredwork w  string hivescratchdir
setmapwork conf  w getmapwork    hivescratchdir  true
if  w getreducework      null
setreducework conf  w getreducework    hivescratchdir  true
}
}
public static path setmapwork configuration conf  mapwork w  string hivescratchdir  boolean usecache
return setbasework conf  w  hivescratchdir  map_plan_name  usecache
}
public static path setreducework configuration conf  reducework w  string hivescratchdir  boolean usecache
return setbasework conf  w  hivescratchdir  reduce_plan_name  usecache
}
private static path setbasework configuration conf  basework w  string hivescratchdir  string name  boolean usecache
try
setplanpath conf  hivescratchdir
path planpath   getplanpath conf  name
// use the default file system of the conf
filesystem fs   planpath getfilesystem conf
fsdataoutputstream out   fs create planpath
serializeplan w  out
// serialize the plan to the default hdfs instance
// except for hadoop local mode execution where we should be
// able to get the plan directly from the cache
if  usecache     shimloader gethadoopshims   islocalmode conf
// set up distributed cache
if   distributedcache getsymlink conf
distributedcache createsymlink conf
}
string uriwithlink   planpath touri   tostring         name
distributedcache addcachefile new uri uriwithlink   conf
// set replication of the plan file to a high number. we use the same
// replication factor as used by the hadoop jobclient for job.xml etc.
short replication    short  conf getint    10
fs setreplication planpath  replication
}
// cache the plan in this process
gworkmap put planpath  w
return planpath
catch  exception e
e printstacktrace
throw new runtimeexception e
}
}
private static path getplanpath configuration conf  string name
path planpath   getplanpath conf
if  planpath    null
return null
}
return new path planpath  name
}
private static void setplanpath configuration conf  string hivescratchdir  throws ioexception
if  getplanpath conf     null
// this is the unique conf id, which is kept in jobconf as part of the plan file name
string jobid   uuid randomuuid   tostring
path planpath   new path hivescratchdir  jobid
filesystem fs   planpath getfilesystem conf
fs mkdirs planpath
hiveconf setvar conf  hiveconf confvars plan  planpath touri   tostring
}
}
private static path getplanpath configuration conf
string plan   hiveconf getvar conf  hiveconf confvars plan
if  plan    null     plan isempty
return new path plan
}
return null
}
public static string serializeexpression exprnodedesc expr
bytearrayoutputstream baos   new bytearrayoutputstream
xmlencoder encoder   new xmlencoder baos
encoder setpersistencedelegate java sql date class  new datepersistencedelegate
encoder setpersistencedelegate timestamp class  new timestamppersistencedelegate
try
encoder writeobject expr
finally
encoder close
}
try
return baos tostring
catch  unsupportedencodingexception ex
throw new runtimeexception    ex
}
}
public static exprnodedesc deserializeexpression string s  configuration conf
byte bytes
try
bytes   s getbytes
catch  unsupportedencodingexception ex
throw new runtimeexception    ex
}
bytearrayinputstream bais   new bytearrayinputstream bytes
xmldecoder decoder   new xmldecoder bais  null  null
try
exprnodedesc expr    exprnodedesc  decoder readobject
return expr
finally
decoder close
}
}
public static class collectionpersistencedelegate extends defaultpersistencedelegate
@override
protected expression instantiate object oldinstance  encoder out
return new expression oldinstance  oldinstance getclass       null
}
@override
protected void initialize class type  object oldinstance  object newinstance  encoder out
iterator ite     collection  oldinstance  iterator
while  ite hasnext
out writestatement new statement oldinstance     new object  ite next
}
}
}
/**
* serializes the plan.
* @param plan the plan, such as queryplan, mapredwork, etc.
* @param out the stream to write to.
*/
public static void serializeplan object plan  outputstream out
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger serialize_plan
serializeobject plan  out
perflogger perflogend log  perflogger serialize_plan
}
/**
* deserializes the plan.
* @param in the stream to read from.
* @return the plan, such as queryplan, mapredwork, etc.
*/
public static <t> t deserializeplan inputstream in
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger deserialize_plan
t result   deserializeobject in
perflogger perflogend log  perflogger deserialize_plan
return result
}
/**
* clones using the powers of xml. do not use unless necessary.
* @param plan the plan.
* @return the clone.
*/
public static <t> t cloneplan t plan
// todo: need proper clone. meanwhiel, let's at least keep this horror in one place
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger clone_plan
bytearrayoutputstream baos   new bytearrayoutputstream
utilities serializeobject plan  baos
t copy   utilities deserializeobject new bytearrayinputstream baos tobytearray
perflogger perflogend log  perflogger clone_plan
return copy
}
/**
* serialize the object. this helper function mainly makes sure that enums,
* counters, etc are handled properly.
*/
public static void serializeobject object plan  outputstream out
xmlencoder e   new xmlencoder out
e setexceptionlistener new exceptionlistener
public void exceptionthrown exception e
log warn org apache hadoop util stringutils stringifyexception e
throw new runtimeexception    e
}
// workaround for java 1.5
e setpersistencedelegate expressiontypes class  new enumdelegate
e setpersistencedelegate groupbydesc mode class  new enumdelegate
e setpersistencedelegate operator progresscounter class  new enumdelegate
e setpersistencedelegate java sql date class  new datepersistencedelegate
e setpersistencedelegate timestamp class  new timestamppersistencedelegate
e setpersistencedelegate org datanucleus store types backed map class  new mapdelegate
e setpersistencedelegate org datanucleus store types backed list class  new listdelegate
e setpersistencedelegate org antlr runtime commontoken class  new commontokendelegate
e writeobject plan
e close
}
/**
* de-serialize an object. this helper function mainly makes sure that enums,
* counters, etc are handled properly.
*/
@suppresswarnings
public static <t> t deserializeobject inputstream in
xmldecoder d   null
try
d   new xmldecoder in  null  null
return  t  d readobject
finally
if  null    d
d close
}
}
}
public static tabledesc defaulttd
static
// by default we expect ^a separated strings
// this tabledesc does not provide column names. we should always use
// planutils.getdefaulttabledesc(string separatorcode, string columns)
// or getbinarysortabletabledesc(list<fieldschema> fieldschemas) when
// we know the column names.
defaulttd   planutils getdefaulttabledesc     utilities ctrlacode
}
public static final int carriagereturncode   13
public static final int newlinecode   10
public static final int tabcode   9
public static final int ctrlacode   1
public static final string indent
// note: when ddl supports specifying what string to represent null,
// we should specify "null" to represent null in the temp table, and then
// we can make the following translation deprecated.
public static string nullstringstorage
public static string nullstringoutput
public static random randgen   new random
/**
* gets the task id if we are running as a hadoop job. gets a random number otherwise.
*/
public static string gettaskid configuration hconf
string taskid    hconf    null  ? null   hconf get
if   taskid    null     taskid equals
return      math abs randgen nextint
} else {
/*
* extract the task and attempt id from the hadoop taskid. in version 17 the leading component
* was 'task_'. thereafter the leading component is 'attempt_'. in 17 - hadoop also seems to
* have used _map_ and _reduce_ to denote map/reduce task types
*/
string ret   taskid replaceall       replaceall
return  ret
}
}
public static hashmap makemap object    olist
hashmap ret   new hashmap
for  int i   0  i < olist length  i    2
ret put olist  olist
}
return  ret
}
public static properties makeproperties string    olist
properties ret   new properties
for  int i   0  i < olist length  i    2
ret setproperty olist  olist
}
return  ret
}
public static arraylist makelist object    olist
arraylist ret   new arraylist
for  object element   olist
ret add element
}
return  ret
}
/**
* streamprinter.
*
*/
public static class streamprinter extends thread
inputstream is
string type
printstream os
public streamprinter inputstream is  string type  printstream os
this is   is
this type   type
this os   os
}
@override
public void run
bufferedreader br   null
try
inputstreamreader isr   new inputstreamreader is
br   new bufferedreader isr
string line   null
if  type    null
while   line   br readline       null
os println type       line
}
} else {
while   line   br readline       null
os println line
}
}
br close
br null
catch  ioexception ioe
ioe printstacktrace
finally
ioutils closestream br
}
}
}
public static tabledesc gettabledesc table tbl
return  new tabledesc tbl getdeserializer   getclass    tbl getinputformatclass    tbl
getoutputformatclass    tbl getmetadata
}
// column names and column types are all delimited by comma
public static tabledesc gettabledesc string cols  string coltypes
return  new tabledesc lazysimpleserde class  sequencefileinputformat class
hivesequencefileoutputformat class  utilities makeproperties
org apache hadoop hive serde serdeconstants serialization_format      utilities ctrlacode
org apache hadoop hive serde serdeconstants list_columns  cols
org apache hadoop hive serde serdeconstants list_column_types  coltypes
}
public static partitiondesc getpartitiondesc partition part  throws hiveexception
return  new partitiondesc part
}
public static partitiondesc getpartitiondescfromtabledesc tabledesc tbldesc  partition part
throws hiveexception
return new partitiondesc part  tbldesc
}
private static string getoptreeskel_helper operator<?> op  string indent
if  op    null
return
}
stringbuilder sb   new stringbuilder
sb append indent
sb append op tostring
sb append
if  op getchildoperators      null
for  object child   op getchildoperators
sb append getoptreeskel_helper  operator<?>  child  indent
}
}
return sb tostring
}
public static string getoptreeskel operator<?> op
return getoptreeskel_helper op
}
private static boolean iswhitespace int c
if  c     1
return false
}
return character iswhitespace  char  c
}
public static boolean contentsequal inputstream is1  inputstream is2  boolean ignorewhitespace
throws ioexception
try
if   is1    is2      is1    null    is2    null
return true
}
if  is1    null    is2    null
return false
}
while  true
int c1   is1 read
while  ignorewhitespace    iswhitespace c1
c1   is1 read
}
int c2   is2 read
while  ignorewhitespace    iswhitespace c2
c2   is2 read
}
if  c1     1    c2     1
return true
}
if  c1    c2
break
}
}
catch  filenotfoundexception e
e printstacktrace
}
return false
}
/**
* convert "from src insert blah blah" to "from src insert ... blah"
*/
public static string abbreviate string str  int max
str   str trim
int len   str length
int suffixlength   20
if  len <  max
return str
}
suffixlength   math min suffixlength   max   3    2
string rev   stringutils reverse str
// get the last few words
string suffix   wordutils abbreviate rev  0  suffixlength
suffix   stringutils reverse suffix
// first few ..
string prefix   stringutils abbreviate str  max   suffix length
return prefix   suffix
}
public static final string nstr
/**
* streamstatus.
*
*/
public static enum streamstatus
eof  terminated
}
public static streamstatus readcolumn datainput in  outputstream out  throws ioexception
boolean foundcrchar   false
while  true
int b
try
b   in readbyte
catch  eofexception e
return streamstatus eof
}
// default new line characters on windows are "crlf" so detect if there are any windows
// native newline characters and handle them.
if  shell windows
// if the cr is not followed by the lf on windows then add it back to the stream and
// proceed with next characters in the input stream.
if  foundcrchar    b    utilities newlinecode
out write utilities carriagereturncode
foundcrchar   false
}
if  b    utilities carriagereturncode
foundcrchar   true
continue
}
}
if  b    utilities newlinecode
return streamstatus terminated
}
out write b
}
// unreachable
}
/**
* convert an output stream to a compressed output stream based on codecs and compression options
* specified in the job configuration.
*
* @param jc
*          job configuration
* @param out
*          output stream to be converted into compressed output stream
* @return compressed output stream
*/
public static outputstream createcompressedstream jobconf jc  outputstream out
throws ioexception
boolean iscompressed   fileoutputformat getcompressoutput jc
return createcompressedstream jc  out  iscompressed
}
/**
* convert an output stream to a compressed output stream based on codecs codecs in the job
* configuration. caller specifies directly whether file is compressed or not
*
* @param jc
*          job configuration
* @param out
*          output stream to be converted into compressed output stream
* @param iscompressed
*          whether the output stream needs to be compressed or not
* @return compressed output stream
*/
public static outputstream createcompressedstream jobconf jc  outputstream out
boolean iscompressed  throws ioexception
if  iscompressed
class<? extends compressioncodec> codecclass   fileoutputformat getoutputcompressorclass jc
defaultcodec class
compressioncodec codec    compressioncodec  reflectionutils newinstance codecclass  jc
return codec createoutputstream out
} else {
return  out
}
}
/**
* based on compression option and configured output codec - get extension for output file. this
* is only required for text files - not sequencefiles
*
* @param jc
*          job configuration
* @param iscompressed
*          whether the output file is compressed or not
* @return the required file extension (example: .gz)
* @deprecated use {@link #getfileextension(jobconf, boolean, hiveoutputformat)}
*/
@deprecated
public static string getfileextension jobconf jc  boolean iscompressed
return getfileextension jc  iscompressed  new hiveignorekeytextoutputformat
}
/**
* based on compression option, output format, and configured output codec -
* get extension for output file. text files require an extension, whereas
* others, like sequence files, do not.
* <p>
* the property <code>hive.output.file.extension</code> is used to determine
* the extension - if set, it will override other logic for choosing an
* extension.
*
* @param jc
*          job configuration
* @param iscompressed
*          whether the output file is compressed or not
* @param hiveoutputformat
*          the output format, used to detect if the format is text
* @return the required file extension (example: .gz)
*/
public static string getfileextension jobconf jc  boolean iscompressed
hiveoutputformat<?  ?> hiveoutputformat
string extension   hiveconf getvar jc  hiveconf confvars output_file_extension
if   stringutils isempty extension
return extension
}
if   hiveoutputformat instanceof hiveignorekeytextoutputformat     iscompressed
class<? extends compressioncodec> codecclass   fileoutputformat getoutputcompressorclass jc
defaultcodec class
compressioncodec codec    compressioncodec  reflectionutils newinstance codecclass  jc
return codec getdefaultextension
}
return
}
/**
* create a sequencefile output stream based on job configuration.
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @param keyclass
*          java class for key
* @param valclass
*          java class for value
* @return output stream over the created sequencefile
*/
public static sequencefile writer createsequencewriter jobconf jc  filesystem fs  path file
class<?> keyclass  class<?> valclass  throws ioexception
boolean iscompressed   fileoutputformat getcompressoutput jc
return createsequencewriter jc  fs  file  keyclass  valclass  iscompressed
}
/**
* create a sequencefile output stream based on job configuration uses user supplied compression
* flag (rather than obtaining it from the job configuration).
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @param keyclass
*          java class for key
* @param valclass
*          java class for value
* @return output stream over the created sequencefile
*/
public static sequencefile writer createsequencewriter jobconf jc  filesystem fs  path file
class<?> keyclass  class<?> valclass  boolean iscompressed  throws ioexception
compressioncodec codec   null
compressiontype compressiontype   compressiontype none
class codecclass   null
if  iscompressed
compressiontype   sequencefileoutputformat getoutputcompressiontype jc
codecclass   fileoutputformat getoutputcompressorclass jc  defaultcodec class
codec    compressioncodec  reflectionutils newinstance codecclass  jc
}
return  sequencefile createwriter fs  jc  file  keyclass  valclass  compressiontype  codec
}
/**
* create a rcfile output stream based on job configuration uses user supplied compression flag
* (rather than obtaining it from the job configuration).
*
* @param jc
*          job configuration
* @param fs
*          file system to create file in
* @param file
*          path to be created
* @return output stream over the created rcfile
*/
public static rcfile writer creatercfilewriter jobconf jc  filesystem fs  path file
boolean iscompressed  throws ioexception
compressioncodec codec   null
class<?> codecclass   null
if  iscompressed
codecclass   fileoutputformat getoutputcompressorclass jc  defaultcodec class
codec    compressioncodec  reflectionutils newinstance codecclass  jc
}
return new rcfile writer fs  jc  file  null  codec
}
/**
* shamelessly cloned from genericoptionsparser.
*/
public static string realfile string newfile  configuration conf  throws ioexception
path path   new path newfile
uri pathuri   path touri
filesystem fs
if  pathuri getscheme      null
fs   filesystem getlocal conf
} else {
fs   path getfilesystem conf
}
if   fs exists path
return null
}
string file   path makequalified fs  tostring
// for compatibility with hadoop 0.17, change file:/a/b/c to file:///a/b/c
if  stringutils startswith file         stringutils startswith file
file       file substring   length
}
return file
}
public static list<string> mergeuniqelems list<string> src  list<string> dest
if  dest    null
return src
}
if  src    null
return dest
}
int pos   0
while  pos < dest size
if   src contains dest get pos
src add dest get pos
}
pos
}
return src
}
private static final string tmpprefix
private static final string tasktmpprefix
public static path totasktemppath path orig
if  orig getname   indexof tasktmpprefix     0
return orig
}
return new path orig getparent    tasktmpprefix   orig getname
}
public static path totasktemppath string orig
return totasktemppath new path orig
}
public static path totemppath path orig
if  orig getname   indexof tmpprefix     0
return orig
}
return new path orig getparent    tmpprefix   orig getname
}
/**
* given a path, convert to a temporary path.
*/
public static path totemppath string orig
return totemppath new path orig
}
/**
* detect if the supplied file is a temporary path.
*/
public static boolean istemppath filestatus file
string name   file getpath   getname
// in addition to detecting hive temporary files, we also check hadoop
// temporary folders that used to show up in older releases
return  name startswith       name startswith tmpprefix
}
/**
* rename src to dst, or in the case dst already exists, move files in src to dst. if there is an
* existing file with the same name, the new file's name will be appended with "_1", "_2", etc.
*
* @param fs
*          the filesystem where src and dst are on.
* @param src
*          the src directory
* @param dst
*          the target directory
* @throws ioexception
*/
public static void rename filesystem fs  path src  path dst  throws ioexception  hiveexception
if   fs rename src  dst
throw new hiveexception     src       dst
}
}
/**
* rename src to dst, or in the case dst already exists, move files in src to dst. if there is an
* existing file with the same name, the new file's name will be appended with "_1", "_2", etc.
*
* @param fs
*          the filesystem where src and dst are on.
* @param src
*          the src directory
* @param dst
*          the target directory
* @throws ioexception
*/
public static void renameormovefiles filesystem fs  path src  path dst  throws ioexception
hiveexception {
if   fs exists dst
if   fs rename src  dst
throw new hiveexception     src       dst
}
} else {
// move file by file
filestatus files   fs liststatus src
for  filestatus file   files
path srcfilepath   file getpath
string filename   srcfilepath getname
path dstfilepath   new path dst  filename
if  file isdir
renameormovefiles fs  srcfilepath  dstfilepath
}
else
if  fs exists dstfilepath
int suffix   0
do
suffix
dstfilepath   new path dst  filename       suffix
while  fs exists dstfilepath
}
if   fs rename srcfilepath  dstfilepath
throw new hiveexception     src       dst
}
}
}
}
}
/**
* the first group will contain the task id. the second group is the optional extension. the file
* name looks like: "0_0" or "0_0.gz". there may be a leading prefix (tmp_). since gettaskid() can
* return an integer only - this should match a pure integer as well. {1,3} is used to limit
* matching for attempts #'s 0-999.
*/
private static final pattern file_name_to_task_id_regex
pattern compile
/**
* this retruns prefix part + taskid for bucket join for partitioned table
*/
private static final pattern file_name_prefixed_task_id_regex
pattern compile
/**
* this breaks a prefixed bucket number into the prefix and the taskid
*/
private static final pattern prefixed_task_id_regex
pattern compile
/**
* get the task id from the filename. it is assumed that the filename is derived from the output
* of gettaskid
*
* @param filename
*          filename to extract taskid from
*/
public static string gettaskidfromfilename string filename
return getidfromfilename filename  file_name_to_task_id_regex
}
/**
* get the part-spec + task id from the filename. it is assumed that the filename is derived
* from the output of gettaskid
*
* @param filename
*          filename to extract taskid from
*/
public static string getprefixedtaskidfromfilename string filename
return getidfromfilename filename  file_name_prefixed_task_id_regex
}
private static string getidfromfilename string filename  pattern pattern
string taskid   filename
int dirend   filename lastindexof path separator
if  dirend     1
taskid   filename substring dirend   1
}
matcher m   pattern matcher taskid
if   m matches
log warn     filename
taskid
} else {
taskid   m group 1
}
log debug     filename       taskid
return taskid
}
public static string getfilenamefromdirname string dirname
int dirend   dirname lastindexof path separator
if  dirend     1
return dirname substring dirend   1
}
return dirname
}
/**
* replace the task id from the filename. it is assumed that the filename is derived from the
* output of gettaskid
*
* @param filename
*          filename to replace taskid "0_0" or "0_0.gz" by 33 to "33_0" or "33_0.gz"
*/
public static string replacetaskidfromfilename string filename  int bucketnum
return replacetaskidfromfilename filename  string valueof bucketnum
}
public static string replacetaskidfromfilename string filename  string fileid
string taskid   gettaskidfromfilename filename
string newtaskid   replacetaskid taskid  fileid
string ret   replacetaskidfromfilename filename  taskid  newtaskid
return  ret
}
private static string replacetaskid string taskid  int bucketnum
return replacetaskid taskid  string valueof bucketnum
}
/**
* returns strbucketnum with enough 0's prefixing the task id portion of the string to make it
* equal in length to taskid
*
* @param taskid - the taskid used as a template for length
* @param strbucketnum - the bucket number of the output, may or may not be prefixed
* @return
*/
private static string replacetaskid string taskid  string strbucketnum
matcher m   prefixed_task_id_regex matcher strbucketnum
if   m matches
log warn     strbucketnum
return adjustbucketnumlen strbucketnum  taskid
} else {
string adjustedbucketnum   adjustbucketnumlen m group 2   taskid
return  m group 1     null ?     m group 1     adjustedbucketnum
}
}
/**
* adds 0's to the beginning of bucketnum until bucketnum and taskid are the same length.
*
* @param bucketnum - the bucket number, should not be prefixed
* @param taskid - the taskid used as a template for length
* @return
*/
private static string adjustbucketnumlen string bucketnum  string taskid
int bucketnumlen   bucketnum length
int taskidlen   taskid length
stringbuffer s   new stringbuffer
for  int i   0  i < taskidlen   bucketnumlen  i
s append
}
return s tostring     bucketnum
}
/**
* replace the oldtaskid appearing in the filename by the newtaskid. the string oldtaskid could
* appear multiple times, we should only replace the last one.
*
* @param filename
* @param oldtaskid
* @param newtaskid
* @return
*/
private static string replacetaskidfromfilename string filename  string oldtaskid
string newtaskid
string spl   filename split oldtaskid
if   spl length    0      spl length    1
return filename replaceall oldtaskid  newtaskid
}
stringbuffer snew   new stringbuffer
for  int idx   0  idx < spl length   1  idx
if  idx > 0
snew append oldtaskid
}
snew append spl
}
snew append newtaskid
snew append spl
return snew tostring
}
/**
* returns null if path is not exist
*/
public static filestatus liststatusifexists path path  filesystem fs  throws ioexception
try
return fs liststatus path
catch  filenotfoundexception e
// fs in hadoop 2.0 throws fnf instead of returning null
return null
}
}
/**
* get all file status from a root path and recursively go deep into certain levels.
*
* @param path
*          the root path
* @param level
*          the depth of directory should explore
* @param fs
*          the file system
* @return array of filestatus
* @throws ioexception
*/
public static filestatus getfilestatusrecurse path path  int level  filesystem fs
throws ioexception
// construct a path pattern (e.g., /*/*) to find all dynamically generated paths
stringbuilder sb = new stringbuilder(path.touri().getpath());
for (int i = 0; i < level; ++i) {
sb.append(path.separator).append("*");
}
path pathpattern = new path(path, sb.tostring());
return fs.globstatus(pathpattern);
}
public static void mvfiletofinalpath(string specpath, configuration hconf,
boolean success, log log, dynamicpartitionctx dpctx, filesinkdesc conf,
reporter reporter) throws ioexception,
hiveexception {
filesystem fs = (new path(specpath)).getfilesystem(hconf);
path tmppath = utilities.totemppath(specpath);
path tasktmppath = utilities.totasktemppath(specpath);
path intermediatepath = new path(tmppath.getparent(), tmppath.getname()
+ ".intermediate");
path finalpath = new path(specpath);
if (success) {
if (fs.exists(tmppath)) {
// step1: rename tmp output folder to intermediate path. after this
// point, updates from speculative tasks still writing to tmppath
// will not appear in finalpath.
log.info("moving tmp dir: " + tmppath + " to: " + intermediatepath);
utilities.rename(fs, tmppath, intermediatepath);
// step2: remove any tmp file or double-committed output files
arraylist<string> emptybuckets =
utilities.removetemporduplicatefiles(fs, intermediatepath, dpctx);
// create empty buckets if necessary
if (emptybuckets.size() > 0) {
createemptybuckets(hconf, emptybuckets, conf, reporter);
}
// step3: move to the file destination
log.info("moving tmp dir: " + intermediatepath + " to: " + finalpath);
utilities.renameormovefiles(fs, intermediatepath, finalpath);
}
} else {
fs.delete(tmppath, true);
}
fs.delete(tasktmppath, true);
}
/**
* check the existence of buckets according to bucket specification. create empty buckets if
* needed.
*
* @param hconf
* @param paths a list of empty buckets to create
* @param conf the definition of the filesink.
* @param reporter the mapreduce reporter object
* @throws hiveexception
* @throws ioexception
*/
private static void createemptybuckets configuration hconf  arraylist<string> paths
filesinkdesc conf  reporter reporter
throws hiveexception  ioexception
jobconf jc
if  hconf instanceof jobconf
jc   new jobconf hconf
} else {
// test code path
jc   new jobconf hconf
}
hiveoutputformat<?  ?> hiveoutputformat   null
class<? extends writable> outputclass   null
boolean iscompressed   conf getcompressed
tabledesc tableinfo   conf gettableinfo
try
serializer serializer    serializer  tableinfo getdeserializerclass   newinstance
serializer initialize null  tableinfo getproperties
outputclass   serializer getserializedclass
hiveoutputformat   conf gettableinfo   getoutputfileformatclass   newinstance
catch  serdeexception e
throw new hiveexception e
catch  instantiationexception e
throw new hiveexception e
catch  illegalaccessexception e
throw new hiveexception e
}
for  string p   paths
path path   new path p
recordwriter writer   hivefileformatutils getrecordwriter
jc  hiveoutputformat  outputclass  iscompressed
tableinfo getproperties    path  reporter
writer close false
log info     path
}
}
/**
* remove all temporary files and duplicate (double-committed) files from a given directory.
*/
public static void removetemporduplicatefiles filesystem fs  path path  throws ioexception
removetemporduplicatefiles fs  path  null
}
/**
* remove all temporary files and duplicate (double-committed) files from a given directory.
*
* @return a list of path names corresponding to should-be-created empty buckets.
*/
public static arraylist<string> removetemporduplicatefiles filesystem fs  path path
dynamicpartitionctx dpctx  throws ioexception
if  path    null
return null
}
arraylist<string> result   new arraylist<string>
if  dpctx    null
filestatus parts   getfilestatusrecurse path  dpctx getnumdpcols    fs
hashmap<string  filestatus> taskidtofile   null
for  int i   0  i < parts length    i
assert parts isdir         parts getpath
filestatus items   fs liststatus parts getpath
// remove empty directory since dp insert should not generate empty partitions.
// empty directories could be generated by crashed task/scriptoperator
if  items length    0
if   fs delete parts getpath    true
log error     parts getpath
throw new ioexception     parts getpath
}
}
taskidtofile   removetemporduplicatefiles items  fs
// if the table is bucketed and enforce bucketing, we should check and generate all buckets
if  dpctx getnumbuckets   > 0    taskidtofile    null
// refresh the file list
items   fs liststatus parts getpath
// get the missing buckets and generate empty buckets
string taskid1   taskidtofile keyset   iterator   next
path bucketpath   taskidtofile values   iterator   next   getpath
for  int j   0  j < dpctx getnumbuckets      j
string taskid2   replacetaskid taskid1  j
if   taskidtofile containskey taskid2
// create empty bucket, file name should be derived from taskid2
string path2   replacetaskidfromfilename bucketpath touri   getpath   tostring    j
result add path2
}
}
}
}
} else {
filestatus items   fs liststatus path
removetemporduplicatefiles items  fs
}
return result
}
public static hashmap<string  filestatus> removetemporduplicatefiles filestatus items
filesystem fs  throws ioexception
if  items    null    fs    null
return null
}
hashmap<string  filestatus> taskidtofile   new hashmap<string  filestatus>
for  filestatus one   items
if  istemppath one
if   fs delete one getpath    true
throw new ioexception     one getpath
}
} else {
string taskid   getprefixedtaskidfromfilename one getpath   getname
filestatus otherfile   taskidtofile get taskid
if  otherfile    null
taskidtofile put taskid  one
} else {
// compare the file sizes of all the attempt files for the same task, the largest win
// any attempt files could contain partial results (due to task failures or
// speculative runs), but the largest should be the correct one since the result
// of a successful run should never be smaller than a failed/speculative run.
filestatus todelete   null
if  otherfile getlen   >  one getlen
todelete   one
} else {
todelete   otherfile
taskidtofile put taskid  one
}
long len1   todelete getlen
long len2   taskidtofile get taskid  getlen
if   fs delete todelete getpath    true
throw new ioexception     todelete getpath
taskidtofile get taskid  getpath
} else {
log warn     todelete getpath
len1       taskidtofile get taskid  getpath
len2
}
}
}
}
return taskidtofile
}
public static string getnamemessage exception e
return e getclass   getname         e getmessage
}
public static string getresourcefiles configuration conf  sessionstate resourcetype t
// fill in local files to be added to the task environment
sessionstate ss   sessionstate get
set<string> files    ss    null  ? null   ss list_resource t  null
if  files    null
list<string> realfiles   new arraylist<string> files size
for  string one   files
try
realfiles add realfile one  conf
catch  ioexception e
throw new runtimeexception     one
e getmessage    e
}
}
return stringutils join realfiles
} else {
return
}
}
/**
* add new elements to the classpath.
*
* @param newpaths
*          array of classpath elements
*/
public static classloader addtoclasspath classloader cloader  string newpaths  throws exception
urlclassloader loader    urlclassloader  cloader
list<url> curpath   arrays aslist loader geturls
arraylist<url> newpath   new arraylist<url>
// get a list with the current classpath components
for  url onepath   curpath
newpath add onepath
}
curpath   newpath
for  string onestr   newpaths
// special processing for hadoop-17. file:// needs to be removed
if  stringutils indexof onestr        0
onestr   stringutils substring onestr  7
}
url oneurl    new file onestr   tourl
if   curpath contains oneurl
curpath add oneurl
}
}
return new urlclassloader curpath toarray new url   loader
}
/**
* remove elements from the classpath.
*
* @param pathstoremove
*          array of classpath elements
*/
public static void removefromclasspath string pathstoremove  throws exception
thread curthread   thread currentthread
urlclassloader loader    urlclassloader  curthread getcontextclassloader
set<url> newpath   new hashset<url> arrays aslist loader geturls
for  string onestr   pathstoremove
// special processing for hadoop-17. file:// needs to be removed
if  stringutils indexof onestr        0
onestr   stringutils substring onestr  7
}
url oneurl    new file onestr   tourl
newpath remove oneurl
}
loader   new urlclassloader newpath toarray new url
curthread setcontextclassloader loader
sessionstate get   getconf   setclassloader loader
}
public static string formatbinarystring byte array  int start  int length
stringbuilder sb   new stringbuilder
for  int i   start  i < start   length  i
sb append
sb append array < 0 ? array   256   array   0
}
return sb tostring
}
public static list<string> getcolumnnamesfromsortcols list<order> sortcols
list<string> names   new arraylist<string>
for  order o   sortcols
names add o getcol
}
return names
}
public static list<string> getcolumnnamesfromfieldschema list<fieldschema> partcols
list<string> names   new arraylist<string>
for  fieldschema o   partcols
names add o getname
}
return names
}
public static list<string> getcolumnnames properties props
list<string> names   new arraylist<string>
string colnames   props getproperty serdeconstants list_columns
string cols   colnames trim   split
if  cols    null
for  string col   cols
if  col    null     col trim   equals
names add col
}
}
}
return names
}
public static list<string> getcolumntypes properties props
list<string> names   new arraylist<string>
string colnames   props getproperty serdeconstants list_column_types
string cols   colnames trim   split
if  cols    null
for  string col   cols
if  col    null     col trim   equals
names add col
}
}
}
return names
}
public static void validatecolumnnames list<string> colnames  list<string> checkcols
throws semanticexception
iterator<string> checkcolsiter   checkcols iterator
while  checkcolsiter hasnext
string tocheck   checkcolsiter next
boolean found   false
iterator<string> colnamesiter   colnames iterator
while  colnamesiter hasnext
string colname   colnamesiter next
if  tocheck equalsignorecase colname
found   true
break
}
}
if   found
throw new semanticexception errormsg invalid_column getmsg
}
}
}
/**
* gets the default notification interval to send progress updates to the tracker. useful for
* operators that may not output data for a while.
*
* @param hconf
* @return the interval in milliseconds
*/
public static int getdefaultnotificationinterval configuration hconf
int notificationinterval
integer expinterval   integer decode hconf get
if  expinterval    null
notificationinterval   expinterval intvalue     2
} else {
// 5 minutes
notificationinterval   5   60   1000
}
return notificationinterval
}
/**
* copies the storage handler properties configured for a table descriptor to a runtime job
* configuration.
*
* @param tbl
*          table descriptor from which to read
*
* @param job
*          configuration which receives configured properties
*/
public static void copytablejobpropertiestoconf tabledesc tbl  jobconf job
map<string  string> jobproperties   tbl getjobproperties
if  jobproperties    null
return
}
for  map entry<string  string> entry   jobproperties entryset
job set entry getkey    entry getvalue
}
}
public static object input_summary_lock   new object
/**
* calculate the total size of input files.
*
* @param ctx
*          the hadoop job context
* @param work
*          map reduce job plan
* @param filter
*          filter to apply to the input paths before calculating size
* @return the summary of all the input paths.
* @throws ioexception
*/
public static contentsummary getinputsummary context ctx  mapwork work  pathfilter filter
throws ioexception
perflogger perflogger   perflogger getperflogger
perflogger perflogbegin log  perflogger input_summary
long summary    0  0  0
list<string> pathneedprocess   new arraylist<string>
// since multiple threads could call this method concurrently, locking
// this method will avoid number of threads out of control.
synchronized  input_summary_lock
// for each input path, calculate the total size.
for  string path   work getpathtoaliases   keyset
path p   new path path
if  filter    null     filter accept p
continue
}
contentsummary cs   ctx getcs path
if  cs    null
if  path    null
continue
}
pathneedprocess add path
} else {
summary    cs getlength
summary    cs getfilecount
summary    cs getdirectorycount
}
}
// process the case when name node call is needed
final map<string  contentsummary> resultmap   new concurrenthashmap<string  contentsummary>
arraylist<future<?>> results   new arraylist<future<?>>
final threadpoolexecutor executor
int maxthreads   ctx getconf   getint    0
if  pathneedprocess size   > 1    maxthreads > 1
int numexecutors   math min pathneedprocess size    maxthreads
log info     numexecutors
executor   new threadpoolexecutor numexecutors  numexecutors  60  timeunit seconds
new linkedblockingqueue<runnable>
} else {
executor   null
}
hiveinterruptcallback interrup   hiveinterruptutils add new hiveinterruptcallback
@override
public void interrupt
if  executor    null
executor shutdownnow
}
}
try
configuration conf   ctx getconf
jobconf jobconf   new jobconf conf
for  string path   pathneedprocess
final path p   new path path
final string pathstr   path
// all threads share the same configuration and jobconf based on the
// assumption that they are thread safe if only read operations are
// executed. it is not stated in hadoop's javadoc, the sourcce codes
// clearly showed that they made efforts for it and we believe it is
// thread safe. will revisit this piece of codes if we find the assumption
// is not correct.
final configuration myconf   conf
final jobconf myjobconf   jobconf
final partitiondesc partdesc   work getpathtopartitioninfo   get
p tostring
runnable r   new runnable
public void run
try
contentsummary resultcs
class<? extends inputformat> inputformatcls   partdesc
getinputfileformatclass
inputformat inputformatobj   hiveinputformat getinputformatfromcache
inputformatcls  myjobconf
if  inputformatobj instanceof contentsummaryinputformat
resultcs     contentsummaryinputformat  inputformatobj  getcontentsummary p
myjobconf
} else {
filesystem fs   p getfilesystem myconf
resultcs   fs getcontentsummary p
}
resultmap put pathstr  resultcs
catch  ioexception e
// we safely ignore this exception for summary data.
// we don't update the cache to protect it from polluting other
// usages. the worst case is that ioexception will always be
// retried for another getinputsummary(), which is fine as
// ioexception is not considered as a common case.
log info     pathstr
}
}
if  executor    null
r run
} else {
future<?> result   executor submit r
results add result
}
}
if  executor    null
for  future<?> result   results
boolean executordone   false
do
try
result get
executordone   true
catch  interruptedexception e
log info    e
thread currentthread   interrupt
break
catch  executionexception e
throw new ioexception e
}
while   executordone
}
executor shutdown
}
hiveinterruptutils checkinterrupted
for  map entry<string  contentsummary> entry   resultmap entryset
contentsummary cs   entry getvalue
summary    cs getlength
summary    cs getfilecount
summary    cs getdirectorycount
ctx addcs entry getkey    cs
log info     entry getkey         cs getlength
cs getfilecount         cs getdirectorycount
}
perflogger perflogend log  perflogger input_summary
return new contentsummary summary  summary  summary
finally
hiveinterruptutils remove interrup
}
}
}
public static boolean isemptypath jobconf job  path dirpath  context ctx
throws exception
contentsummary cs   ctx getcs dirpath
if  cs    null
log info     dirpath       cs getlength
cs getfilecount         cs getdirectorycount
return  cs getlength      0    cs getfilecount      0    cs getdirectorycount   <  1
} else {
log info     dirpath
}
return isemptypath job  dirpath
}
public static boolean isemptypath jobconf job  path dirpath  throws exception
filesystem inpfs   dirpath getfilesystem job
if  inpfs exists dirpath
filestatus fstats   inpfs liststatus dirpath
if  fstats length > 0
return false
}
}
return true
}
public static list<execdriver> getmrtasks list<task<? extends serializable>> tasks
list<execdriver> mrtasks   new arraylist<execdriver>
if  tasks    null
getmrtasks tasks  mrtasks
}
return mrtasks
}
private static void getmrtasks list<task<? extends serializable>> tasks  list<execdriver> mrtasks
for  task<? extends serializable> task   tasks
if  task instanceof execdriver     mrtasks contains  execdriver  task
mrtasks add  execdriver  task
}
if  task getdependenttasks      null
getmrtasks task getdependenttasks    mrtasks
}
}
}
/**
* construct a list of full partition spec from dynamic partition context and the directory names
* corresponding to these dynamic partitions.
*/
public static list<linkedhashmap<string  string>> getfulldpspecs configuration conf
dynamicpartitionctx dpctx  throws hiveexception
try
path loadpath   new path dpctx getrootpath
filesystem fs   loadpath getfilesystem conf
int numdpcols   dpctx getnumdpcols
filestatus status   utilities getfilestatusrecurse loadpath  numdpcols  fs
if  status length    0
log warn
return null
}
// partial partition specification
map<string  string> partspec   dpctx getpartspec
// list of full partition specification
list<linkedhashmap<string  string>> fullpartspecs   new arraylist<linkedhashmap<string  string>>
// for each dynamically created dp directory, construct a full partition spec
// and load the partition based on that
for  int i   0  i < status length    i
// get the dynamically created directory
path partpath   status getpath
assert fs getfilestatus partpath  isdir         partpath
// generate a full partition specification
linkedhashmap<string  string> fullpartspec   new linkedhashmap<string  string> partspec
warehouse makespecfromname fullpartspec  partpath
fullpartspecs add fullpartspec
}
return fullpartspecs
catch  ioexception e
throw new hiveexception e
}
}
public static statspublisher getstatspublisher jobconf jc
string statsimplementationclass   hiveconf getvar jc  hiveconf confvars hivestatsdbclass
if  statsfactory setimplementation statsimplementationclass  jc
return statsfactory getstatspublisher
} else {
return null
}
}
/**
* if statsprefix's length is greater than maxprefixlength and maxprefixlength > 0,
* then it returns an md5 hash of statsprefix followed by path separator, otherwise
* it returns statsprefix
*
* @param statsprefix
* @param maxprefixlength
* @return
*/
public static string gethashedstatsprefix string statsprefix  int maxprefixlength
string ret   statsprefix
if  maxprefixlength >  0    statsprefix length   > maxprefixlength
try
messagedigest digester   messagedigest getinstance
digester update statsprefix getbytes
ret   new string digester digest      path separator
catch  nosuchalgorithmexception e
throw new runtimeexception e
}
}
return ret
}
public static void setcolumnnamelist jobconf jobconf  operator op
rowschema rowschema   op getschema
if  rowschema    null
return
}
stringbuilder columnnames   new stringbuilder
for  columninfo colinfo   rowschema getsignature
if  columnnames length   > 0
columnnames append
}
columnnames append colinfo getinternalname
}
string columnnamesstring   columnnames tostring
jobconf set serdeconstants list_columns  columnnamesstring
}
public static void setcolumntypelist jobconf jobconf  operator op
rowschema rowschema   op getschema
if  rowschema    null
return
}
stringbuilder columntypes   new stringbuilder
for  columninfo colinfo   rowschema getsignature
if  columntypes length   > 0
columntypes append
}
columntypes append colinfo gettype   gettypename
}
string columntypesstring   columntypes tostring
jobconf set serdeconstants list_column_types  columntypesstring
}
public static void validatepartspec table tbl  map<string  string> partspec
throws semanticexception
list<fieldschema> parts   tbl getpartitionkeys
set<string> partcols   new hashset<string> parts size
for  fieldschema col   parts
partcols add col getname
}
for  string col   partspec keyset
if   partcols contains col
throw new semanticexception errormsg nonexistpartcol getmsg col
}
}
}
public static string suffix
public static string generatepath string baseuri  string dumpfileprefix
byte tag  string bigbucketfilename
string path   new string baseuri   path separator       dumpfileprefix   tag
bigbucketfilename   suffix
return path
}
public static string generatefilename byte tag  string bigbucketfilename
string filename   new string     tag       bigbucketfilename   suffix
return filename
}
public static string generatetmpuri string baseuri  string id
string tmpfileuri   new string baseuri   path separator       id
return tmpfileuri
}
public static string generatetaruri string baseuri  string filename
string tmpfileuri   new string baseuri   path separator   filename
return tmpfileuri
}
public static string generatetaruri path baseuri  string filename
string tmpfileuri   new string baseuri   path separator   filename
return tmpfileuri
}
public static string generatetarfilename string name
string tmpfileuri   new string name
return tmpfileuri
}
public static string generatepath path baseuri  string filename
string path   new string baseuri   path separator   filename
return path
}
public static string now
calendar cal   calendar getinstance
simpledateformat sdf   new simpledateformat
return sdf format cal gettime
}
public static double showtime long time
double result    double  time    double  1000
return result
}
/**
* check if a function can be pushed down to jdo.
* now only {compares, and, or} are supported.
* @param func a generic function.
* @return true if this function can be pushed down to jdo filter.
*/
private static boolean supportedjdofuncs genericudf func
// todo: we might also want to add "not" and "between" here in future.
// todo: change to genericudfbasecompare once dn is upgraded
//       (see hive-2609 - in dn 2.0, substrings do not work in mysql).
return func instanceof genericudfopequal
func instanceof genericudfopnotequal
func instanceof genericudfopand
func instanceof genericudfopor
}
/**
* check if a function can be pushed down to jdo for integral types.
* only {=, !=} are supported. lt/gt/etc. to be dealt with in hive-4888.
* @param func a generic function.
* @return true iff this function can be pushed down to jdo filter for integral types.
*/
private static boolean doesjdofuncsupportintegral genericudf func
// and, or etc. don't need to be specified here.
return func instanceof genericudfopequal
func instanceof genericudfopnotequal
}
/**
* @param type type
* @param constant the constant, if any.
* @return true iff type is an integral type.
*/
private static boolean isintegraltype string type
return type equals serdeconstants tinyint_type_name
type equals serdeconstants smallint_type_name
type equals serdeconstants int_type_name
type equals serdeconstants bigint_type_name
}
/**
* check if the partition pruning expression can be pushed down to jdo filtering.
* the partition expression contains only partition columns.
* the criteria that an expression can be pushed down are that:
*  1) the expression only contains function specified in supportedjdofuncs().
*     now only {=, and, or} can be pushed down.
*  2) the partition column type and the constant type have to be string. this is
*     restriction by the current jdo filtering implementation.
* @param tab the table that contains the partition columns.
* @param expr the partition pruning expression
* @param parent parent udf of expr if parent exists and contains a udf; otherwise null.
* @return null if the partition pruning expression can be pushed down to jdo filtering.
*/
public static string checkjdopushdown
table tab  exprnodedesc expr  genericudf parent
boolean isconst   expr instanceof exprnodeconstantdesc
boolean iscol    isconst     expr instanceof exprnodecolumndesc
boolean isintegralsupported    parent    null      isconst    iscol
doesjdofuncsupportintegral parent
// jdo filter now only support string typed literals, as well as integers
// for some operators; see filter.g and expressiontree.java.
if  isconst
object value     exprnodeconstantdesc expr  getvalue
if  value instanceof string
return null
}
if  isintegralsupported    isintegraltype expr gettypeinfo   gettypename
return null
}
return     value
isintegralsupported ?             expr gettypeinfo   gettypename
else if  iscol
typeinfo type   expr gettypeinfo
if  type gettypename   equals serdeconstants string_type_name
isintegralsupported    isintegraltype type gettypename
string colname     exprnodecolumndesc expr  getcolumn
for  fieldschema fs  tab getpartcols
if  fs getname   equals colname
if  fs gettype   equals serdeconstants string_type_name
isintegralsupported    isintegraltype fs gettype
return null
}
return     fs getname
isintegralsupported ?             fs gettype
}
}
assert false      cannot find the partition column
} else {
return     expr getexprstring
isintegralsupported ?             type gettypename
}
else if  expr instanceof exprnodegenericfuncdesc
exprnodegenericfuncdesc funcdesc    exprnodegenericfuncdesc  expr
genericudf func   funcdesc getgenericudf
if   supportedjdofuncs func
return     expr getexprstring
}
boolean allchildrenconstant   true
list<exprnodedesc> children   funcdesc getchildexprs
for  exprnodedesc child  children
if    child instanceof exprnodeconstantdesc
allchildrenconstant   false
}
string message   checkjdopushdown tab  child  func
if  message    null
return message
}
}
// if all the children of the expression are constants then jdo cannot parse the expression
// see filter.g
if  allchildrenconstant
return     expr getexprstring
}
return null
}
return     expr getexprstring
}
/**
* the check here is kind of not clean. it first use a for loop to go through
* all input formats, and choose the ones that extend reworkmapredinputformat
* to a set. and finally go through the reworkmapredinputformat set, and call
* rework for each one.
*
* technically all these can be avoided if all hive's input formats can share
* a same interface. as in today's hive and hadoop, it is not possible because
* a lot of hive's input formats are in hadoop's code. and most of hadoop's
* input formats just extend inputformat interface.
*
* @param task
* @param reworkmapredwork
* @param conf
* @throws semanticexception
*/
public static void reworkmapredwork task<? extends serializable> task
boolean reworkmapredwork  hiveconf conf  throws semanticexception
if  reworkmapredwork     task instanceof mapredtask
try
mapredwork mapredwork     mapredtask  task  getwork
set<class<? extends inputformat>> reworkinputformats   new hashset<class<? extends inputformat>>
for  partitiondesc part   mapredwork getmapwork   getpathtopartitioninfo   values
class<? extends inputformat> inputformatcls   part
getinputfileformatclass
if  reworkmapredinputformat class isassignablefrom inputformatcls
reworkinputformats add inputformatcls
}
}
if  reworkinputformats size   > 0
for  class<? extends inputformat> inputformatcls   reworkinputformats
reworkmapredinputformat inst    reworkmapredinputformat  reflectionutils
newinstance inputformatcls  null
inst rework conf  mapredwork
}
}
catch  ioexception e
throw new semanticexception e
}
}
}
public static class sqlcommand<t>
public t run preparedstatement stmt  throws sqlexception
return null
}
}
/**
* retry sql execution with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param cmd the sql command
* @param stmt the prepared statement of sql.
* @param basewindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static <t> t executewithretry sqlcommand<t> cmd  preparedstatement stmt
int basewindow  int maxretries   throws sqlexception
random r   new random
t result   null
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
result   cmd run stmt
return result
catch  sqltransientexception e
log warn     failures        e getmessage
if  failures >  maxretries
throw e
}
long waittime   getrandomwaittime basewindow  failures  r
try
thread sleep waittime
catch  interruptedexception iex
}
catch  sqlexception e
// throw other types of sqlexceptions (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* retry connecting to a database with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param connectionstring the jdbc connection string.
* @param waitwindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static connection connectwithretry string connectionstring
int waitwindow  int maxretries  throws sqlexception
random r   new random
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
connection conn   drivermanager getconnection connectionstring
return conn
catch  sqltransientexception e
if  failures >  maxretries
log error     e
throw e
}
long waittime   utilities getrandomwaittime waitwindow  failures  r
try
thread sleep waittime
catch  interruptedexception e1
}
catch  sqlexception e
// just throw other types (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* retry preparing a sql statement with random backoff (same as the one implemented in hdfs-767).
* this function only retries when the sql query throws a sqltransientexception (which
* might be able to succeed with a simple retry). it doesn't retry when the exception
* is a sqlrecoverableexception or sqlnontransientexception. for sqlrecoverableexception
* the caller needs to reconnect to the database and restart the whole transaction.
*
* @param conn a jdbc connection.
* @param stmt the sql statement to be prepared.
* @param waitwindow  the base time window (in milliseconds) before the next retry.
* see {@link #getrandomwaittime} for details.
* @param maxretries the maximum # of retries when getting a sqltransientexception.
* @throws sqlexception throws sqlrecoverableexception or sqlnontransientexception the
* first time it is caught, or sqltransientexception when the maxretries has reached.
*/
public static preparedstatement preparewithretry connection conn  string stmt
int waitwindow  int maxretries  throws sqlexception
random r   new random
// retry with # of maxretries before throwing exception
for  int failures   0    failures
try
return conn preparestatement stmt
catch  sqltransientexception e
if  failures >  maxretries
log error     stmt       e
throw e
}
long waittime   utilities getrandomwaittime waitwindow  failures  r
try
thread sleep waittime
catch  interruptedexception e1
}
catch  sqlexception e
// just throw other types (sqlnontransientexception / sqlrecoverableexception)
throw e
}
}
}
/**
* introducing a random factor to the wait time before another retry.
* the wait time is dependent on # of failures and a random factor.
* at the first time of getting an exception , the wait time
* is a random number between 0..basewindow msec. if the first retry
* still fails, we will wait basewindow msec grace period before the 2nd retry.
* also at the second retry, the waiting window is expanded to 2*basewindow msec
* alleviating the request rate from the server. similarly the 3rd retry
* will wait 2*basewindow msec. grace period before retry and the waiting window is
* expanded to 3*basewindow msec and so on.
* @param basewindow the base waiting window.
* @param failures number of failures so far.
* @param r a random generator.
* @return number of milliseconds for the next wait time.
*/
public static long getrandomwaittime int basewindow  int failures  random r
return  long
basewindow   failures          grace period for the last round of attempt
basewindow    failures   1    r nextdouble        expanding time window for each failure
}
public static final char sqlescapechar
/**
* escape the '_', '%', as well as the escape characters inside the string key.
* @param key the string that will be used for the sql like operator.
* @return a string with escaped '_' and '%'.
*/
public static string escapesqllike string key
stringbuffer sb   new stringbuffer key length
for  char c  key tochararray
switch c
case
case
case sqlescapechar
sb append sqlescapechar
// fall through
default
sb append c
break
}
}
return sb tostring
}
/**
* format number of milliseconds to strings
*
* @param msec milliseconds
* @return a formatted string like "x days y hours z minutes a seconds b msec"
*/
public static string formatmsectostr long msec
long day    1  hour    1  minute    1  second    1
long ms   msec % 1000
long timeleft   msec   1000
if  timeleft > 0
second   timeleft % 60
timeleft    60
if  timeleft > 0
minute   timeleft % 60
timeleft    60
if  timeleft > 0
hour   timeleft % 24
day   timeleft   24
}
}
}
stringbuilder sb   new stringbuilder
if  day     1
sb append day
}
if  hour     1
sb append hour
}
if  minute     1
sb append minute
}
if  second     1
sb append second
}
sb append ms
return sb tostring
}
/**
* estimate the number of reducers needed for this job, based on job input,
* and configuration parameters.
*
* the output of this method should only be used if the output of this
* mapredtask is not being used to populate a bucketed table and the user
* has not specified the number of reducers to use.
*
* @return the number of reducers.
*/
public static int estimatenumberofreducers hiveconf conf  contentsummary inputsummary
mapwork work  boolean finalmapred  throws ioexception
long bytesperreducer   conf getlongvar hiveconf confvars bytesperreducer
int maxreducers   conf getintvar hiveconf confvars maxreducers
double samplepercentage   gethighestsamplepercentage work
long totalinputfilesize   gettotalinputfilesize inputsummary  work  samplepercentage
// if all inputs are sampled, we should shrink the size of reducers accordingly.
if  totalinputfilesize    inputsummary getlength
log info     bytesperreducer
maxreducers       totalinputfilesize
} else {
log info     bytesperreducer
maxreducers       totalinputfilesize
}
int reducers    int    totalinputfilesize   bytesperreducer   1    bytesperreducer
reducers   math max 1  reducers
reducers   math min maxreducers  reducers
// if this map reduce job writes final data to a table and bucketing is being inferred,
// and the user has configured hive to do this, make sure the number of reducers is a
// power of two
if  conf getboolvar hiveconf confvars hive_infer_bucket_sort_num_buckets_power_two
finalmapred     work getbucketedcolsbydirectory   isempty
int reducerslog    int  math log reducers    math log 2     1
int reducerspowertwo    int math pow 2  reducerslog
// if the original number of reducers was a power of two, use that
if  reducerspowertwo   2    reducers
return reducers
else if  reducerspowertwo > maxreducers
// if the next power of two greater than the original number of reducers is greater
// than the max number of reducers, use the preceding power of two, which is strictly
// less than the original number of reducers and hence the max
reducers   reducerspowertwo   2
} else {
// otherwise use the smallest power of two greater than the original number of reducers
reducers   reducerspowertwo
}
}
return reducers
}
/**
* computes the total input file size. if block sampling was used it will scale this
* value by the highest sample percentage (as an estimate for input).
*
* @param inputsummary
* @param work
* @param highestsamplepercentage
* @return estimated total input size for job
*/
public static long gettotalinputfilesize  contentsummary inputsummary  mapwork work
double highestsamplepercentage
long totalinputfilesize   inputsummary getlength
if  work getnametosplitsample      null    work getnametosplitsample   isempty
// if percentage block sampling wasn't used, we don't need to do any estimation
return totalinputfilesize
}
if  highestsamplepercentage >  0
totalinputfilesize   math min  long   totalinputfilesize   highestsamplepercentage   100d
totalinputfilesize
}
return totalinputfilesize
}
/**
* computes the total number of input files. if block sampling was used it will scale this
* value by the highest sample percentage (as an estimate for # input files).
*
* @param inputsummary
* @param work
* @param highestsamplepercentage
* @return
*/
public static long gettotalinputnumfiles  contentsummary inputsummary  mapwork work
double highestsamplepercentage
long totalinputnumfiles   inputsummary getfilecount
if  work getnametosplitsample      null    work getnametosplitsample   isempty
// if percentage block sampling wasn't used, we don't need to do any estimation
return totalinputnumfiles
}
if  highestsamplepercentage >  0
totalinputnumfiles   math min  long   totalinputnumfiles   highestsamplepercentage   100d
totalinputnumfiles
}
return totalinputnumfiles
}
/**
* returns the highest sample percentage of any alias in the given mapwork
*/
public static double gethighestsamplepercentage  mapwork work
double highestsamplepercentage   0
for  string alias   work getaliastowork   keyset
if  work getnametosplitsample   containskey alias
double rate   work getnametosplitsample   get alias  getpercent
if  rate    null    rate > highestsamplepercentage
highestsamplepercentage   rate
}
} else {
highestsamplepercentage    1
break
}
}
return highestsamplepercentage
}
/**
* computes a list of all input paths needed to compute the given mapwork. all aliases
* are considered and a merged list of input paths is returned. if any input path points
* to an empty table or partition a dummy file in the scratch dir is instead created and
* added to the list. this is needed to avoid special casing the operator pipeline for
* these cases.
*
* @param job jobconf used to run the job
* @param work mapwork encapsulating the info about the task
* @param hivescratchdir the tmp dir used to create dummy files if needed
* @param ctx context object
* @return list of paths to process for the given mapwork
* @throws exception
*/
public static list<path> getinputpaths jobconf job  mapwork work  string hivescratchdir  context ctx
throws exception
int sequencenumber   0
set<path> pathsprocessed   new hashset<path>
list<path> pathstoadd   new linkedlist<path>
// aliastowork contains all the aliases
for  string alias   work getaliastowork   keyset
log info     alias
// the alias may not have any path
path path   null
for  string file   new linkedlist<string> work getpathtoaliases   keyset
list<string> aliases   work getpathtoaliases   get file
if  aliases contains alias
path   new path file
// multiple aliases can point to the same path - it should be
// processed only once
if  pathsprocessed contains path
continue
}
pathsprocessed add path
log info     path
if  isemptypath job  path  ctx
path   createdummyfileforemptypartition path  job  work
hivescratchdir  alias  sequencenumber
}
pathstoadd add path
}
}
// if the query references non-existent partitions
// we need to add a empty file, it is not acceptable to change the
// operator tree
// consider the query:
// select * from (select count(1) from t union all select count(1) from
// t2) x;
// if t is empty and t2 contains 100 rows, the user expects: 0, 100 (2
// rows)
if  path    null
path   createdummyfileforemptytable job  work  hivescratchdir
alias  sequencenumber
pathstoadd add path
}
}
return pathstoadd
}
@suppresswarnings
private static path createemptyfile string hivescratchdir
class<? extends hiveoutputformat> outfileformat  jobconf job
int sequencenumber  properties props  boolean dummyrow
throws ioexception  instantiationexception  illegalaccessexception
// create a dummy empty file in a new directory
string newdir   hivescratchdir   file separator   sequencenumber
path newpath   new path newdir
filesystem fs   newpath getfilesystem job
fs mkdirs newpath
//qualify the path against the file system. the user configured path might contain default port which is skipped
//in the file status. this makes sure that all paths which goes into pathtopartitioninfo are always listed status
//file path.
newpath   fs makequalified newpath
string newfile   newdir   file separator
path newfilepath   new path newfile
string onefile   newpath tostring
recordwriter recwriter   outfileformat newinstance   gethiverecordwriter job  newfilepath
text class  false  props  null
if  dummyrow
// empty files are omitted at combinehiveinputformat.
// for meta-data only query, it effectively makes partition columns disappear..
// this could be fixed by other methods, but this seemed to be the most easy (hivev-2955)
recwriter write new text          written via hiveignorekeytextoutputformat
}
recwriter close false
return newpath
}
@suppresswarnings
private static path createdummyfileforemptypartition path path  jobconf job  mapwork work
string hivescratchdir  string alias  int sequencenumber
throws ioexception  instantiationexception  illegalaccessexception
string strpath   path tostring
// the input file does not exist, replace it by a empty file
partitiondesc partdesc   work getpathtopartitioninfo   get strpath
boolean nonnative   partdesc gettabledesc   isnonnative
boolean onerow   partdesc getinputfileformatclass      onenullrowinputformat class
properties props   partdesc getproperties
class<? extends hiveoutputformat> outfileformat   partdesc getoutputfileformatclass
if  nonnative
// if this isn't a hive table we can't create an empty file for it.
return path
}
path newpath   createemptyfile hivescratchdir  outfileformat  job
sequencenumber  props  onerow
log info     newpath
// update the work
string strnewpath   newpath tostring
linkedhashmap<string  arraylist<string>> pathtoaliases   work getpathtoaliases
pathtoaliases put strnewpath  pathtoaliases get strpath
pathtoaliases remove strpath
work setpathtoaliases pathtoaliases
linkedhashmap<string  partitiondesc> pathtopartitioninfo   work getpathtopartitioninfo
pathtopartitioninfo put strnewpath  pathtopartitioninfo get strpath
pathtopartitioninfo remove strpath
work setpathtopartitioninfo pathtopartitioninfo
return newpath
}
@suppresswarnings
private static path createdummyfileforemptytable jobconf job  mapwork work
string hivescratchdir  string alias  int sequencenumber
throws ioexception  instantiationexception  illegalaccessexception
tabledesc tabledesc   work getaliastopartninfo   get alias  gettabledesc
properties props   tabledesc getproperties
boolean nonnative   tabledesc isnonnative
class<? extends hiveoutputformat> outfileformat   tabledesc getoutputfileformatclass
if  nonnative
// if this isn't a hive table we can't create an empty file for it.
return null
}
path newpath   createemptyfile hivescratchdir  outfileformat  job
sequencenumber  props  false
log info     newpath tostring
// update the work
linkedhashmap<string  arraylist<string>> pathtoaliases   work getpathtoaliases
arraylist<string> newlist   new arraylist<string>
newlist add alias
pathtoaliases put newpath touri   tostring    newlist
work setpathtoaliases pathtoaliases
linkedhashmap<string  partitiondesc> pathtopartitioninfo   work getpathtopartitioninfo
partitiondesc pdesc   work getaliastopartninfo   get alias  clone
pathtopartitioninfo put newpath touri   tostring    pdesc
work setpathtopartitioninfo pathtopartitioninfo
return newpath
}
/**
* setinputpaths add all the paths in the provided list to the job conf object
* as input paths for the job.
*
* @param job
* @param pathstoadd
*/
public static void setinputpaths jobconf job  list<path> pathstoadd
path addedpaths   fileinputformat getinputpaths job
if  addedpaths    null
addedpaths   new path
}
path combined   new path
system arraycopy addedpaths  0  combined  0  addedpaths length
int i   0
for path p  pathstoadd
combined   p
}
fileinputformat setinputpaths job  combined
}
/**
* set hive input format, and input format file if necessary.
*/
public static void setinputattributes configuration conf  mapwork mwork
if  mwork getinputformat      null
hiveconf setvar conf  hiveconf confvars hiveinputformat  mwork getinputformat
}
if  mwork getindexintermediatefile      null
conf set    mwork getindexintermediatefile
conf set    mwork getindexintermediatefile
}
// intentionally overwrites anything the user may have put here
conf setboolean    mwork isinputformatsorted
}
/**
* hive uses tmp directories to capture the output of each filesinkoperator.
* this method creates all necessary tmp directories for filesinks in the mapwork.
*
* @param conf used to get the right filesystem
* @param mwork used to find filesinkoperators
* @throws ioexception
*/
public static void createtmpdirs configuration conf  mapwork mwork
throws ioexception
map<string  arraylist<string>> pa   mwork getpathtoaliases
if  pa    null
list<operator<? extends operatordesc>> ops
new arraylist<operator<? extends operatordesc>>
for  list<string> ls   pa values
for  string a   ls
ops add mwork getaliastowork   get a
}
}
createtmpdirs conf  ops
}
}
/**
* hive uses tmp directories to capture the output of each filesinkoperator.
* this method creates all necessary tmp directories for filesinks in the reducework.
*
* @param conf used to get the right filesystem
* @param rwork used to find filesinkoperators
* @throws ioexception
*/
@suppresswarnings
public static void createtmpdirs configuration conf  reducework rwork
throws ioexception
if  rwork    null
return
}
list<operator<? extends operatordesc>> ops
new linkedlist<operator<? extends operatordesc>>
ops add rwork getreducer
createtmpdirs conf  ops
}
private static void createtmpdirs configuration conf
list<operator<? extends operatordesc>> ops  throws ioexception
while   ops isempty
operator<? extends operatordesc> op   ops remove 0
if  op instanceof filesinkoperator
filesinkdesc fdesc     filesinkoperator  op  getconf
string tempdir   fdesc getdirname
if  tempdir    null
path temppath   utilities totemppath new path tempdir
filesystem fs   temppath getfilesystem conf
fs mkdirs temppath
}
}
if  op getchildoperators      null
ops addall op getchildoperators
}
}
}
}