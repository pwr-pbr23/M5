/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing,
* software distributed under the license is distributed on an
* "as is" basis, without warranties or conditions of any
* kind, either express or implied.  see the license for the
* specific language governing permissions and limitations
* under the license.
*/
package org apache hcatalog security
import static org apache hadoop hive metastore metastoreutils default_database_name
import java io filenotfoundexception
import java io ioexception
import java util enumset
import java util list
import javax security auth login loginexception
import org apache commons lang arrayutils
import org apache hadoop conf configuration
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop fs permission fsaction
import org apache hadoop fs permission fspermission
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api database
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive ql metadata authorizationexception
import org apache hadoop hive ql metadata hive
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql security authorization hiveauthorizationprovider
import org apache hadoop hive ql security authorization hiveauthorizationproviderbase
import org apache hadoop hive ql security authorization privilege
import org apache hadoop hive shims hadoopshims
import org apache hadoop hive shims shimloader
import org apache hadoop security accesscontrolexception
import org apache hadoop security usergroupinformation
/**
* an authorizationprovider, which checks against the data access level permissions on hdfs.
* it makes sense to eventually move this class to hive, so that all hive users can
* use this authorization model.
* @deprecated use {@link org.apache.hadoop.hive.ql.security.authorization.storagebasedauthorizationprovider}
*/
public class hdfsauthorizationprovider extends hiveauthorizationproviderbase
protected warehouse wh
//config variables : create an enum to store them if we have more
private static final string proxy_user_name
public hdfsauthorizationprovider
super
public hdfsauthorizationprovider configuration conf
super
setconf conf
@override
public void init configuration conf  throws hiveexception
hive_db   new hiveproxy hive get new hiveconf conf  hiveauthorizationprovider class
@override
public void setconf configuration conf
super setconf conf
try
this wh   new warehouse conf
catch  metaexception ex
throw new runtimeexception ex
protected fsaction getfsaction privilege priv  path path
switch  priv getpriv
case all
throw new authorizationexception
case alter_data
return fsaction write
case alter_metadata
return fsaction write
case create
return fsaction write
case drop
return fsaction write
case index
return fsaction write
case lock
return fsaction write
case select
return fsaction read
case show_database
return fsaction read
case unknown
default
throw new authorizationexception
protected enumset<fsaction> getfsactions privilege privs  path path
enumset<fsaction> actions   enumset noneof fsaction class
if  privs    null
return actions
for  privilege priv   privs
actions add getfsaction priv  path
return actions
private static final string database_warehouse_suffix
private path getdefaultdatabasepath string dbname  throws metaexception
if  dbname equalsignorecase default_database_name
return wh getwhroot
return new path wh getwhroot    dbname tolowercase     database_warehouse_suffix
protected path getdblocation database db  throws hiveexception
try
string location   db getlocationuri
if  location    null
return getdefaultdatabasepath db getname
else
return wh getdnspath wh getdatabasepath db
catch  metaexception ex
throw new hiveexception ex getmessage
@override
public void authorize privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
//authorize for global level permissions at the warehouse dir
path root
try
root   wh getwhroot
authorize root  readrequiredpriv  writerequiredpriv
catch  metaexception ex
throw new hiveexception ex
@override
public void authorize database db  privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
if  db    null
return
path path   getdblocation db
authorize path  readrequiredpriv  writerequiredpriv
@override
public void authorize table table  privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
if  table    null
return
//unlike hive's model, this can be called at create table as well, since we should authorize
//against the table's declared location
path path   null
try
if  table getttable   getsd   getlocation      null
table getttable   getsd   getlocation   isempty
path   wh gettablepath hive_db getdatabase table getdbname     table gettablename
else
path   table getpath
catch  metaexception ex
throw new hiveexception ex
authorize path  readrequiredpriv  writerequiredpriv
//todo: hiveauthorizationprovider should expose this interface instead of #authorize(partition, privilege[], privilege[])
public void authorize table table  partition part  privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
if  part    null    part getlocation      null
authorize table  readrequiredpriv  writerequiredpriv
else
authorize part getpartitionpath    readrequiredpriv  writerequiredpriv
@override
public void authorize partition part  privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
if  part    null
return
authorize part gettable    part  readrequiredpriv  writerequiredpriv
@override
public void authorize table table  partition part  list<string> columns
privilege readrequiredpriv  privilege writerequiredpriv  throws hiveexception
authorizationexception
//columns cannot live in different files, just check for partition level permissions
authorize table  part  readrequiredpriv  writerequiredpriv
/**
* authorization privileges against a path.
* @param path a filesystem path
* @param readrequiredpriv a list of privileges needed for inputs.
* @param writerequiredpriv a list of privileges needed for outputs.
*/
public void authorize path path  privilege readrequiredpriv  privilege writerequiredpriv
throws hiveexception  authorizationexception
try
enumset<fsaction> actions   getfsactions readrequiredpriv  path
actions addall getfsactions writerequiredpriv  path
if  actions isempty
return
checkpermissions getconf    path  actions
catch  accesscontrolexception ex
throw new authorizationexception ex
catch  loginexception ex
throw new authorizationexception ex
catch  ioexception ex
throw new hiveexception ex
/**
* checks the permissions for the given path and current user on hadoop fs. if the given path
* does not exists, it checks for it's parent folder.
*/
protected static void checkpermissions final configuration conf  final path path
final enumset<fsaction> actions  throws ioexception  loginexception
if  path    null
throw new illegalargumentexception
hadoopshims shims   shimloader gethadoopshims
final usergroupinformation ugi
if  conf get proxy_user_name     null
ugi   usergroupinformation createremoteuser conf get proxy_user_name
else
ugi   shims getugiforconf conf
final string user   shims getshortusername ugi
final filesystem fs   path getfilesystem conf
if  fs exists path
checkpermissions fs  path  actions  user  ugi getgroupnames
else if  path getparent      null
// find the ancestor which exists to check it's permissions
path par   path getparent
while  par    null
if  fs exists par
break
par   par getparent
checkpermissions fs  par  actions  user  ugi getgroupnames
/**
* checks the permissions for the given path and current user on hadoop fs. if the given path
* does not exists, it returns.
*/
@suppresswarnings
protected static void checkpermissions final filesystem fs  final path path
final enumset<fsaction> actions  string user  string groups  throws ioexception
accesscontrolexception
final filestatus stat
try
stat   fs getfilestatus path
catch  filenotfoundexception fnfe
// file named by path doesn't exist; nothing to validate.
return
catch  org apache hadoop fs permission accesscontrolexception ace
// older hadoop version will throw this @deprecated exception.
throw new accesscontrolexception ace getmessage
final fspermission dirperms   stat getpermission
final string grp   stat getgroup
for  fsaction action   actions
if  user equals stat getowner
if  dirperms getuseraction   implies action
continue
if  arrayutils contains groups  grp
if  dirperms getgroupaction   implies action
continue
if  dirperms getotheraction   implies action
continue
throw new accesscontrolexception     action
path       user