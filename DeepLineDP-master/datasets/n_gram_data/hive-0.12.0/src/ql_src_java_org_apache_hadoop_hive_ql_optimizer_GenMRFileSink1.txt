/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql optimizer
import java io serializable
import java util arraylist
import java util hashmap
import java util linkedhashmap
import java util list
import java util map
import java util stack
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs path
import org apache hadoop hive conf hiveconf
import org apache hadoop hive conf hiveconf confvars
import org apache hadoop hive ql context
import org apache hadoop hive ql exec columninfo
import org apache hadoop hive ql exec conditionaltask
import org apache hadoop hive ql exec dependencycollectiontask
import org apache hadoop hive ql exec filesinkoperator
import org apache hadoop hive ql exec movetask
import org apache hadoop hive ql exec operator
import org apache hadoop hive ql exec operatorfactory
import org apache hadoop hive ql exec rowschema
import org apache hadoop hive ql exec task
import org apache hadoop hive ql exec taskfactory
import org apache hadoop hive ql exec unionoperator
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql io rcfileinputformat
import org apache hadoop hive ql io rcfile merge mergework
import org apache hadoop hive ql lib node
import org apache hadoop hive ql lib nodeprocessor
import org apache hadoop hive ql lib nodeprocessorctx
import org apache hadoop hive ql parse parsecontext
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql plan conditionalresolvermergefiles
import org apache hadoop hive ql plan conditionalresolvermergefiles conditionalresolvermergefilesctx
import org apache hadoop hive ql plan conditionalwork
import org apache hadoop hive ql plan dynamicpartitionctx
import org apache hadoop hive ql plan filesinkdesc
import org apache hadoop hive ql plan loadfiledesc
import org apache hadoop hive ql plan mapwork
import org apache hadoop hive ql plan mapredwork
import org apache hadoop hive ql plan movework
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan partitiondesc
import org apache hadoop hive ql plan statswork
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive serde2 typeinfo typeinfofactory
import org apache hadoop mapred inputformat
/**
* processor for the rule - table scan followed by reduce sink.
*/
public class genmrfilesink1 implements nodeprocessor
static final private log log   logfactory getlog genmrfilesink1 class getname
public genmrfilesink1
/**
* file sink operator encountered.
*
* @param nd
*          the file sink operator encountered
* @param opprocctx
*          context
*/
public object process node nd  stack<node> stack  nodeprocessorctx opprocctx
object    nodeoutputs  throws semanticexception
genmrproccontext ctx    genmrproccontext  opprocctx
parsecontext parsectx   ctx getparsectx
boolean chdir   false
task<? extends serializable> currtask   ctx getcurrtask
ctx addrootifpossible currtask
filesinkoperator fsop    filesinkoperator  nd
boolean isinserttable      is insert overwrite table
fsop getconf   gettableinfo   gettablename      null
parsectx getqb   getparseinfo   isinserttotable
hiveconf hconf   parsectx getconf
// mark this task as a final map reduce task (ignoring the optional merge task)
mapredwork currtask getwork    setfinalmapred true
// if this file sink desc has been processed due to a linked file sink desc,
// use that task
map<filesinkdesc  task<? extends serializable>> filesinkdescs   ctx getlinkedfiledesctasks
if  filesinkdescs    null
task<? extends serializable> childtask   filesinkdescs get fsop getconf
processlinkedfiledesc ctx  childtask
return true
// has the user enabled merging of files for map-only jobs or for all jobs
if   ctx getmvtask      null       ctx getmvtask   isempty
list<task<movework>> mvtasks   ctx getmvtask
// in case of unions or map-joins, it is possible that the file has
// already been seen.
// so, no need to attempt to merge the files again.
if   ctx getseenfilesinkops      null
ctx getseenfilesinkops   contains nd
// no need of merging if the move is to a local file system
movetask mvtask    movetask  findmovetask mvtasks  fsop
if  mvtask    null    isinserttable    hconf getboolvar confvars hivestatsautogather
addstatstask fsop  mvtask  currtask  parsectx getconf
if   mvtask    null      mvtask islocal      fsop getconf   canbemerged
if  fsop getconf   islinkedfilesink
// if the user has hivemergemapredfiles set to false, the idea was the
// number of reducers are few, so the number of files anyway are small.
// however, with this optimization, we are increasing the number of files
// possibly by a big margin. so, merge aggresively.
if  hconf getboolvar confvars hivemergemapfiles
hconf getboolvar confvars hivemergemapredfiles
chdir   true
else
// there are separate configuration parameters to control whether to
// merge for a map-only job
// or for a map-reduce job
mapredwork currwork    mapredwork  currtask getwork
boolean mergemaponly
hconf getboolvar confvars hivemergemapfiles     currwork getreducework      null
boolean mergemapred
hconf getboolvar confvars hivemergemapredfiles
currwork getreducework      null
if  mergemaponly    mergemapred
chdir   true
string finalname   processfs fsop  stack  opprocctx  chdir
if  chdir
// merge the files in the destination table/partitions by creating map-only merge job
// if underlying data is rcfile a rcfileblockmerge task would be created.
log info
createmrworkformergingfiles fsop  ctx  finalname
filesinkdesc filesinkdesc   fsop getconf
if  filesinkdesc islinkedfilesink
map<filesinkdesc  task<? extends serializable>> linkedfiledesctasks
ctx getlinkedfiledesctasks
if  linkedfiledesctasks    null
linkedfiledesctasks   new hashmap<filesinkdesc  task<? extends serializable>>
ctx setlinkedfiledesctasks linkedfiledesctasks
// the child tasks may be null in case of a select
if   currtask getchildtasks      null
currtask getchildtasks   size      1
for  filesinkdesc filedesc   filesinkdesc getlinkedfilesinkdesc
linkedfiledesctasks put filedesc  currtask getchildtasks   get 0
return true
/*
* multiple file sink descriptors are linked.
* use the task created by the first linked file descriptor
*/
private void processlinkedfiledesc genmrproccontext ctx
task<? extends serializable> childtask  throws semanticexception
task<? extends serializable> currtask   ctx getcurrtask
operator<? extends operatordesc> currtopop   ctx getcurrtopop
if  currtopop    null     ctx isseenop currtask  currtopop
string curraliasid   ctx getcurraliasid
genmapredutils settaskplan curraliasid  currtopop  currtask  false  ctx
if  childtask    null
currtask adddependenttask childtask
/**
* add the statstask as a dependent task of the movetask
* because statstask will change the table/partition metadata. for atomicity, we
* should not change it before the data is actually there done by movetask.
*
* @param nd
*          the filesinkoperator whose results are taken care of by the movetask.
* @param mvtask
*          the movetask that moves the filesinkoperator's results.
* @param currtask
*          the mapredtask that the filesinkoperator belongs to.
* @param hconf
*          hiveconf
*/
private void addstatstask filesinkoperator nd  movetask mvtask
task<? extends serializable> currtask  hiveconf hconf
movework mvwork     movetask  mvtask  getwork
statswork statswork   null
if  mvwork getloadtablework      null
statswork   new statswork mvwork getloadtablework
else if  mvwork getloadfilework      null
statswork   new statswork mvwork getloadfilework
assert statswork    null
statswork setstatsreliable hconf getboolvar confvars hive_stats_reliable
mapredwork mrwork    mapredwork  currtask getwork
// aggkey in statswork is used for stats aggregation while statsaggprefix
// in filesinkdesc is used for stats publishing. they should be consistent.
statswork setaggkey   filesinkoperator  nd  getconf   getstatsaggprefix
task<? extends serializable> statstask   taskfactory get statswork  hconf
// mark the mapredwork and filesinkoperator for gathering stats
nd getconf   setgatherstats true
mrwork getmapwork   setgatheringstats true
if  mrwork getreducework      null
mrwork getreducework   setgatheringstats true
nd getconf   setstatsreliable hconf getboolvar confvars hive_stats_reliable
nd getconf   setmaxstatskeyprefixlength
hconf getintvar confvars hive_stats_key_prefix_max_length
// mrwork.adddestinationtable(nd.getconf().gettableinfo().gettablename());
// subscribe feeds from the movetask so that movetask can forward the list
// of dynamic partition list to the statstask
mvtask adddependenttask statstask
statstask subscribefeed mvtask
/**
* @param fsinput the filesink operator.
* @param ctx the mr processing context.
* @param finalname the final destination path the merge job should output.
* @throws semanticexception
* create a map-only merge job using combinehiveinputformat for all partitions with
* following operators:
*          mr job j0:
*          ...
*          |
*          v
*          filesinkoperator_1 (fsinput)
*          |
*          v
*          merge job j1:
*          |
*          v
*          tablescan (using combinehiveinputformat) (tsmerge)
*          |
*          v
*          filesinkoperator (fsmerge)
*
*          here the pathtopartitioninfo & pathtoalias will remain the same, which means the paths
*          do
*          not contain the dynamic partitions (their parent). so after the dynamic partitions are
*          created (after the first job finished before the movetask or conditionaltask start),
*          we need to change the pathtopartitioninfo & pathtoalias to include the dynamic
*          partition
*          directories.
*
*/
private void createmrworkformergingfiles  filesinkoperator fsinput  genmrproccontext ctx
string finalname  throws semanticexception
//
// 1. create the operator tree
//
hiveconf conf   ctx getparsectx   getconf
filesinkdesc fsinputdesc   fsinput getconf
// create a tablescan operator
rowschema inputrs   fsinput getschema
operator<? extends operatordesc> tsmerge
operatorfactory get tablescandesc class  inputrs
// create a filesink operator
tabledesc ts    tabledesc  fsinputdesc gettableinfo   clone
filesinkdesc fsoutputdesc   new filesinkdesc finalname  ts
conf getboolvar confvars compressresult
filesinkoperator fsoutput    filesinkoperator  operatorfactory getandmakechild
fsoutputdesc  inputrs  tsmerge
// if the input filesinkoperator is a dynamic partition enabled, the tsmerge input schema
// needs to include the partition column, and the fsoutput should have
// a dynamicpartitionctx to indicate that it needs to dynamically partitioned.
dynamicpartitionctx dpctx   fsinputdesc getdynpartctx
if  dpctx    null    dpctx getnumdpcols   > 0
// adding dp columninfo to the rowschema signature
arraylist<columninfo> signature   inputrs getsignature
string tblalias   fsinputdesc gettableinfo   gettablename
linkedhashmap<string  string> colmap   new linkedhashmap<string  string>
stringbuilder partcols   new stringbuilder
for  string dpcol   dpctx getdpcolnames
columninfo colinfo   new columninfo dpcol
typeinfofactory stringtypeinfo     all partition column type should be string
tblalias  true      partition column is virtual column
signature add colinfo
colmap put dpcol  dpcol      input and output have the same column name
partcols append dpcol  append
partcols setlength partcols length     1      remove the last
inputrs setsignature signature
// create another dynamicpartitionctx, which has a different input-to-dp column mapping
dynamicpartitionctx dpctx2   new dynamicpartitionctx dpctx
dpctx2 setinputtodpcols colmap
fsoutputdesc setdynpartctx dpctx2
// update the filesinkoperator to include partition columns
fsinputdesc gettableinfo   getproperties   setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
partcols tostring        list of dynamic partition column names
else
// non-partitioned table
fsinputdesc gettableinfo   getproperties   remove
org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
//
// 2. constructing a conditional task consisting of a move task and a map reduce task
//
movework dummymv   new movework null  null  null
new loadfiledesc fsinputdesc getfinaldirname    finalname  true  null  null   false
mapwork cplan
serializable work
if  conf getboolvar confvars hivemergercfileblocklevel
fsinputdesc gettableinfo   getinputfileformatclass   equals rcfileinputformat class
// check if inputformatclass is valid
string inputformatclass   conf getvar confvars hivemergeinputformatblocklevel
try
class c    class<? extends inputformat>  class forname inputformatclass
log info
cplan   creatercfilemergetask fsinputdesc  finalname
dpctx    null    dpctx getnumdpcols   > 0
work   cplan
catch  classnotfoundexception e
string msg       inputformatclass
throw new semanticexception msg
else
cplan   createmrworkformergingfiles conf  tsmerge  fsinputdesc
work   new mapredwork
mapredwork work  setmapwork cplan
// use combinehiveinputformat for map-only merging
cplan setinputformat
// note: we should gather stats in mr1 rather than mr2 at merge job since we don't
// know if merge mr2 will be triggered at execution time
conditionaltask cndtsk   createcondtask conf  ctx getcurrtask    dummymv  work
fsinputdesc getfinaldirname
// keep the dynamic partition context in conditional task resolver context
conditionalresolvermergefilesctx mrctx
conditionalresolvermergefilesctx  cndtsk getresolverctx
mrctx setdpctx fsinputdesc getdynpartctx
mrctx setlbctx fsinputdesc getlbctx
//
// 3. add the movetask as the children of the conditional task
//
linkmovetask ctx  fsoutput  cndtsk
/**
* make the move task in the genmrproccontext following the filesinkoperator a dependent of all
* possible subtrees branching from the conditionaltask.
*
* @param ctx
* @param newoutput
* @param cndtsk
*/
private void linkmovetask genmrproccontext ctx  filesinkoperator newoutput
conditionaltask cndtsk
list<task<movework>> mvtasks   ctx getmvtask
task<movework> mvtask   findmovetask mvtasks  newoutput
for  task<? extends serializable> tsk   cndtsk getlisttasks
linkmovetask ctx  mvtask  tsk
/**
* follows the task tree down from task and makes all leaves parents of mvtask
*
* @param ctx
* @param mvtask
* @param task
*/
private void linkmovetask genmrproccontext ctx  task<movework> mvtask
task<? extends serializable> task
if  task getdependenttasks      null    task getdependenttasks   isempty
// if it's a leaf, add the move task as a child
adddependentmovetasks ctx  mvtask  task
else
// otherwise, for each child run this method recursively
for  task<? extends serializable> childtask   task getdependenttasks
linkmovetask ctx  mvtask  childtask
/**
* adds the dependencytaskformultiinsert in ctx as a dependent of parenttask. if mvtask is a
* load table, and hive_multi_insert_atomic_outputs is set, adds mvtask as a dependent of
* dependencytaskformultiinsert in ctx, otherwise adds mvtask as a dependent of parenttask as
* well.
*
* @param ctx
* @param mvtask
* @param parenttask
*/
private void adddependentmovetasks genmrproccontext ctx  task<movework> mvtask
task<? extends serializable> parenttask
if  mvtask    null
if  ctx getconf   getboolvar confvars hive_multi_insert_move_tasks_share_dependencies
dependencycollectiontask dependencytask   ctx getdependencytaskformultiinsert
parenttask adddependenttask dependencytask
if  mvtask getwork   getloadtablework      null
// moving tables/partitions depend on the dependencytask
dependencytask adddependenttask mvtask
else
// moving files depends on the parenttask (we still want the dependencytask to depend
// on the parenttask)
parenttask adddependenttask mvtask
else
parenttask adddependenttask mvtask
/**
* create a mapredwork based on input path, the top operator and the input
* table descriptor.
*
* @param conf
* @param topop
*          the table scan operator that is the root of the mapreduce task.
* @param fsdesc
*          the file sink descriptor that serves as the input to this merge task.
* @param parentmr
*          the parent mapreduce work
* @param parentfs
*          the last filesinkoperator in the parent mapreduce work
* @return the mapredwork
*/
private mapwork createmrworkformergingfiles  hiveconf conf
operator<? extends operatordesc> topop   filesinkdesc fsdesc
arraylist<string> aliases   new arraylist<string>
string inputdir   fsdesc getfinaldirname
tabledesc tbldesc   fsdesc gettableinfo
aliases add inputdir      dummy alias  just use the input path
// constructing the default mapredwork
mapredwork cmrplan   genmapredutils getmapredworkfromconf conf
mapwork cplan   cmrplan getmapwork
cplan getpathtoaliases   put inputdir  aliases
cplan getpathtopartitioninfo   put inputdir  new partitiondesc tbldesc  null
cplan getaliastowork   put inputdir  topop
cplan setmappercannotspanpartns true
return cplan
/**
* create a block level merge task for rcfiles.
*
* @param fsinputdesc
* @param finalname
* @return mergework if table is stored as rcfile,
*         null otherwise
*/
private mapwork creatercfilemergetask filesinkdesc fsinputdesc
string finalname  boolean hasdynamicpartitions  throws semanticexception
string inputdir   fsinputdesc getfinaldirname
tabledesc tbldesc   fsinputdesc gettableinfo
if  tbldesc getinputfileformatclass   equals rcfileinputformat class
arraylist<string> inputdirs   new arraylist<string>
if   hasdynamicpartitions
isskewedstoredasdirs fsinputdesc
inputdirs add inputdir
mergework work   new mergework inputdirs  finalname
hasdynamicpartitions  fsinputdesc getdynpartctx
linkedhashmap<string  arraylist<string>> pathtoaliases
new linkedhashmap<string  arraylist<string>>
pathtoaliases put inputdir   arraylist<string>  inputdirs clone
work setmappercannotspanpartns true
work setpathtoaliases pathtoaliases
work setaliastowork
new linkedhashmap<string  operator<? extends operatordesc>>
if  hasdynamicpartitions
isskewedstoredasdirs fsinputdesc
work getpathtopartitioninfo   put inputdir
new partitiondesc tbldesc  null
work setlistbucketingctx fsinputdesc getlbctx
return work
throw new semanticexception
/**
* check if it is skewed table and stored as dirs.
*
* @param fsinputdesc
* @return
*/
private boolean isskewedstoredasdirs filesinkdesc fsinputdesc
return  fsinputdesc getlbctx      null  ? false   fsinputdesc getlbctx
isskewedstoredasdir
/**
* construct a conditional task given the current leaf task, the movework and the mapredwork.
*
* @param conf
*          hiveconf
* @param currtask
*          current leaf task
* @param mvwork
*          movework for the move task
* @param mergework
*          mapredwork for the merge task.
* @param inputpath
*          the input directory of the merge/move task
* @return the conditional task
*/
private conditionaltask createcondtask hiveconf conf
task<? extends serializable> currtask  movework mvwork
serializable mergework  string inputpath
// there are 3 options for this conditionaltask:
// 1) merge the partitions
// 2) move the partitions (i.e. don't merge the partitions)
// 3) merge some partitions and move other partitions (i.e. merge some partitions and don't
// merge others) in this case the merge is done first followed by the move to prevent
// conflicts.
task<? extends serializable> mergeonlymergetask   taskfactory get mergework  conf
task<? extends serializable> moveonlymovetask   taskfactory get mvwork  conf
task<? extends serializable> mergeandmovemergetask   taskfactory get mergework  conf
task<? extends serializable> mergeandmovemovetask   taskfactory get mvwork  conf
// note! it is necessary merge task is the parent of the move task, and not
// the other way around, for the proper execution of the execute method of
// conditionaltask
mergeandmovemergetask adddependenttask mergeandmovemovetask
list<serializable> listworks   new arraylist<serializable>
listworks add mvwork
listworks add mergework
conditionalwork cndwork   new conditionalwork listworks
list<task<? extends serializable>> listtasks   new arraylist<task<? extends serializable>>
listtasks add moveonlymovetask
listtasks add mergeonlymergetask
listtasks add mergeandmovemergetask
conditionaltask cndtsk    conditionaltask  taskfactory get cndwork  conf
cndtsk setlisttasks listtasks
// create resolver
cndtsk setresolver new conditionalresolvermergefiles
conditionalresolvermergefilesctx mrctx
new conditionalresolvermergefilesctx listtasks  inputpath
cndtsk setresolverctx mrctx
// make the conditional task as the child of the current leaf task
currtask adddependenttask cndtsk
return cndtsk
private task<movework> findmovetask
list<task<movework>> mvtasks  filesinkoperator fsop
// find the move task
for  task<movework> mvtsk   mvtasks
movework mvwork   mvtsk getwork
string srcdir   null
if  mvwork getloadfilework      null
srcdir   mvwork getloadfilework   getsourcedir
else if  mvwork getloadtablework      null
srcdir   mvwork getloadtablework   getsourcedir
string fsopdirname   fsop getconf   getfinaldirname
if   srcdir    null
srcdir equalsignorecase fsopdirname
return mvtsk
return null
/**
* process the filesink operator to generate a movetask if necessary.
*
* @param fsop
*          current filesink operator
* @param stack
*          parent operators
* @param opprocctx
* @param chdir
*          whether the operator should be first output to a tmp dir and then merged
*          to the final dir later
* @return the final file name to which the filesinkoperator should store.
* @throws semanticexception
*/
private string processfs filesinkoperator fsop  stack<node> stack
nodeprocessorctx opprocctx  boolean chdir  throws semanticexception
genmrproccontext ctx    genmrproccontext  opprocctx
list<filesinkoperator> seenfsops   ctx getseenfilesinkops
if  seenfsops    null
seenfsops   new arraylist<filesinkoperator>
if   seenfsops contains fsop
seenfsops add fsop
ctx setseenfilesinkops seenfsops
task<? extends serializable> currtask   ctx getcurrtask
// if the directory needs to be changed, send the new directory
string dest   null
if  chdir
dest   fsop getconf   getfinaldirname
// generate the temporary file
// it must be on the same file system as the current destination
parsecontext parsectx   ctx getparsectx
context basectx   parsectx getcontext
string tmpdir   basectx getexternaltmpfileuri  new path dest   touri
filesinkdesc filesinkdesc   fsop getconf
// change all the linked file sink descriptors
if  filesinkdesc islinkedfilesink
for  filesinkdesc fsconf filesinkdesc getlinkedfilesinkdesc
string filename   utilities getfilenamefromdirname fsconf getdirname
fsconf setparentdir tmpdir
fsconf setdirname tmpdir   path separator   filename
else
filesinkdesc setdirname tmpdir
task<movework> mvtask   null
if   chdir
mvtask   findmovetask ctx getmvtask    fsop
operator<? extends operatordesc> currtopop   ctx getcurrtopop
string curraliasid   ctx getcurraliasid
hashmap<operator<? extends operatordesc>  task<? extends serializable>> optaskmap
ctx getoptaskmap
// set the move task to be dependent on the current task
if  mvtask    null
adddependentmovetasks ctx  mvtask  currtask
// in case of multi-table insert, the path to alias mapping is needed for
// all the sources. since there is no
// reducer, treat it as a plan with null reducer
// if it is a map-only job, the task needs to be processed
if  currtopop    null
task<? extends serializable> maptask   optaskmap get null
if  maptask    null
if   ctx isseenop currtask  currtopop
genmapredutils settaskplan curraliasid  currtopop  currtask  false  ctx
optaskmap put null  currtask
else
if   ctx isseenop currtask  currtopop
genmapredutils settaskplan curraliasid  currtopop  maptask  false  ctx
else
unionoperator currunionop   ctx getcurrunionop
if  currunionop    null
optaskmap put null  currtask
ctx setcurrtopop null
genmapredutils initunionplan ctx  currunionop  currtask  false
return dest
// maptask and currtask should be merged by and join/union operator
// (e.g., genmrunion1) which has multiple topops.
// assert maptask == currtask : "maptask.id = " + maptask.getid()
// + "; currtask.id = " + currtask.getid();
return dest
unionoperator currunionop   ctx getcurrunionop
if  currunionop    null
optaskmap put null  currtask
genmapredutils initunionplan ctx  currunionop  currtask  false
return dest
return dest