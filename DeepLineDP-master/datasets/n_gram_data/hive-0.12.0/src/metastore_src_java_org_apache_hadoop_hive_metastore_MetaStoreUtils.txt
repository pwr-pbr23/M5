/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive metastore
import java io file
import java io ioexception
import java lang reflect constructor
import java lang reflect invocationtargetexception
import java net inetsocketaddress
import java net serversocket
import java net socket
import java util arraylist
import java util hashmap
import java util hashset
import java util iterator
import java util list
import java util map
import java util map entry
import java util properties
import java util set
import java util regex matcher
import java util regex pattern
import org apache commons lang stringutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop fs filesystem
import org apache hadoop fs path
import org apache hadoop hive common javautils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api invalidoperationexception
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive metastore api serdeinfo
import org apache hadoop hive metastore api storagedescriptor
import org apache hadoop hive metastore api table
import org apache hadoop hive metastore api hive_metastoreconstants
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 serdeutils
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive serde2 objectinspector listobjectinspector
import org apache hadoop hive serde2 objectinspector mapobjectinspector
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspector category
import org apache hadoop hive serde2 objectinspector structfield
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive shims shimloader
import org apache hadoop hive thrift hadoopthriftauthbridge
public class metastoreutils
protected static final log log   logfactory getlog
public static final string default_database_name
public static final string default_database_comment
public static final string database_warehouse_suffix
/**
* printstacktrace
*
* helper function to print an exception stack trace to the log and not stderr
*
* @param e
*          the exception
*
*/
static public void printstacktrace exception e
for  stacktraceelement s   e getstacktrace
log error s
public static table createcolumnsetschema string name  list<string> columns
list<string> partcols  configuration conf  throws metaexception
if  columns    null
throw new metaexception     name
table ttable   new table
ttable settablename name
ttable setsd new storagedescriptor
storagedescriptor sd   ttable getsd
sd setserdeinfo new serdeinfo
serdeinfo serdeinfo   sd getserdeinfo
serdeinfo setserializationlib lazysimpleserde class getname
serdeinfo setparameters new hashmap<string  string>
serdeinfo getparameters   put
org apache hadoop hive serde serdeconstants serialization_format
list<fieldschema> fields   new arraylist<fieldschema>
sd setcols fields
for  string col   columns
fieldschema field   new fieldschema col
org apache hadoop hive serde serdeconstants string_type_name
fields add field
ttable setpartitionkeys new arraylist<fieldschema>
for  string partcol   partcols
fieldschema part   new fieldschema
part setname partcol
part settype org apache hadoop hive serde serdeconstants string_type_name      default
// partition
// key
ttable getpartitionkeys   add part
sd setnumbuckets  1
return ttable
/**
* recursivedelete
*
* just recursively deletes a dir - you'd think java would have something to
* do this??
*
* @param f
*          - the file/dir to delete
* @exception ioexception
*              propogate f.delete() exceptions
*
*/
static public void recursivedelete file f  throws ioexception
if  f isdirectory
file fs   f listfiles
for  file subf   fs
recursivedelete subf
if   f delete
throw new ioexception     f getpath
/**
* getdeserializer
*
* get the deserializer for a table given its name and properties.
*
* @param conf
*          hadoop config
* @param schema
*          the properties to use to instantiate the deserializer
* @return
*   returns instantiated deserializer by looking up class name of deserializer stored in passed
*   in properties. also, initializes the deserializer with schema stored in passed in properties.
* @exception metaexception
*              if any problems instantiating the deserializer
*
*              todo - this should move somewhere into serde.jar
*
*/
static public deserializer getdeserializer configuration conf
properties schema  throws metaexception
string lib   schema
getproperty org apache hadoop hive serde serdeconstants serialization_lib
try
deserializer deserializer   serdeutils lookupdeserializer lib
deserializer  initialize conf  schema
return deserializer
catch  exception e
log error     e getclass   getname
e getmessage
metastoreutils printstacktrace e
throw new metaexception e getclass   getname         e getmessage
/**
* getdeserializer
*
* get the deserializer for a table.
*
* @param conf
*          - hadoop config
* @param table
*          the table
* @return
*   returns instantiated deserializer by looking up class name of deserializer stored in
*   storage descriptor of passed in table. also, initializes the deserializer with schema
*   of table.
* @exception metaexception
*              if any problems instantiating the deserializer
*
*              todo - this should move somewhere into serde.jar
*
*/
static public deserializer getdeserializer configuration conf
org apache hadoop hive metastore api table table  throws metaexception
string lib   table getsd   getserdeinfo   getserializationlib
if  lib    null
return null
try
deserializer deserializer   serdeutils lookupdeserializer lib
deserializer initialize conf  metastoreutils gettablemetadata table
return deserializer
catch  runtimeexception e
throw e
catch  exception e
log error     e getclass   getname
e getmessage
metastoreutils printstacktrace e
throw new metaexception e getclass   getname         e getmessage
/**
* getdeserializer
*
* get the deserializer for a partition.
*
* @param conf
*          - hadoop config
* @param part
*          the partition
* @param table the table
* @return
*   returns instantiated deserializer by looking up class name of deserializer stored in
*   storage descriptor of passed in partition. also, initializes the deserializer with
*   schema of partition.
* @exception metaexception
*              if any problems instantiating the deserializer
*
*/
static public deserializer getdeserializer configuration conf
org apache hadoop hive metastore api partition part
org apache hadoop hive metastore api table table  throws metaexception
string lib   part getsd   getserdeinfo   getserializationlib
try
deserializer deserializer   serdeutils lookupdeserializer lib
deserializer initialize conf  metastoreutils getpartitionmetadata part  table
return deserializer
catch  runtimeexception e
throw e
catch  exception e
log error     e getclass   getname
e getmessage
metastoreutils printstacktrace e
throw new metaexception e getclass   getname         e getmessage
static public void deletewhdirectory path path  configuration conf
boolean use_trash  throws metaexception
try
if   path getfilesystem conf  exists path
log warn
path
return
if  use_trash
int count   0
path newpath   new path
path getparent   touri   getpath
if  path getfilesystem conf  exists newpath     false
path getfilesystem conf  mkdirs newpath
do
newpath   new path     path touri   getpath
count
if  path getfilesystem conf  exists newpath
count
continue
if  path getfilesystem conf  rename path  newpath
break
while    count < 50
if  count >  50
throw new metaexception
else
// directly delete it
path getfilesystem conf  delete path  true
catch  ioexception e
log error     e
throw new metaexception e getmessage
catch  metaexception e
log error     e
throw e
/**
* given a list of partition columns and a partial mapping from
* some partition columns to values the function returns the values
* for the column.
* @param partcols the list of table partition columns
* @param partspec the partial mapping from partition column to values
* @return list of values of for given partition columns, any missing
*         values in partspec is replaced by an empty string
*/
public static list<string> getpvals list<fieldschema> partcols
map<string  string> partspec
list<string> pvals   new arraylist<string>
for  fieldschema field   partcols
string val   partspec get field getname
if  val    null
val
pvals add val
return pvals
/**
* validatename
*
* checks the name conforms to our standars which are: "[a-za-z_0-9]+". checks
* this is just characters and numbers and _
*
* @param name
*          the name to validate
* @return true or false depending on conformance
* @exception metaexception
*              if it doesn't match the pattern.
*/
static public boolean validatename string name
pattern tpat   pattern compile
matcher m   tpat matcher name
if  m matches
return true
return false
static public string validatetblcolumns list<fieldschema> cols
for  fieldschema fieldschema   cols
if   validatename fieldschema getname
return     fieldschema getname
if   validatecolumntype fieldschema gettype
return     fieldschema gettype
return null
static void throwexceptionifincompatiblecoltypechange
list<fieldschema> oldcols  list<fieldschema> newcols
throws invalidoperationexception
list<string> incompatiblecols   new arraylist<string>
int maxcols   math min oldcols size    newcols size
for  int i   0  i < maxcols  i
if   arecoltypescompatible oldcols get i  gettype    newcols get i  gettype
incompatiblecols add newcols get i  getname
if   incompatiblecols isempty
throw new invalidoperationexception
stringutils join incompatiblecols
/**
* @return true if oldtype and newtype are compatible.
* two types are compatible if we have internal functions to cast one to another.
*/
static private boolean arecoltypescompatible string oldtype  string newtype
if  oldtype equals newtype
return true
/*
* rcfile default serde (columnarserde) serializes the values in such a way that the
* datatypes can be converted from string to any type. the map is also serialized as
* a string, which can be read as a string as well. however, with any binary
* serialization, this is not true.
*
* primitive types like int, string, bigint, etc are compatible with each other and are
* not blocked.
*/
if serdeconstants primitivetypes contains oldtype tolowercase
serdeconstants primitivetypes contains newtype tolowercase
return true
return false
/**
* validate column type
*
* if it is predefined, yes. otherwise no
* @param name
* @return
*/
static public boolean validatecolumntype string type
int last   0
boolean lastalphadigit   character isletterordigit type charat last
for  int i   1  i <  type length    i
if  i    type length
character isletterordigit type charat i      lastalphadigit
string token   type substring last  i
last   i
if   hivethrifttypemap contains token
return false
break
return true
public static string validateskewedcolnames list<string> cols
if  null    cols
return null
for  string col   cols
if   validatename col
return col
return null
public static string validateskewedcolnamessubsetcol list<string> skewedcolnames
list<fieldschema> cols
if  null    skewedcolnames
return null
list<string> colnames   new arraylist<string>
for  fieldschema fieldschema   cols
colnames add fieldschema getname
// make a copy
list<string> copyskewedcolnames   new arraylist<string> skewedcolnames
// remove valid columns
copyskewedcolnames removeall colnames
if  copyskewedcolnames isempty
return null
return copyskewedcolnames tostring
public static string getlisttype string t
return     t
public static string getmaptype string k  string v
return     k       v
public static void setserdeparam serdeinfo sdi  properties schema
string param
string val   schema getproperty param
if  org apache commons lang stringutils isnotblank val
sdi getparameters   put param  val
static hashmap<string  string> typetothrifttypemap
static
typetothrifttypemap   new hashmap<string  string>
typetothrifttypemap put
org apache hadoop hive serde serdeconstants boolean_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants tinyint_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants smallint_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants int_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants bigint_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants double_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants float_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants list_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants map_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants string_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants binary_type_name
// these 4 types are not supported yet.
// we should define a complex type date in thrift that contains a single int
// member, and dynamicserde
// should convert it to date type at runtime.
typetothrifttypemap put
org apache hadoop hive serde serdeconstants date_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants datetime_type_name
typetothrifttypemap
put org apache hadoop hive serde serdeconstants timestamp_type_name
typetothrifttypemap put
org apache hadoop hive serde serdeconstants decimal_type_name
static set<string> hivethrifttypemap    for validation
static
hivethrifttypemap   new hashset<string>
hivethrifttypemap addall serdeconstants primitivetypes
hivethrifttypemap addall org apache hadoop hive serde serdeconstants collectiontypes
hivethrifttypemap add org apache hadoop hive serde serdeconstants union_type_name
hivethrifttypemap add org apache hadoop hive serde serdeconstants struct_type_name
/**
* convert type to thrifttype. we do that by tokenizing the type and convert
* each token.
*/
public static string typetothrifttype string type
stringbuilder thrifttype   new stringbuilder
int last   0
boolean lastalphadigit   character isletterordigit type charat last
for  int i   1  i <  type length    i
if  i    type length
character isletterordigit type charat i      lastalphadigit
string token   type substring last  i
last   i
string thrifttoken   typetothrifttypemap get token
thrifttype append thrifttoken    null ? token   thrifttoken
lastalphadigit    lastalphadigit
return thrifttype tostring
/**
* convert fieldschemas to thrift ddl + column names and column types
*
* @param structname
*          the name of the table
* @param fieldschemas
*          list of fields along with their schemas
* @return string containing "thrift
*         ddl#comma-separated-column-names#colon-separated-columntypes
*         example:
*         "struct result { a string, map<int,string> b}#a,b#string:map<int,string>"
*/
public static string getfullddlfromfieldschema string structname
list<fieldschema> fieldschemas
stringbuilder ddl   new stringbuilder
ddl append getddlfromfieldschema structname  fieldschemas
ddl append
stringbuilder colnames   new stringbuilder
stringbuilder coltypes   new stringbuilder
boolean first   true
for  fieldschema col   fieldschemas
if  first
first   false
else
colnames append
coltypes append
colnames append col getname
coltypes append col gettype
ddl append colnames
ddl append
ddl append coltypes
return ddl tostring
/**
* convert fieldschemas to thrift ddl.
*/
public static string getddlfromfieldschema string structname
list<fieldschema> fieldschemas
stringbuilder ddl   new stringbuilder
ddl append
ddl append structname
ddl append
boolean first   true
for  fieldschema col   fieldschemas
if  first
first   false
else
ddl append
ddl append typetothrifttype col gettype
ddl append
ddl append col getname
ddl append
log debug     ddl
return ddl tostring
public static properties gettablemetadata
org apache hadoop hive metastore api table table
return metastoreutils getschema table getsd    table getsd    table
getparameters    table getdbname    table gettablename    table getpartitionkeys
public static properties getpartitionmetadata
org apache hadoop hive metastore api partition partition
org apache hadoop hive metastore api table table
return metastoreutils
getschema partition getsd    partition getsd    partition
getparameters    table getdbname    table gettablename
table getpartitionkeys
public static properties getschema
org apache hadoop hive metastore api partition part
org apache hadoop hive metastore api table table
return metastoreutils getschema part getsd    table getsd    table
getparameters    table getdbname    table gettablename    table getpartitionkeys
/**
* get partition level schema from table level schema.
* this function will use the same column names, column types and partition keys for
* each partition properties. their values are copied from the table properties. this
* is mainly to save cpu and memory. cpu is saved because the first time the
* storagedescriptor column names are accessed, jdo needs to execute a sql query to
* retrieve the data. if we know the data will be the same as the table level schema
* and they are immutable, we should just reuse the table level schema objects.
*
* @param sd the partition level storage descriptor.
* @param tblsd the table level storage descriptor.
* @param parameters partition level parameters
* @param databasename db name
* @param tablename table name
* @param partitionkeys partition columns
* @param tblschema the table level schema from which this partition should be copied.
* @return the properties
*/
public static properties getpartschemafromtableschema
org apache hadoop hive metastore api storagedescriptor sd
org apache hadoop hive metastore api storagedescriptor tblsd
map<string  string> parameters  string databasename  string tablename
list<fieldschema> partitionkeys
properties tblschema
// inherent most properties from table level schema and overwrite some properties
// in the following code.
// this is mainly for saving cpu and memory to reuse the column names, types and
// partition columns in the table level schema.
properties schema    properties  tblschema clone
// inputformat
string inputformat   sd getinputformat
if  inputformat    null    inputformat length      0
string tblinput
schema getproperty org apache hadoop hive metastore api hive_metastoreconstants file_input_format
if  tblinput    null
inputformat   org apache hadoop mapred sequencefileinputformat class getname
else
inputformat   tblinput
schema setproperty org apache hadoop hive metastore api hive_metastoreconstants file_input_format
inputformat
// outputformat
string outputformat   sd getoutputformat
if  outputformat    null    outputformat length      0
string tbloutput
schema getproperty org apache hadoop hive metastore api hive_metastoreconstants file_output_format
if  tbloutput    null
outputformat   org apache hadoop mapred sequencefileoutputformat class getname
else
outputformat   tbloutput
schema setproperty org apache hadoop hive metastore api hive_metastoreconstants file_output_format
outputformat
// location
if  sd getlocation      null
schema setproperty org apache hadoop hive metastore api hive_metastoreconstants meta_table_location
sd getlocation
// bucket count
schema setproperty org apache hadoop hive metastore api hive_metastoreconstants bucket_count
integer tostring sd getnumbuckets
if  sd getbucketcols      null    sd getbucketcols   size   > 0
schema setproperty org apache hadoop hive metastore api hive_metastoreconstants bucket_field_name
sd getbucketcols   get 0
// serdeinfo
if  sd getserdeinfo      null
// we should not update the following 3 values if serdeinfo contains these.
// this is to keep backward compatible with getschema(), where these 3 keys
// are updated after serdeinfo properties got copied.
string cols   org apache hadoop hive metastore api hive_metastoreconstants meta_table_columns
string coltypes   org apache hadoop hive metastore api hive_metastoreconstants meta_table_column_types
string parts   org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
for  map entry<string string> param   sd getserdeinfo   getparameters   entryset
string key   param getkey
if  schema get key     null
key equals cols     key equals coltypes     key equals parts
continue
schema put key   param getvalue      null  ? param getvalue
if  sd getserdeinfo   getserializationlib      null
schema setproperty org apache hadoop hive serde serdeconstants serialization_lib
sd getserdeinfo   getserializationlib
// skipping columns since partition level field schemas are the same as table level's
// skipping partition keys since it is the same as table level partition keys
if  parameters    null
for  entry<string  string> e   parameters entryset
schema setproperty e getkey    e getvalue
return schema
public static properties getschema
org apache hadoop hive metastore api storagedescriptor sd
org apache hadoop hive metastore api storagedescriptor tblsd
map<string  string> parameters  string databasename  string tablename
list<fieldschema> partitionkeys
properties schema   new properties
string inputformat   sd getinputformat
if  inputformat    null    inputformat length      0
inputformat   org apache hadoop mapred sequencefileinputformat class
getname
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants file_input_format
inputformat
string outputformat   sd getoutputformat
if  outputformat    null    outputformat length      0
outputformat   org apache hadoop mapred sequencefileoutputformat class
getname
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants file_output_format
outputformat
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_name
databasename       tablename
if  sd getlocation      null
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_location
sd getlocation
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants bucket_count  integer
tostring sd getnumbuckets
if  sd getbucketcols      null    sd getbucketcols   size   > 0
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants bucket_field_name  sd
getbucketcols   get 0
if  sd getserdeinfo      null
for  map entry<string string> param   sd getserdeinfo   getparameters   entryset
schema put param getkey     param getvalue      null  ? param getvalue
if  sd getserdeinfo   getserializationlib      null
schema setproperty
org apache hadoop hive serde serdeconstants serialization_lib  sd
getserdeinfo   getserializationlib
stringbuilder colnamebuf   new stringbuilder
stringbuilder coltypebuf   new stringbuilder
boolean first   true
for  fieldschema col   tblsd getcols
if   first
colnamebuf append
coltypebuf append
colnamebuf append col getname
coltypebuf append col gettype
first   false
string colnames   colnamebuf tostring
string coltypes   coltypebuf tostring
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_columns
colnames
schema setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_column_types
coltypes
if  sd getcols      null
schema setproperty
org apache hadoop hive serde serdeconstants serialization_ddl
getddlfromfieldschema tablename  sd getcols
string partstring
string partstringsep
for  fieldschema partkey   partitionkeys
partstring   partstring concat partstringsep
partstring   partstring concat partkey getname
if  partstringsep length      0
partstringsep
if  partstring length   > 0
schema
setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_partition_columns
partstring
if  parameters    null
for  entry<string  string> e   parameters entryset
// add non-null parameters to the schema
if   e getvalue      null
schema setproperty e getkey    e getvalue
return schema
/**
* convert fieldschemas to columnnames.
*/
public static string getcolumnnamesfromfieldschema
list<fieldschema> fieldschemas
stringbuilder sb   new stringbuilder
for  int i   0  i < fieldschemas size    i
if  i > 0
sb append
sb append fieldschemas get i  getname
return sb tostring
/**
* convert fieldschemas to columntypes.
*/
public static string getcolumntypesfromfieldschema
list<fieldschema> fieldschemas
stringbuilder sb   new stringbuilder
for  int i   0  i < fieldschemas size    i
if  i > 0
sb append
sb append fieldschemas get i  gettype
return sb tostring
public static void makedir path path  hiveconf hiveconf  throws metaexception
filesystem fs
try
fs   path getfilesystem hiveconf
if   fs exists path
fs mkdirs path
catch  ioexception e
throw new metaexception     path
public static void startmetastore final int port
final hadoopthriftauthbridge bridge  throws exception
thread thread   new thread new runnable
@override
public void run
try
hivemetastore startmetastore port  bridge
catch  throwable e
log error   e
thread setdaemon true
thread start
loopuntilhmsready port
/**
* a simple connect test to make sure that the metastore is up
* @throws exception
*/
private static void loopuntilhmsready int port  throws exception
int retries   0
exception exc   null
while  true
try
socket socket   new socket
socket connect new inetsocketaddress port   5000
socket close
return
catch  exception e
if  retries   > 6      give up
exc   e
break
thread sleep 10000
throw exc
/**
* finds a free port on the machine.
*
* @return
* @throws ioexception
*/
public static int findfreeport   throws ioexception
serversocket socket  new serversocket 0
int port   socket getlocalport
socket close
return port
/**
* catches exceptions that can't be handled and bundles them to metaexception
*
* @param e
* @throws metaexception
*/
static void logandthrowmetaexception exception e  throws metaexception
string exinfo       e getclass   getname
e getmessage
log error exinfo  e
log error
throw new metaexception exinfo
/**
* @param tablename
* @param deserializer
* @return the list of fields
* @throws serdeexception
* @throws metaexception
*/
public static list<fieldschema> getfieldsfromdeserializer string tablename
deserializer deserializer  throws serdeexception  metaexception
objectinspector oi   deserializer getobjectinspector
string names   tablename split
string last_name   names
for  int i   1  i < names length  i
if  oi instanceof structobjectinspector
structobjectinspector soi    structobjectinspector  oi
structfield sf   soi getstructfieldref names
if  sf    null
throw new metaexception     names
else
oi   sf getfieldobjectinspector
else if  oi instanceof listobjectinspector
names equalsignorecase
listobjectinspector loi    listobjectinspector  oi
oi   loi getlistelementobjectinspector
else if  oi instanceof mapobjectinspector
names equalsignorecase
mapobjectinspector moi    mapobjectinspector  oi
oi   moi getmapkeyobjectinspector
else if  oi instanceof mapobjectinspector
names equalsignorecase
mapobjectinspector moi    mapobjectinspector  oi
oi   moi getmapvalueobjectinspector
else
throw new metaexception     names
arraylist<fieldschema> str_fields   new arraylist<fieldschema>
// rules on how to recurse the objectinspector based on its type
if  oi getcategory      category struct
str_fields add new fieldschema last_name  oi gettypename
from_serializer
else
list<? extends structfield> fields     structobjectinspector  oi
getallstructfieldrefs
for  int i   0  i < fields size    i
structfield structfield   fields get i
string fieldname   structfield getfieldname
string fieldtypename   structfield getfieldobjectinspector   gettypename
string fieldcomment   determinefieldcomment structfield getfieldcomment
str_fields add new fieldschema fieldname  fieldtypename  fieldcomment
return str_fields
private static final string from_serializer
private static string determinefieldcomment string comment
return  comment    null    comment isempty    ? from_serializer   comment
/**
* convert typeinfo to fieldschema.
*/
public static fieldschema getfieldschemafromtypeinfo string fieldname
typeinfo typeinfo
return new fieldschema fieldname  typeinfo gettypename
/**
* determines whether a table is an external table.
*
* @param table table of interest
*
* @return true if external
*/
public static boolean isexternaltable table table
if  table    null
return false
map<string  string> params   table getparameters
if  params    null
return false
return   equalsignorecase params get
public static boolean isarchived
org apache hadoop hive metastore api partition part
map<string  string> params   part getparameters
if    equalsignorecase params get hive_metastoreconstants is_archived
return true
else
return false
public static path getoriginallocation
org apache hadoop hive metastore api partition part
map<string  string> params   part getparameters
assert isarchived part
string originallocation   params get hive_metastoreconstants original_location
assert  originallocation    null
return new path originallocation
public static boolean isnonnativetable table table
if  table    null
return false
return  table getparameters   get hive_metastoreconstants meta_table_storage     null
/**
* returns true if partial has the same values as full for all values that
* aren't empty in partial.
*/
public static boolean pvalmatches list<string> partial  list<string> full
if partial size   > full size
return false
iterator<string> p   partial iterator
iterator<string> f   full iterator
while p hasnext
string pval   p next
string fval   f next
if  pval length      0     pval equals fval
return false
return true
public static string getindextablename string dbname  string basetblname  string indexname
return dbname       basetblname       indexname
public static boolean isindextable table table
if  table    null
return false
return tabletype index_table tostring   equals table gettabletype
/**
* given a map of partition column names to values, this creates a filter
* string that can be used to call the *byfilter methods
* @param m
* @return the filter string
*/
public static string makefilterstringfrommap map<string  string> m
stringbuilder filter   new stringbuilder
for  entry<string  string> e   m entryset
string col   e getkey
string val   e getvalue
if  filter length      0
filter append col
else
filter append     col
return filter tostring
/**
* create listener instances as per the configuration.
*
* @param clazz
* @param conf
* @param listenerimpllist
* @return
* @throws metaexception
*/
static <t> list<t> getmetastorelisteners class<t> clazz
hiveconf conf  string listenerimpllist  throws metaexception
list<t> listeners   new arraylist<t>
listenerimpllist   listenerimpllist trim
if  listenerimpllist equals
return listeners
string listenerimpls   listenerimpllist split
for  string listenerimpl   listenerimpls
try
t listener    t  class forname
listenerimpl trim    true  javautils getclassloader    getconstructor
configuration class  newinstance conf
listeners add listener
catch  invocationtargetexception ie
throw new metaexception
listenerimpl       ie getcause
catch  exception e
throw new metaexception
listenerimpl       e
return listeners
public static class<?> getclass string rawstoreclassname
throws metaexception
try
return class forname rawstoreclassname  true  javautils getclassloader
catch  classnotfoundexception e
throw new metaexception rawstoreclassname
/**
* create an object of the given class.
* @param theclass
* @param parametertypes
*          an array of parametertypes for the constructor
* @param initargs
*          the list of arguments for the constructor
*/
public static <t> t newinstance class<t> theclass  class<?> parametertypes
object initargs
// perform some sanity checks on the arguments.
if  parametertypes length    initargs length
throw new illegalargumentexception
for  int i   0  i < parametertypes length  i
class<?> clazz   parametertypes
if    clazz isinstance initargs
throw new illegalargumentexception     initargs
clazz
try
constructor<t> meth   theclass getdeclaredconstructor parametertypes
meth setaccessible true
return meth newinstance initargs
catch  exception e
throw new runtimeexception     theclass getname    e
public static void validatepartitionnamecharacters list<string> partvals
pattern partitionvalidationpattern  throws metaexception
string invalidpartitionval
getpartitionvalwithinvalidcharacter partvals  partitionvalidationpattern
if  invalidpartitionval    null
throw new metaexception     invalidpartitionval
partitionvalidationpattern tostring
hiveconf confvars metastore_partition_name_whitelist_pattern varname
public static boolean partitionnamehasvalidcharacters list<string> partvals
pattern partitionvalidationpattern
return getpartitionvalwithinvalidcharacter partvals  partitionvalidationpattern     null
/**
* @param schema1: the first schema to be compared
* @param schema2: the second schema to be compared
* @return true if the two schemas are the same else false
*         for comparing a field we ignore the comment it has
*/
public static boolean comparefieldcolumns list<fieldschema> schema1  list<fieldschema> schema2
if  schema1 size      schema2 size
return false
for  int i   0  i < schema1 size    i
fieldschema f1   schema1 get i
fieldschema f2   schema2 get i
// the default equals provided by thrift compares the comments too for
// equality, thus we need to compare the relevant fields here.
if  f1 getname      null
if  f2 getname      null
return false
else if   f1 getname   equals f2 getname
return false
if  f1 gettype      null
if  f2 gettype      null
return false
else if   f1 gettype   equals f2 gettype
return false
return true
/**
* read and return the meta store sasl configuration. currently it uses the default
* hadoop sasl configuration and can be configured using "hadoop.rpc.protection"
* @param conf
* @return the sasl configuration
*/
public static map<string  string> getmetastoresaslproperties hiveconf conf
// as of now hive meta store uses the same configuration as hadoop sasl configuration
return shimloader gethadoopthriftauthbridge   gethadoopsaslproperties conf
private static string getpartitionvalwithinvalidcharacter list<string> partvals
pattern partitionvalidationpattern
if  partitionvalidationpattern    null
return null
for  string partval   partvals
if   partitionvalidationpattern matcher partval  matches
return partval
return null