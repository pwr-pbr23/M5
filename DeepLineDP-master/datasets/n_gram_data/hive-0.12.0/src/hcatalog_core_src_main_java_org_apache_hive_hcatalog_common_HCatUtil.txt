/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing,
* software distributed under the license is distributed on an
* "as is" basis, without warranties or conditions of any
* kind, either express or implied.  see the license for the
* specific language governing permissions and limitations
* under the license.
*/
package org apache hive hcatalog common
import java io bytearrayinputstream
import java io bytearrayoutputstream
import java io ioexception
import java io objectinputstream
import java io objectoutputstream
import java io serializable
import java util arraylist
import java util hashmap
import java util linkedlist
import java util list
import java util map
import java util properties
import org apache hadoop classification interfaceaudience
import org apache hadoop classification interfacestability
import org apache hadoop conf configuration
import org apache hadoop fs permission fsaction
import org apache hadoop hive common javautils
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore hivemetastoreclient
import org apache hadoop hive metastore metastoreutils
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive metastore api nosuchobjectexception
import org apache hadoop hive ql io ignorekeytextoutputformat
import org apache hadoop hive ql metadata hivestoragehandler
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql plan tabledesc
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive serde2 typeinfo typeinfoutils
import org apache hadoop hive thrift delegationtokenidentifier
import org apache hadoop io text
import org apache hadoop mapred jobclient
import org apache hadoop mapred jobconf
import org apache hadoop mapreduce jobcontext
import org apache hadoop security token token
import org apache hadoop security token delegation abstractdelegationtokenidentifier
import org apache hadoop util reflectionutils
import org apache hive hcatalog data pair
import org apache hive hcatalog data schema hcatfieldschema
import org apache hive hcatalog data schema hcatschema
import org apache hive hcatalog data schema hcatschemautils
import org apache hive hcatalog mapreduce fosterstoragehandler
import org apache hive hcatalog mapreduce hcatoutputformat
import org apache hive hcatalog mapreduce inputjobinfo
import org apache hive hcatalog mapreduce outputjobinfo
import org apache hive hcatalog mapreduce partinfo
import org apache hive hcatalog mapreduce storerinfo
import org apache thrift texception
import org slf4j logger
import org slf4j loggerfactory
import javax security auth login loginexception
public class hcatutil
private static final logger log   loggerfactory getlogger hcatutil class
private static volatile hiveclientcache hiveclientcache
private final static int default_hive_cache_expiry_time_seconds   2   60
public static boolean checkjobcontextifrunningfrombackend jobcontext j
if  j getconfiguration   get       equals
equals j getconfiguration   get
return false
return true
public static string serialize serializable obj  throws ioexception
if  obj    null
return
try
bytearrayoutputstream serialobj   new bytearrayoutputstream
objectoutputstream objstream   new objectoutputstream serialobj
objstream writeobject obj
objstream close
return encodebytes serialobj tobytearray
catch  exception e
throw new ioexception     e getmessage    e
public static object deserialize string str  throws ioexception
if  str    null    str length      0
return null
try
bytearrayinputstream serialobj   new bytearrayinputstream
decodebytes str
objectinputstream objstream   new objectinputstream serialobj
return objstream readobject
catch  exception e
throw new ioexception     e getmessage    e
public static string encodebytes byte bytes
stringbuffer strbuf   new stringbuffer
for  int i   0  i < bytes length  i
strbuf append  char     bytes >> 4    0xf
strbuf append  char     bytes    0xf
return strbuf tostring
public static byte decodebytes string str
byte bytes   new byte
for  int i   0  i < str length    i    2
char c   str charat i
bytes    byte    c      << 4
c   str charat i   1
bytes     c
return bytes
public static list<hcatfieldschema> gethcatfieldschemalist
fieldschema    fields  throws hcatexception
list<hcatfieldschema> result   new arraylist<hcatfieldschema>
fields length
for  fieldschema f   fields
result add hcatschemautils gethcatfieldschema f
return result
public static list<hcatfieldschema> gethcatfieldschemalist
list<fieldschema> fields  throws hcatexception
if  fields    null
return null
else
list<hcatfieldschema> result   new arraylist<hcatfieldschema>
for  fieldschema f   fields
result add hcatschemautils gethcatfieldschema f
return result
public static hcatschema extractschema table table  throws hcatexception
return new hcatschema hcatutil gethcatfieldschemalist table getcols
public static hcatschema extractschema partition partition  throws hcatexception
return new hcatschema hcatutil gethcatfieldschemalist partition getcols
public static list<fieldschema> getfieldschemalist
list<hcatfieldschema> hcatfields
if  hcatfields    null
return null
else
list<fieldschema> result   new arraylist<fieldschema>
for  hcatfieldschema f   hcatfields
result add hcatschemautils getfieldschema f
return result
public static table gettable hivemetastoreclient client  string dbname  string tablename
throws nosuchobjectexception  texception  metaexception
return new table client gettable dbname  tablename
public static hcatschema gettableschemawithptncols table table  throws ioexception
hcatschema tableschema   new hcatschema hcatutil gethcatfieldschemalist table getcols
if  table getpartitionkeys   size      0
// add partition keys to table schema
// note : this assumes that we do not ever have ptn keys as columns
// inside the table schema as well!
for  fieldschema fs   table getpartitionkeys
tableschema append hcatschemautils gethcatfieldschema fs
return tableschema
/**
* return the partition columns from a table instance
*
* @param table the instance to extract partition columns from
* @return hcatschema instance which contains the partition columns
* @throws ioexception
*/
public static hcatschema getpartitioncolumns table table  throws ioexception
hcatschema cols   new hcatschema new linkedlist<hcatfieldschema>
if  table getpartitionkeys   size      0
for  fieldschema fs   table getpartitionkeys
cols append hcatschemautils gethcatfieldschema fs
return cols
/**
* validate partition schema, checks if the column types match between the
* partition and the existing table schema. returns the list of columns
* present in the partition but not in the table.
*
* @param table the table
* @param partitionschema the partition schema
* @return the list of newly added fields
* @throws ioexception signals that an i/o exception has occurred.
*/
public static list<fieldschema> validatepartitionschema table table
hcatschema partitionschema  throws ioexception
map<string  fieldschema> partitionkeymap   new hashmap<string  fieldschema>
for  fieldschema field   table getpartitionkeys
partitionkeymap put field getname   tolowercase    field
list<fieldschema> tablecols   table getcols
list<fieldschema> newfields   new arraylist<fieldschema>
for  int i   0  i < partitionschema getfields   size    i
fieldschema field   hcatschemautils getfieldschema partitionschema
getfields   get i
fieldschema tablefield
if  i < tablecols size
tablefield   tablecols get i
if   tablefield getname   equalsignorecase field getname
throw new hcatexception
errortype error_schema_column_mismatch
tablefield getname
i   1
field getname
else
tablefield   partitionkeymap get field getname   tolowercase
if  tablefield    null
throw new hcatexception
errortype error_schema_partition_key
field getname
if  tablefield    null
// field present in partition but not in table
newfields add field
else
// field present in both. validate type has not changed
typeinfo partitiontype   typeinfoutils
gettypeinfofromtypestring field gettype
typeinfo tabletype   typeinfoutils
gettypeinfofromtypestring tablefield gettype
if   partitiontype equals tabletype
throw new hcatexception
errortype error_schema_type_mismatch
field getname
tabletype gettypename
partitiontype gettypename
return newfields
/**
* test if the first fsaction is more permissive than the second. this is
* useful in cases where we want to ensure that a file owner has more
* permissions than the group they belong to, for eg. more completely(but
* potentially more cryptically) owner-r >= group-r >= world-r : bitwise
* and-masked with 0444 => 444 >= 440 >= 400 >= 000 owner-w >= group-w >=
* world-w : bitwise and-masked with &0222 => 222 >= 220 >= 200 >= 000
* owner-x >= group-x >= world-x : bitwise and-masked with &0111 => 111 >=
* 110 >= 100 >= 000
*
* @return true if first fsaction is more permissive than the second, false
*         if not.
*/
public static boolean validatemorepermissive fsaction first  fsaction second
if   first    fsaction all      second    fsaction none
first    second
return true
switch  first
case read_execute
return   second    fsaction read      second    fsaction execute
case read_write
return   second    fsaction read      second    fsaction write
case write_execute
return   second    fsaction write      second    fsaction execute
return false
/**
* ensure that read or write permissions are not granted without also
* granting execute permissions. essentially, r-- , rw- and -w- are invalid,
* r-x, -wx, rwx, ---, --x are valid
*
* @param perms the fsaction to verify
* @return true if the presence of read or write permission is accompanied
*         by execute permissions
*/
public static boolean validateexecutebitpresentifreadorwrite fsaction perms
if   perms    fsaction read      perms    fsaction write
perms    fsaction read_write
return false
return true
public static token<org apache hadoop mapreduce security token delegation delegationtokenidentifier> getjobtrackerdelegationtoken
configuration conf  string username  throws exception
// log.info("getjobtrackerdelegationtoken("+conf+","+username+")");
jobclient jcl   new jobclient new jobconf conf  hcatoutputformat class
token<org apache hadoop mapreduce security token delegation delegationtokenidentifier> t   jcl
getdelegationtoken new text username
// log.info("got "+t);
return t
// return null;
public static token<? extends abstractdelegationtokenidentifier> extractthrifttoken
string tokenstrform  string tokensignature  throws metaexception
texception  ioexception
// log.info("extractthrifttoken("+tokenstrform+","+tokensignature+")");
token<? extends abstractdelegationtokenidentifier> t   new token<delegationtokenidentifier>
t decodefromurlstring tokenstrform
t setservice new text tokensignature
// log.info("returning "+t);
return t
/**
* create an instance of a storage handler defined in storerinfo. if one cannot be found
* then fosterstoragehandler is used to encapsulate the inputformat, outputformat and serde.
* this storagehandler assumes the other supplied storage artifacts are for a file-based storage system.
* @param conf job's configuration will be used to configure the configurable storagehandler
* @param storerinfo storerinfo to definining the storagehandler and inputformat, outputformat and serde
* @return storagehandler instance
* @throws ioexception
*/
public static hivestoragehandler getstoragehandler configuration conf  storerinfo storerinfo  throws ioexception
return getstoragehandler conf
storerinfo getstoragehandlerclass
storerinfo getserdeclass
storerinfo getifclass
storerinfo getofclass
public static hivestoragehandler getstoragehandler configuration conf  partinfo partitioninfo  throws ioexception
return hcatutil getstoragehandler
conf
partitioninfo getstoragehandlerclassname
partitioninfo getserdeclassname
partitioninfo getinputformatclassname
partitioninfo getoutputformatclassname
/**
* create an instance of a storage handler. if storagehandler == null,
* then surrrogate storagehandler is used to encapsulate the inputformat, outputformat and serde.
* this storagehandler assumes the other supplied storage artifacts are for a file-based storage system.
* @param conf job's configuration will be used to configure the configurable storagehandler
* @param storagehandler fully qualified class name of the desired storagehandle instance
* @param serde fully qualified class name of the desired serde instance
* @param inputformat fully qualified class name of the desired inputformat instance
* @param outputformat fully qualified class name of the desired outputformat instance
* @return storagehandler instance
* @throws ioexception
*/
public static hivestoragehandler getstoragehandler configuration conf
string storagehandler
string serde
string inputformat
string outputformat
throws ioexception
if   storagehandler    null      storagehandler equals fosterstoragehandler class getname
try
fosterstoragehandler fosterstoragehandler
new fosterstoragehandler inputformat  outputformat  serde
fosterstoragehandler setconf conf
return fosterstoragehandler
catch  classnotfoundexception e
throw new ioexception
e
try
class<? extends hivestoragehandler> handlerclass
class<? extends hivestoragehandler>  class
forname storagehandler  true  javautils getclassloader
return  hivestoragehandler  reflectionutils newinstance
handlerclass  conf
catch  classnotfoundexception e
throw new ioexception
e getmessage    e
public static pair<string  string> getdbandtablename string tablename  throws ioexception
string dbtablenametokens   tablename split
if  dbtablenametokens length    1
return new pair<string  string> metastoreutils default_database_name  tablename
else if  dbtablenametokens length    2
return new pair<string  string> dbtablenametokens  dbtablenametokens
else
throw new ioexception
tablename
public static map<string  string>
getinputjobproperties hivestoragehandler storagehandler
inputjobinfo inputjobinfo
tabledesc tabledesc   new tabledesc storagehandler getserdeclass
storagehandler getinputformatclass
storagehandler getoutputformatclass
inputjobinfo gettableinfo   getstorerinfo   getproperties
if  tabledesc getjobproperties      null
tabledesc setjobproperties new hashmap<string  string>
properties mytableproperties   tabledesc getproperties
mytableproperties setproperty org apache hadoop hive metastore api hive_metastoreconstants meta_table_name inputjobinfo getdatabasename        inputjobinfo gettablename
map<string  string> jobproperties   new hashmap<string  string>
try
tabledesc getjobproperties   put
hcatconstants hcat_key_job_info
hcatutil serialize inputjobinfo
storagehandler configureinputjobproperties tabledesc
jobproperties
catch  ioexception e
throw new illegalstateexception
e
return jobproperties
@interfaceaudience private
@interfacestability evolving
public static void
configureoutputstoragehandler hivestoragehandler storagehandler
configuration conf
outputjobinfo outputjobinfo
//todo replace ignorekeytextoutputformat with a
//hiveoutputformatwrapper in storagehandler
tabledesc tabledesc   new tabledesc storagehandler getserdeclass
storagehandler getinputformatclass
ignorekeytextoutputformat class
outputjobinfo gettableinfo   getstorerinfo   getproperties
if  tabledesc getjobproperties      null
tabledesc setjobproperties new hashmap<string  string>
for  map entry<string  string> el   conf
tabledesc getjobproperties   put el getkey    el getvalue
properties mytableproperties   tabledesc getproperties
mytableproperties setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_name
outputjobinfo getdatabasename        outputjobinfo gettablename
map<string  string> jobproperties   new hashmap<string  string>
try
tabledesc getjobproperties   put
hcatconstants hcat_key_output_info
hcatutil serialize outputjobinfo
storagehandler configureoutputjobproperties tabledesc
jobproperties
map<string  string> tablejobproperties   tabledesc getjobproperties
if  tablejobproperties    null
if  tablejobproperties containskey hcatconstants hcat_key_output_info
string jobstring   tablejobproperties get hcatconstants hcat_key_output_info
if  jobstring    null
if    jobproperties containskey hcatconstants hcat_key_output_info
jobproperties put hcatconstants hcat_key_output_info
tablejobproperties get hcatconstants hcat_key_output_info
for  map entry<string  string> el   jobproperties entryset
conf set el getkey    el getvalue
catch  ioexception e
throw new illegalstateexception
e
/**
* replace the contents of dest with the contents of src
* @param src
* @param dest
*/
public static void copyconf configuration src  configuration dest
dest clear
for  map entry<string  string> el   src
dest set el getkey    el getvalue
/**
* get or create a hive client depending on whether it exits in cache or not
* @param hiveconf the hive configuration
* @return the client
* @throws metaexception when hivemetastoreclient couldn't be created
* @throws ioexception
*/
public static hivemetastoreclient gethiveclient hiveconf hiveconf
throws metaexception  ioexception
// singleton behaviour: create the cache instance if required. the cache needs to be created lazily and
// using the expiry time available in hiveconf.
if  hiveclientcache    null
synchronized  hivemetastoreclient class
if  hiveclientcache    null
hiveclientcache   new hiveclientcache hiveconf getint hcatconstants hcat_hive_client_expiry_time
default_hive_cache_expiry_time_seconds
try
return hiveclientcache get hiveconf
catch  loginexception e
throw new ioexception    e
public static void closehiveclientquietly hivemetastoreclient client
try
if  client    null
client close
catch  exception e
log debug    e
public static hiveconf gethiveconf configuration conf
throws ioexception
hiveconf hiveconf   new hiveconf conf  hcatutil class
//copy the hive conf into the job conf and restore it
//in the backend context
if  conf get hcatconstants hcat_key_hive_conf     null
conf set hcatconstants hcat_key_hive_conf
hcatutil serialize hiveconf getallproperties
else
//copy configuration properties into the hive conf
properties properties    properties  hcatutil deserialize
conf get hcatconstants hcat_key_hive_conf
for  map entry<object  object> prop   properties entryset
if  prop getvalue   instanceof string
hiveconf set  string  prop getkey     string  prop getvalue
else if  prop getvalue   instanceof integer
hiveconf setint  string  prop getkey
integer  prop getvalue
else if  prop getvalue   instanceof boolean
hiveconf setboolean  string  prop getkey
boolean  prop getvalue
else if  prop getvalue   instanceof long
hiveconf setlong  string  prop getkey     long  prop getvalue
else if  prop getvalue   instanceof float
hiveconf setfloat  string  prop getkey
float  prop getvalue
if  conf get hcatconstants hcat_key_token_signature     null
hiveconf set
conf get hcatconstants hcat_key_token_signature
return hiveconf
public static jobconf getjobconffromcontext jobcontext jobcontext
jobconf jobconf
// we need to convert the jobcontext into a jobconf
// 0.18 jobconf (hive) vs 0.20+ jobcontext (hcat)
// begin conversion..
jobconf   new jobconf jobcontext getconfiguration
// ..end of conversion
return jobconf
public static void copyjobpropertiestojobconf
map<string  string> jobproperties  jobconf jobconf
for  map entry<string  string> entry   jobproperties entryset
jobconf set entry getkey    entry getvalue
public static boolean ishadoop23
string version   org apache hadoop util versioninfo getversion
if  version matches     version matches
return true
return false