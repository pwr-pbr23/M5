/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import static org apache commons lang stringutils join
import static org apache hadoop util stringutils stringifyexception
import java io bufferedwriter
import java io dataoutput
import java io dataoutputstream
import java io filenotfoundexception
import java io ioexception
import java io outputstreamwriter
import java io serializable
import java io writer
import java net uri
import java net urisyntaxexception
import java util arraylist
import java util arrays
import java util collections
import java util comparator
import java util date
import java util hashmap
import java util iterator
import java util list
import java util map
import java util map entry
import java util set
import java util sortedset
import java util treeset
import org apache commons lang stringescapeutils
import org apache commons lang stringutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs fsdataoutputstream
import org apache hadoop fs filestatus
import org apache hadoop fs filesystem
import org apache hadoop fs fsshell
import org apache hadoop fs path
import org apache hadoop hive conf hiveconf
import org apache hadoop hive conf hiveconf confvars
import org apache hadoop hive metastore metastoreutils
import org apache hadoop hive metastore protectmode
import org apache hadoop hive metastore tabletype
import org apache hadoop hive metastore warehouse
import org apache hadoop hive metastore api alreadyexistsexception
import org apache hadoop hive metastore api database
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive metastore api hiveobjectprivilege
import org apache hadoop hive metastore api hiveobjectref
import org apache hadoop hive metastore api hiveobjecttype
import org apache hadoop hive metastore api index
import org apache hadoop hive metastore api invalidoperationexception
import org apache hadoop hive metastore api metaexception
import org apache hadoop hive metastore api nosuchobjectexception
import org apache hadoop hive metastore api order
import org apache hadoop hive metastore api principaltype
import org apache hadoop hive metastore api privilegebag
import org apache hadoop hive metastore api privilegegrantinfo
import org apache hadoop hive metastore api role
import org apache hadoop hive metastore api serdeinfo
import org apache hadoop hive metastore api skewedinfo
import org apache hadoop hive metastore api storagedescriptor
import org apache hadoop hive ql context
import org apache hadoop hive ql drivercontext
import org apache hadoop hive ql errormsg
import org apache hadoop hive ql queryplan
import org apache hadoop hive ql exec archiveutils partspecinfo
import org apache hadoop hive ql hooks readentity
import org apache hadoop hive ql hooks writeentity
import org apache hadoop hive ql io rcfile merge blockmergetask
import org apache hadoop hive ql io rcfile merge mergework
import org apache hadoop hive ql io rcfile truncate columntruncatetask
import org apache hadoop hive ql io rcfile truncate columntruncatework
import org apache hadoop hive ql lockmgr hivelock
import org apache hadoop hive ql lockmgr hivelockmanager
import org apache hadoop hive ql lockmgr hivelockmode
import org apache hadoop hive ql lockmgr hivelockobject
import org apache hadoop hive ql lockmgr hivelockobject hivelockobjectdata
import org apache hadoop hive ql metadata checkresult
import org apache hadoop hive ql metadata hive
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata hivemetastorechecker
import org apache hadoop hive ql metadata hivestoragehandler
import org apache hadoop hive ql metadata hiveutils
import org apache hadoop hive ql metadata invalidtableexception
import org apache hadoop hive ql metadata partition
import org apache hadoop hive ql metadata table
import org apache hadoop hive ql metadata formatting metadataformatutils
import org apache hadoop hive ql metadata formatting metadataformatter
import org apache hadoop hive ql parse altertablepartmergefilesdesc
import org apache hadoop hive ql parse basesemanticanalyzer
import org apache hadoop hive ql plan addpartitiondesc
import org apache hadoop hive ql plan alterdatabasedesc
import org apache hadoop hive ql plan alterindexdesc
import org apache hadoop hive ql plan altertablealterpartdesc
import org apache hadoop hive ql plan altertabledesc
import org apache hadoop hive ql plan altertabledesc altertabletypes
import org apache hadoop hive ql plan altertableexchangepartition
import org apache hadoop hive ql plan altertablesimpledesc
import org apache hadoop hive ql plan createdatabasedesc
import org apache hadoop hive ql plan createindexdesc
import org apache hadoop hive ql plan createtabledesc
import org apache hadoop hive ql plan createtablelikedesc
import org apache hadoop hive ql plan createviewdesc
import org apache hadoop hive ql plan ddlwork
import org apache hadoop hive ql plan descdatabasedesc
import org apache hadoop hive ql plan descfunctiondesc
import org apache hadoop hive ql plan desctabledesc
import org apache hadoop hive ql plan dropdatabasedesc
import org apache hadoop hive ql plan dropindexdesc
import org apache hadoop hive ql plan droptabledesc
import org apache hadoop hive ql plan grantdesc
import org apache hadoop hive ql plan grantrevokeroleddl
import org apache hadoop hive ql plan locktabledesc
import org apache hadoop hive ql plan msckdesc
import org apache hadoop hive ql plan partitionspec
import org apache hadoop hive ql plan principaldesc
import org apache hadoop hive ql plan privilegedesc
import org apache hadoop hive ql plan privilegeobjectdesc
import org apache hadoop hive ql plan renamepartitiondesc
import org apache hadoop hive ql plan revokedesc
import org apache hadoop hive ql plan roleddldesc
import org apache hadoop hive ql plan showcolumnsdesc
import org apache hadoop hive ql plan showcreatetabledesc
import org apache hadoop hive ql plan showdatabasesdesc
import org apache hadoop hive ql plan showfunctionsdesc
import org apache hadoop hive ql plan showgrantdesc
import org apache hadoop hive ql plan showindexesdesc
import org apache hadoop hive ql plan showlocksdesc
import org apache hadoop hive ql plan showpartitionsdesc
import org apache hadoop hive ql plan showtablestatusdesc
import org apache hadoop hive ql plan showtablesdesc
import org apache hadoop hive ql plan showtblpropertiesdesc
import org apache hadoop hive ql plan switchdatabasedesc
import org apache hadoop hive ql plan truncatetabledesc
import org apache hadoop hive ql plan unlocktabledesc
import org apache hadoop hive ql plan api stagetype
import org apache hadoop hive ql security authorization privilege
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 metadatatypedcolumnsetserde
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 serdeutils
import org apache hadoop hive serde2 columnar columnarserde
import org apache hadoop hive serde2 dynamic_type dynamicserde
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive shims hadoopshims
import org apache hadoop hive shims shimloader
import org apache hadoop io ioutils
import org apache hadoop util toolrunner
import org stringtemplate v4 st
/**
* ddltask implementation.
*
**/
public class ddltask extends task<ddlwork> implements serializable
private static final long serialversionuid   1l
private static final log log   logfactory getlog
transient hiveconf conf
private static final int separator   utilities tabcode
private static final int terminator   utilities newlinecode
// these are suffixes attached to intermediate directory names used in the
// archiving / un-archiving process.
private static string intermediate_archived_dir_suffix
private static string intermediate_original_dir_suffix
private static string intermediate_extracted_dir_suffix
private metadataformatter formatter
@override
public boolean requirelock
return this work    null    this work getneedlock
public ddltask
super
@override
public void initialize hiveconf conf  queryplan queryplan  drivercontext ctx
super initialize conf  queryplan  ctx
this conf   conf
// pick the formatter to use to display the results.  either the
// normal human readable output or a json object.
formatter   metadataformatutils getformatter conf
intermediate_archived_dir_suffix
hiveconf getvar conf  confvars metastore_int_archived
intermediate_original_dir_suffix
hiveconf getvar conf  confvars metastore_int_original
intermediate_extracted_dir_suffix
hiveconf getvar conf  confvars metastore_int_extracted
@override
public int execute drivercontext drivercontext
// create the db
hive db
try
db   hive get conf
createdatabasedesc createdatabasedesc   work getcreatedatabasedesc
if  null    createdatabasedesc
return createdatabase db  createdatabasedesc
dropdatabasedesc dropdatabasedesc   work getdropdatabasedesc
if  dropdatabasedesc    null
return dropdatabase db  dropdatabasedesc
switchdatabasedesc switchdatabasedesc   work getswitchdatabasedesc
if  switchdatabasedesc    null
return switchdatabase db  switchdatabasedesc
descdatabasedesc descdatabasedesc   work getdescdatabasedesc
if  descdatabasedesc    null
return descdatabase descdatabasedesc
alterdatabasedesc alterdatabasedesc   work getalterdatabasedesc
if  alterdatabasedesc    null
return alterdatabase alterdatabasedesc
createtabledesc crttbl   work getcreatetbldesc
if  crttbl    null
return createtable db  crttbl
createindexdesc crtindex   work getcreateindexdesc
if  crtindex    null
return createindex db  crtindex
alterindexdesc alterindex   work getalterindexdesc
if  alterindex    null
return alterindex db  alterindex
dropindexdesc dropidx   work getdropidxdesc
if  dropidx    null
return dropindex db  dropidx
createtablelikedesc crttbllike   work getcreatetbllikedesc
if  crttbllike    null
return createtablelike db  crttbllike
droptabledesc droptbl   work getdroptbldesc
if  droptbl    null
return droptable db  droptbl
altertabledesc altertbl   work getaltertbldesc
if  altertbl    null
return altertable db  altertbl
createviewdesc crtview   work getcreateviewdesc
if  crtview    null
return createview db  crtview
addpartitiondesc addpartitiondesc   work getaddpartitiondesc
if  addpartitiondesc    null
return addpartition db  addpartitiondesc
renamepartitiondesc renamepartitiondesc   work getrenamepartitiondesc
if  renamepartitiondesc    null
return renamepartition db  renamepartitiondesc
altertablesimpledesc simpledesc   work getaltertblsimpledesc
if  simpledesc    null
if  simpledesc gettype      altertabletypes touch
return touch db  simpledesc
else if  simpledesc gettype      altertabletypes archive
return archive db  simpledesc  drivercontext
else if  simpledesc gettype      altertabletypes unarchive
return unarchive db  simpledesc
msckdesc msckdesc   work getmsckdesc
if  msckdesc    null
return msck db  msckdesc
desctabledesc desctbl   work getdesctbldesc
if  desctbl    null
return describetable db  desctbl
descfunctiondesc descfunc   work getdescfunctiondesc
if  descfunc    null
return describefunction descfunc
showdatabasesdesc showdatabases   work getshowdatabasesdesc
if  showdatabases    null
return showdatabases db  showdatabases
showtablesdesc showtbls   work getshowtblsdesc
if  showtbls    null
return showtables db  showtbls
showcolumnsdesc showcols   work getshowcolumnsdesc
if  showcols    null
return showcolumns db  showcols
showtablestatusdesc showtblstatus   work getshowtblstatusdesc
if  showtblstatus    null
return showtablestatus db  showtblstatus
showtblpropertiesdesc showtblproperties   work getshowtblpropertiesdesc
if  showtblproperties    null
return showtableproperties db  showtblproperties
showfunctionsdesc showfuncs   work getshowfuncsdesc
if  showfuncs    null
return showfunctions showfuncs
showlocksdesc showlocks   work getshowlocksdesc
if  showlocks    null
return showlocks showlocks
locktabledesc locktbl   work getlocktbldesc
if  locktbl    null
return locktable locktbl
unlocktabledesc unlocktbl   work getunlocktbldesc
if  unlocktbl    null
return unlocktable unlocktbl
showpartitionsdesc showparts   work getshowpartsdesc
if  showparts    null
return showpartitions db  showparts
showcreatetabledesc showcreatetbl   work getshowcreatetbldesc
if  showcreatetbl    null
return showcreatetable db  showcreatetbl
roleddldesc roleddldesc   work getroleddldesc
if  roleddldesc    null
return roleddl roleddldesc
grantdesc grantdesc   work getgrantdesc
if  grantdesc    null
return grantorrevokeprivileges grantdesc getprincipals    grantdesc
getprivileges    grantdesc getprivilegesubjectdesc    grantdesc getgrantor    grantdesc getgrantortype    grantdesc isgrantoption    true
revokedesc revokedesc   work getrevokedesc
if  revokedesc    null
return grantorrevokeprivileges revokedesc getprincipals    revokedesc
getprivileges    revokedesc getprivilegesubjectdesc    null  null  false  false
showgrantdesc showgrantdesc   work getshowgrantdesc
if  showgrantdesc    null
return showgrants showgrantdesc
grantrevokeroleddl grantorrevokeroleddl   work getgrantrevokeroleddl
if  grantorrevokeroleddl    null
return grantorrevokerole grantorrevokeroleddl
showindexesdesc showindexes   work getshowindexesdesc
if  showindexes    null
return showindexes db  showindexes
altertablepartmergefilesdesc mergefilesdesc   work getmergefilesdesc
if  mergefilesdesc    null
return mergefiles db  mergefilesdesc
altertablealterpartdesc alterpartdesc   work getaltertablealterpartdesc
if alterpartdesc    null
return altertablealterpart db  alterpartdesc
truncatetabledesc truncatetabledesc   work gettruncatetbldesc
if  truncatetabledesc    null
return truncatetable db  truncatetabledesc
altertableexchangepartition altertableexchangepartition
work getaltertableexchangepartition
if  altertableexchangepartition    null
return exchangetablepartition db  altertableexchangepartition
catch  throwable e
setexception e
log error stringifyexception e
return 1
assert false
return 0
/**
* first, make sure the source table/partition is not
* archived/indexes/non-rcfile. if either of these is true, throw an
* exception.
*
* the way how it does the merge is to create a blockmergetask from the
* mergefilesdesc.
*
* @param db
* @param mergefilesdesc
* @return
* @throws hiveexception
*/
private int mergefiles hive db  altertablepartmergefilesdesc mergefilesdesc
throws hiveexception
// merge work only needs input and output.
mergework mergework   new mergework mergefilesdesc getinputdir
mergefilesdesc getoutputdir
mergework setlistbucketingctx mergefilesdesc getlbctx
mergework resolveconcatenatemerge db getconf
mergework setmappercannotspanpartns true
drivercontext drivercxt   new drivercontext
blockmergetask taskexec   new blockmergetask
taskexec initialize db getconf    null  drivercxt
taskexec setwork mergework
taskexec setqueryplan this getqueryplan
int ret   taskexec execute drivercxt
return ret
private int grantorrevokerole grantrevokeroleddl grantorrevokeroleddl
throws hiveexception
try
boolean grantrole   grantorrevokeroleddl getgrant
list<principaldesc> principals   grantorrevokeroleddl getprincipaldesc
list<string> roles   grantorrevokeroleddl getroles
for  principaldesc principal   principals
string username   principal getname
for  string rolename   roles
if  grantrole
db grantrole rolename  username  principal gettype
grantorrevokeroleddl getgrantor    grantorrevokeroleddl
getgrantortype    grantorrevokeroleddl isgrantoption
else
db revokerole rolename  username  principal gettype
catch  exception e
throw new hiveexception e
return 0
private int showgrants showgrantdesc showgrantdesc  throws hiveexception
dataoutput outstream   null
try
path resfile   new path showgrantdesc getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
principaldesc principaldesc   showgrantdesc getprincipaldesc
privilegeobjectdesc hiveobjectdesc   showgrantdesc gethiveobj
string principalname   principaldesc getname
if  hiveobjectdesc    null
list<hiveobjectprivilege> users   db showprivilegegrant
hiveobjecttype global  principalname  principaldesc gettype
null  null  null  null
if  users    null    users size   > 0
boolean first   true
sortprivileges users
for  hiveobjectprivilege usr   users
if   first
outstream write terminator
else
first   false
writegrantinfo outstream  principaldesc gettype    principalname
null  null  null  null  usr getgrantinfo
else
string obj   hiveobjectdesc getobject
boolean notfound   true
string dbname   null
string tablename   null
table tableobj   null
database dbobj   null
if  hiveobjectdesc gettable
string dbtab   obj split
if  dbtab length    2
dbname   dbtab
tablename   dbtab
else
dbname   sessionstate get   getcurrentdatabase
tablename   obj
dbobj   db getdatabase dbname
tableobj   db gettable dbname  tablename
notfound    dbobj    null    tableobj    null
else
dbname   hiveobjectdesc getobject
dbobj   db getdatabase dbname
notfound    dbobj    null
if  notfound
throw new hiveexception obj
string partname   null
list<string> partvalues   null
if  hiveobjectdesc getpartspec      null
partname   warehouse
makepartname hiveobjectdesc getpartspec    false
partvalues   warehouse getpartvaluesfrompartname partname
if   hiveobjectdesc gettable
// show database level privileges
list<hiveobjectprivilege> dbs   db showprivilegegrant hiveobjecttype database  principalname
principaldesc gettype    dbname  null  null  null
if  dbs    null    dbs size   > 0
boolean first   true
sortprivileges dbs
for  hiveobjectprivilege db   dbs
if   first
outstream write terminator
else
first   false
writegrantinfo outstream  principaldesc gettype    principalname
dbname  null  null  null  db getgrantinfo
else
if  showgrantdesc getcolumns      null
// show column level privileges
for  string columnname   showgrantdesc getcolumns
list<hiveobjectprivilege> columnss   db showprivilegegrant
hiveobjecttype column  principalname
principaldesc gettype    dbname  tablename  partvalues
columnname
if  columnss    null    columnss size   > 0
boolean first   true
sortprivileges columnss
for  hiveobjectprivilege col   columnss
if   first
outstream write terminator
else
first   false
writegrantinfo outstream  principaldesc gettype
principalname  dbname  tablename  partname  columnname
col getgrantinfo
else if  hiveobjectdesc getpartspec      null
// show partition level privileges
list<hiveobjectprivilege> parts   db showprivilegegrant
hiveobjecttype partition  principalname  principaldesc
gettype    dbname  tablename  partvalues  null
if  parts    null    parts size   > 0
boolean first   true
sortprivileges parts
for  hiveobjectprivilege part   parts
if   first
outstream write terminator
else
first   false
writegrantinfo outstream  principaldesc gettype
principalname  dbname  tablename  partname  null  part getgrantinfo
else
// show table level privileges
list<hiveobjectprivilege> tbls   db showprivilegegrant
hiveobjecttype table  principalname  principaldesc gettype
dbname  tablename  null  null
if  tbls    null    tbls size   > 0
boolean first   true
sortprivileges tbls
for  hiveobjectprivilege tbl   tbls
if   first
outstream write terminator
else
first   false
writegrantinfo outstream  principaldesc gettype
principalname  dbname  tablename  null  null  tbl getgrantinfo
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log info     stringifyexception e
return 1
catch  ioexception e
log info     stringifyexception e
return 1
catch  exception e
e printstacktrace
throw new hiveexception e
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
private static void sortprivileges list<hiveobjectprivilege> privileges
collections sort privileges  new comparator<hiveobjectprivilege>
@override
public int compare hiveobjectprivilege one  hiveobjectprivilege other
return one getgrantinfo   getprivilege   compareto other getgrantinfo   getprivilege
private int grantorrevokeprivileges list<principaldesc> principals
list<privilegedesc> privileges  privilegeobjectdesc privsubjectdesc
string grantor  principaltype grantortype  boolean grantoption  boolean isgrant
if  privileges    null    privileges size      0
console printerror
return 1
string dbname   null
string tablename   null
table tableobj   null
database dbobj   null
try
if  privsubjectdesc    null
if  privsubjectdesc getpartspec      null    isgrant
throw new hiveexception
string obj   privsubjectdesc getobject
boolean notfound   true
if  privsubjectdesc gettable
string dbtab   obj split
if  dbtab length    2
dbname   dbtab
tablename   dbtab
else
dbname   sessionstate get   getcurrentdatabase
tablename   obj
dbobj   db getdatabase dbname
tableobj   db gettable dbname  tablename
notfound    dbobj    null    tableobj    null
else
dbname   privsubjectdesc getobject
dbobj   db getdatabase dbname
notfound    dbobj    null
if  notfound
throw new hiveexception obj
privilegebag privbag   new privilegebag
if  privsubjectdesc    null
for  int idx   0  idx < privileges size    idx
privilege priv   privileges get idx  getprivilege
if  privileges get idx  getcolumns      null
privileges get idx  getcolumns   size   > 0
throw new hiveexception
privileges get idx  getcolumns   tostring
privbag addtoprivileges new hiveobjectprivilege new hiveobjectref
hiveobjecttype global  null  null  null  null   null  null
new privilegegrantinfo priv tostring    0  grantor  grantortype
grantoption
else
org apache hadoop hive metastore api partition partobj   null
list<string> partvalues   null
if  tableobj    null
if    tableobj ispartitioned
privsubjectdesc getpartspec      null
throw new hiveexception
privsubjectdesc getpartspec   tostring
if  privsubjectdesc getpartspec      null
partobj   db getpartition tableobj  privsubjectdesc getpartspec
false  gettpartition
partvalues   partobj getvalues
for  privilegedesc privdesc   privileges
list<string> columns   privdesc getcolumns
privilege priv   privdesc getprivilege
if  columns    null    columns size   > 0
if   priv supportcolumnlevel
throw new hiveexception priv tostring
if  privsubjectdesc    null    tablename    null
throw new hiveexception
columns
for  int i   0  i < columns size    i
privbag addtoprivileges new hiveobjectprivilege
new hiveobjectref hiveobjecttype column  dbname  tablename
partvalues  columns get i    null  null   new privilegegrantinfo priv tostring    0  grantor  grantortype  grantoption
else
if  privsubjectdesc gettable
if  privsubjectdesc getpartspec      null
privbag addtoprivileges new hiveobjectprivilege
new hiveobjectref hiveobjecttype partition  dbname
tablename  partvalues  null   null  null   new privilegegrantinfo priv tostring    0  grantor  grantortype  grantoption
else
privbag
addtoprivileges new hiveobjectprivilege
new hiveobjectref hiveobjecttype table  dbname
tablename  null  null   null  null  new privilegegrantinfo priv tostring    0  grantor  grantortype  grantoption
else
privbag addtoprivileges new hiveobjectprivilege
new hiveobjectref hiveobjecttype database  dbname  null
null  null   null  null  new privilegegrantinfo priv tostring    0  grantor  grantortype  grantoption
for  principaldesc principal   principals
for  int i   0  i < privbag getprivileges   size    i
hiveobjectprivilege objprivs   privbag getprivileges   get i
objprivs setprincipalname principal getname
objprivs setprincipaltype principal gettype
if  isgrant
db grantprivileges privbag
else
db revokeprivileges privbag
catch  exception e
console printerror     e getmessage
return 1
return 0
private int roleddl roleddldesc roleddldesc
roleddldesc roleoperation operation   roleddldesc getoperation
dataoutput outstream   null
try
if  operation equals roleddldesc roleoperation create_role
db createrole roleddldesc getname    roleddldesc getroleownername
else if  operation equals roleddldesc roleoperation drop_role
db droprole roleddldesc getname
else if  operation equals roleddldesc roleoperation show_role_grant
list<role> roles   db showrolegrant roleddldesc getname    roleddldesc
getprincipaltype
if  roles    null    roles size   > 0
path resfile   new path roleddldesc getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
for  role role   roles
outstream writebytes     role getrolename
outstream write terminator
fsdataoutputstream  outstream  close
outstream   null
else
throw new hiveexception
operation getoperationname
catch  hiveexception e
console printerror
operation getoperationname
roleddldesc getname         e getmessage
return 1
catch  ioexception e
log info     stringifyexception e
return 1
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
private int alterdatabase alterdatabasedesc alterdbdesc  throws hiveexception
string dbname   alterdbdesc getdatabasename
database database   db getdatabase dbname
map<string  string> newparams   alterdbdesc getdatabaseproperties
if  database    null
map<string  string> params   database getparameters
// if both old and new params are not null, merge them
if  params    null    newparams    null
params putall newparams
database setparameters params
else      if one of them is null  replace the old params with the new one
database setparameters newparams
db alterdatabase database getname    database
else
throw new hiveexception errormsg database_not_exists  dbname
return 0
private int dropindex hive db  dropindexdesc dropidx  throws hiveexception
db dropindex sessionstate get   getcurrentdatabase    dropidx gettablename
dropidx getindexname    true
return 0
private int createindex hive db  createindexdesc crtindex  throws hiveexception
if  crtindex getserde      null
validateserde crtindex getserde
db
createindex
crtindex gettablename    crtindex getindexname    crtindex getindextypehandlerclass
crtindex getindexedcols    crtindex getindextablename    crtindex getdeferredrebuild
crtindex getinputformat    crtindex getoutputformat    crtindex getserde
crtindex getstoragehandler    crtindex getlocation    crtindex getidxprops    crtindex gettblprops
crtindex getserdeprops    crtindex getcollitemdelim    crtindex getfielddelim    crtindex getfieldescape
crtindex getlinedelim    crtindex getmapkeydelim    crtindex getindexcomment
if  hiveutils getindexhandler conf  crtindex getindextypehandlerclass    usesindextable
string indextablename
crtindex getindextablename      null ? crtindex getindextablename
metastoreutils getindextablename sessionstate get   getcurrentdatabase
crtindex gettablename    crtindex getindexname
table indextable   db gettable indextablename
work getoutputs   add new writeentity indextable
return 0
private int alterindex hive db  alterindexdesc alterindex  throws hiveexception
string dbname   alterindex getdbname
string basetablename   alterindex getbasetablename
string indexname   alterindex getindexname
index idx   db getindex dbname  basetablename  indexname
switch alterindex getop
case addprops
idx getparameters   putall alterindex getprops
break
case updatetimestamp
try
map<string  string> props   new hashmap<string  string>
map<map<string  string>  long> basepartts   new hashmap<map<string  string>  long>
table basetbl   db gettable sessionstate get   getcurrentdatabase
basetablename
if  basetbl ispartitioned
list<partition> baseparts
if  alterindex getspec      null
baseparts   db getpartitions basetbl  alterindex getspec
else
baseparts   db getpartitions basetbl
if  baseparts    null
for  partition p   baseparts
filesystem fs   p getpartitionpath   getfilesystem db getconf
filestatus fss   fs getfilestatus p getpartitionpath
basepartts put p getspec    fss getmodificationtime
else
filesystem fs   basetbl getpath   getfilesystem db getconf
filestatus fss   fs getfilestatus basetbl getpath
basepartts put null  fss getmodificationtime
for  map<string  string> spec   basepartts keyset
if  spec    null
props put spec tostring    basepartts get spec  tostring
else
props put    basepartts get null  tostring
idx getparameters   putall props
catch  hiveexception e
throw new hiveexception
catch  ioexception e
throw new hiveexception
break
default
console printerror
return 1
// set last modified by properties
if   updatemodifiedparameters idx getparameters    conf
return 1
try
db alterindex dbname  basetablename  indexname  idx
catch  invalidoperationexception e
console printerror     e getmessage
log info     stringifyexception e
return 1
catch  hiveexception e
console printerror     e getmessage
return 1
return 0
/**
* add a partition to a table.
*
* @param db
*          database to add the partition to.
* @param addpartitiondesc
*          add this partition.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*/
private int addpartition hive db  addpartitiondesc addpartitiondesc  throws hiveexception
table tbl   db gettable addpartitiondesc getdbname    addpartitiondesc gettablename
// if the add partition was created with if not exists, then we should
// not throw an error if the specified part does exist.
partition checkpart   db getpartition tbl  addpartitiondesc getpartspec    false
if  checkpart    null    addpartitiondesc getifnotexists
return 0
if  addpartitiondesc getlocation      null
db createpartition tbl  addpartitiondesc getpartspec    null
addpartitiondesc getpartparams
addpartitiondesc getinputformat
addpartitiondesc getoutputformat
addpartitiondesc getnumbuckets
addpartitiondesc getcols
addpartitiondesc getserializationlib
addpartitiondesc getserdeparams
addpartitiondesc getbucketcols
addpartitiondesc getsortcols
else
if  tbl isview
throw new hiveexception
// set partition path relative to table
db createpartition tbl  addpartitiondesc getpartspec    new path tbl
getpath    addpartitiondesc getlocation     addpartitiondesc getpartparams
addpartitiondesc getinputformat
addpartitiondesc getoutputformat
addpartitiondesc getnumbuckets
addpartitiondesc getcols
addpartitiondesc getserializationlib
addpartitiondesc getserdeparams
addpartitiondesc getbucketcols
addpartitiondesc getsortcols
partition part   db
getpartition tbl  addpartitiondesc getpartspec    false
work getoutputs   add new writeentity part
return 0
/**
* rename a partition in a table
*
* @param db
*          database to rename the partition.
* @param renamepartitiondesc
*          rename old partition to new one.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*/
private int renamepartition hive db  renamepartitiondesc renamepartitiondesc  throws hiveexception
table tbl   db gettable renamepartitiondesc getdbname    renamepartitiondesc gettablename
partition oldpart   db getpartition tbl  renamepartitiondesc getoldpartspec    false
partition part   db getpartition tbl  renamepartitiondesc getoldpartspec    false
part setvalues renamepartitiondesc getnewpartspec
db renamepartition tbl  renamepartitiondesc getoldpartspec    part
partition newpart   db
getpartition tbl  renamepartitiondesc getnewpartspec    false
work getinputs   add new readentity oldpart
work getoutputs   add new writeentity newpart
return 0
/**
* alter partition column type in a table
*
* @param db
*          database to rename the partition.
* @param alterpartitiondesc
*          change partition column type.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*/
private int altertablealterpart hive db  altertablealterpartdesc alterpartitiondesc
throws hiveexception
table tbl   db gettable alterpartitiondesc getdbname    alterpartitiondesc gettablename
string tabname   alterpartitiondesc gettablename
// this is checked by ddlsemanticanalyzer
assert tbl ispartitioned
list<fieldschema> newpartitionkeys   new arraylist<fieldschema>
for fieldschema col   tbl getttable   getpartitionkeys
if  col getname   compareto alterpartitiondesc getpartkeyspec   getname       0
newpartitionkeys add alterpartitiondesc getpartkeyspec
else
newpartitionkeys add col
tbl getttable   setpartitionkeys newpartitionkeys
try
db altertable tabname  tbl
catch  invalidoperationexception e
throw new hiveexception e  errormsg generic_error      tabname
work getinputs   add new readentity tbl
work getoutputs   add new writeentity tbl
return 0
/**
* rewrite the partition's metadata and force the pre/post execute hooks to
* be fired.
*
* @param db
* @param touchdesc
* @return
* @throws hiveexception
*/
private int touch hive db  altertablesimpledesc touchdesc
throws hiveexception
string dbname   touchdesc getdbname
string tblname   touchdesc gettablename
table tbl   db gettable dbname  tblname
if  touchdesc getpartspec      null
try
db altertable tblname  tbl
catch  invalidoperationexception e
throw new hiveexception
work getinputs   add new readentity tbl
work getoutputs   add new writeentity tbl
else
partition part   db getpartition tbl  touchdesc getpartspec    false
if  part    null
throw new hiveexception
try
db alterpartition tblname  part
catch  invalidoperationexception e
throw new hiveexception e
work getinputs   add new readentity part
work getoutputs   add new writeentity part
return 0
/**
* sets archiving flag locally; it has to be pushed into metastore
* @param p partition to set flag
* @param state desired state of is_archived flag
* @param level desired level for state == true, anything for false
*/
private void setisarchived partition p  boolean state  int level
map<string  string> params   p getparameters
if  state
params put org apache hadoop hive metastore api hive_metastoreconstants is_archived
params put archiveutils archiving_level  integer
tostring level
else
params remove org apache hadoop hive metastore api hive_metastoreconstants is_archived
params remove archiveutils archiving_level
/**
* returns original partition of archived partition, null for unarchived one
*/
private string getoriginallocation partition p
map<string  string> params   p getparameters
return params get
org apache hadoop hive metastore api hive_metastoreconstants original_location
/**
* sets original location of partition which is to be archived
*/
private void setoriginallocation partition p  string loc
map<string  string> params   p getparameters
if  loc    null
params remove org apache hadoop hive metastore api hive_metastoreconstants original_location
else
params put org apache hadoop hive metastore api hive_metastoreconstants original_location  loc
/**
* sets the appropriate attributes in the supplied partition object to mark
* it as archived. note that the metastore is not touched - a separate
* call to alter_partition is needed.
*
* @param p - the partition object to modify
* @param harpath - new location of partition (har schema uri)
*/
private void setarchived partition p  path harpath  int level
assert archiveutils isarchived p     false
setisarchived p  true  level
setoriginallocation p  p getlocation
p setlocation harpath tostring
/**
* sets the appropriate attributes in the supplied partition object to mark
* it as not archived. note that the metastore is not touched - a separate
* call to alter_partition is needed.
*
* @param p - the partition to modify
*/
private void setunarchived partition p
assert archiveutils isarchived p     true
string parentdir   getoriginallocation p
setisarchived p  false  0
setoriginallocation p  null
assert parentdir    null
p setlocation parentdir
private boolean pathexists path p  throws hiveexception
try
filesystem fs   p getfilesystem conf
return fs exists p
catch  ioexception e
throw new hiveexception e
private void movedir filesystem fs  path from  path to  throws hiveexception
try
if   fs rename from  to
throw new hiveexception     from       to
catch  ioexception e
throw new hiveexception e
private void deletedir path dir  throws hiveexception
try
warehouse wh   new warehouse conf
wh deletedir dir  true
catch  metaexception e
throw new hiveexception e
/**
* checks in partition is in custom (not-standard) location.
* @param tbl - table in which partition is
* @param p - partition
* @return true if partition location is custom, false if it is standard
*/
boolean partitionincustomlocation table tbl  partition p
throws hiveexception
string subdir   null
try
subdir   warehouse makepartname tbl getpartcols    p getvalues
catch  metaexception e
throw new hiveexception    e
uri tabledir   tbl getdatalocation
if tabledir    null
throw new hiveexception
string standardlocation    new path tabledir tostring    subdir   tostring
if archiveutils isarchived p
return  getoriginallocation p  equals standardlocation
else
return  p getlocation   equals standardlocation
private int archive hive db  altertablesimpledesc simpledesc
drivercontext drivercontext
throws hiveexception
string dbname   simpledesc getdbname
string tblname   simpledesc gettablename
table tbl   db gettable dbname  tblname
if  tbl gettabletype      tabletype managed_table
throw new hiveexception
map<string  string> partspec   simpledesc getpartspec
partspecinfo partspecinfo   partspecinfo create tbl  partspec
list<partition> partitions   db getpartitions tbl  partspec
path originaldir   null
// when we have partial partitions specification we must assume partitions
// lie in standard place - if they were in custom locations putting
// them into one archive would involve mass amount of copying
// in full partition specification case we allow custom locations
// to keep backward compatibility
if  partitions isempty
throw new hiveexception
else if partspecinfo values size      tbl getpartcols   size
// for partial specifications we need partitions to follow the scheme
for partition p  partitions
if partitionincustomlocation tbl  p
string message   string format
p getlocation
throw new hiveexception message
originaldir   partspecinfo createpath tbl
else
partition p   partitions get 0
// partition can be archived if during recovery
if archiveutils isarchived p
originaldir   new path getoriginallocation p
else
originaldir   p getpartitionpath
path intermediatearchiveddir   new path originaldir getparent
originaldir getname     intermediate_archived_dir_suffix
path intermediateoriginaldir   new path originaldir getparent
originaldir getname     intermediate_original_dir_suffix
console printinfo     intermediatearchiveddir tostring
console printinfo     intermediateoriginaldir tostring
string archivename
filesystem fs   null
try
fs   originaldir getfilesystem conf
catch  ioexception e
throw new hiveexception e
uri archiveuri    new path originaldir  archivename   touri
uri originaluri   archiveutils addslash originaldir touri
archiveutils harpathhelper harhelper   new archiveutils harpathhelper
conf  archiveuri  originaluri
// we checked if partitions matching specification are marked as archived
// in the metadata; if they are and their levels are the same as we would
// set it later it means previous run failed and we have to do the recovery;
// if they are different, we throw an error
for partition p  partitions
if archiveutils isarchived p
if archiveutils getarchivinglevel p     partspecinfo values size
string name   archiveutils getpartialname p  archiveutils getarchivinglevel p
string m   string format    name
throw new hiveexception m
else
throw new hiveexception
boolean recovery   false
if  pathexists intermediatearchiveddir
pathexists intermediateoriginaldir
recovery   true
console printinfo
// the following steps seem roundabout, but they are meant to aid in
// recovery if a failure occurs and to keep a consistent state in the fs
// steps:
// 1. create the archive in a temporary folder
// 2. move the archive dir to an intermediate dir that is in at the same
//    dir as the original partition dir. call the new dir
//    intermediate-archive.
// 3. rename the original partition dir to an intermediate dir. call the
//    renamed dir intermediate-original
// 4. rename intermediate-archive to the original partition dir
// 5. change the metadata
// 6. delete the original partition files in intermediate-original
// the original partition files are deleted after the metadata change
// because the presence of those files are used to indicate whether
// the original partition directory contains archived or unarchived files.
// create an archived version of the partition in a directory ending in
// archive_intermediate_dir_suffix that's the same level as the partition,
// if it does not already exist. if it does exist, we assume the dir is good
// to use as the move operation that created it is atomic.
hadoopshims shim   shimloader gethadoopshims
if   pathexists intermediatearchiveddir
pathexists intermediateoriginaldir
// first create the archive in a tmp dir so that if the job fails, the
// bad files don't pollute the filesystem
path tmppath   new path drivercontext getctx
getexternaltmpfileuri originaldir touri
console printinfo     archivename
originaldir tostring
console printinfo     tmppath
console printinfo
// create the hadoop archive
int ret 0
try
int maxjobnamelen   conf getintvar hiveconf confvars hivejobnamelength
string jobname   string format
tbl gettablename    partspecinfo getname
jobname   utilities abbreviate jobname  maxjobnamelen   6
conf setvar hiveconf confvars hadoopjobname  jobname
ret   shim createhadooparchive conf  originaldir  tmppath  archivename
catch  exception e
throw new hiveexception e
if  ret    0
throw new hiveexception
// move from the tmp dir to an intermediate directory, in the same level as
// the partition directory. e.g. .../hr=12-intermediate-archived
try
console printinfo     tmppath       intermediatearchiveddir
if  pathexists intermediatearchiveddir
throw new hiveexception
fs rename tmppath  intermediatearchiveddir
catch  ioexception e
throw new hiveexception
else
if  pathexists intermediatearchiveddir
console printinfo     intermediatearchiveddir
// if we get to here, we know that we've archived the partition files, but
// they may be in the original partition location, or in the intermediate
// original dir.
// move the original parent directory to the intermediate original directory
// if the move hasn't been made already
if   pathexists intermediateoriginaldir
console printinfo     originaldir
intermediateoriginaldir
movedir fs  originaldir  intermediateoriginaldir
else
console printinfo intermediateoriginaldir
// if there's a failure from here to when the metadata is updated,
// there will be no data in the partition, or an error while trying to read
// the partition (if the archive files have been moved to the original
// partition directory.) but re-running the archive command will allow
// recovery
// move the intermediate archived directory to the original parent directory
if   pathexists originaldir
console printinfo     intermediatearchiveddir
originaldir
movedir fs  intermediatearchiveddir  originaldir
else
console printinfo originaldir
// record this change in the metastore
try
for partition p  partitions
uri originalpartitionuri   archiveutils addslash p getpartitionpath   touri
uri test   p getpartitionpath   touri
uri harpartitiondir   harhelper getharuri originalpartitionuri  shim
path harpath   new path harpartitiondir getscheme
harpartitiondir getauthority
harpartitiondir getpath        make in path to ensure no slash at the end
setarchived p  harpath  partspecinfo values size
db alterpartition tblname  p
catch  exception e
throw new hiveexception    e
// if a failure occurs here, the directory containing the original files
// will not be deleted. the user will run archive again to clear this up
if pathexists intermediateoriginaldir
deletedir intermediateoriginaldir
if recovery
console printinfo
return 0
private int unarchive hive db  altertablesimpledesc simpledesc
throws hiveexception
string dbname   simpledesc getdbname
string tblname   simpledesc gettablename
table tbl   db gettable dbname  tblname
// means user specified a table, not a partition
if  simpledesc getpartspec      null
throw new hiveexception
if  tbl gettabletype      tabletype managed_table
throw new hiveexception
map<string  string> partspec   simpledesc getpartspec
partspecinfo partspecinfo   partspecinfo create tbl  partspec
list<partition> partitions   db getpartitions tbl  partspec
int partspeclevel   partspec size
path originaldir   null
// when we have partial partitions specification we must assume partitions
// lie in standard place - if they were in custom locations putting
// them into one archive would involve mass amount of copying
// in full partition specification case we allow custom locations
// to keep backward compatibility
if  partitions isempty
throw new hiveexception
else if partspecinfo values size      tbl getpartcols   size
// for partial specifications we need partitions to follow the scheme
for partition p  partitions
if partitionincustomlocation tbl  p
string message   string format
p getlocation
throw new hiveexception message
originaldir   partspecinfo createpath tbl
else
partition p   partitions get 0
if archiveutils isarchived p
originaldir   new path getoriginallocation p
else
originaldir   new path p getlocation
uri originaluri   archiveutils addslash originaldir touri
path intermediatearchiveddir   new path originaldir getparent
originaldir getname     intermediate_archived_dir_suffix
path intermediateextracteddir   new path originaldir getparent
originaldir getname     intermediate_extracted_dir_suffix
boolean recovery   false
if pathexists intermediatearchiveddir     pathexists intermediateextracteddir
recovery   true
console printinfo
for partition p  partitions
checkarchiveproperty partspeclevel  recovery  p
string archivename
filesystem fs   null
try
fs   originaldir getfilesystem conf
catch  ioexception e
throw new hiveexception e
// assume the archive is in the original dir, check if it exists
path archivepath   new path originaldir  archivename
uri archiveuri   archivepath touri
archiveutils harpathhelper harhelper   new archiveutils harpathhelper conf
archiveuri  originaluri
hadoopshims shim   shimloader gethadoopshims
uri sourceuri   harhelper getharuri originaluri  shim
path sourcedir   new path sourceuri getscheme    sourceuri getauthority    sourceuri getpath
if  pathexists intermediatearchiveddir      pathexists archivepath
throw new hiveexception
path tmppath   new path drivercontext
getctx
getexternaltmpfileuri originaldir touri
try
fs   tmppath getfilesystem conf
catch  ioexception e
throw new hiveexception e
// some sanity checks
if  originaldir    null
throw new hiveexception
// clarification of terms:
// - the originaldir directory represents the original directory of the
//   partitions' files. they now contain an archived version of those files
//   eg. hdfs:/warehouse/mytable/ds=1/
// - the source directory is the directory containing all the files that
//   should be in the partitions. e.g. har:/warehouse/mytable/ds=1/mytable.har/
//   note the har:/ scheme
// steps:
// 1. extract the archive in a temporary folder
// 2. move the archive dir to an intermediate dir that is in at the same
//    dir as originallocation. call the new dir intermediate-extracted.
// 3. rename the original partitions dir to an intermediate dir. call the
//    renamed dir intermediate-archive
// 4. rename intermediate-extracted to the original partitions dir
// 5. change the metadata
// 6. delete the archived partitions files in intermediate-archive
if   pathexists intermediateextracteddir
pathexists intermediatearchiveddir
try
// copy the files out of the archive into the temporary directory
string copysource   sourcedir tostring
string copydest   tmppath tostring
list<string> args   new arraylist<string>
args add
args add copysource
args add copydest
console printinfo     copysource       copydest
filesystem srcfs   filesystem get sourcedir touri    conf
srcfs initialize sourcedir touri    conf
fsshell fss   new fsshell conf
int ret   0
try
ret   toolrunner run fss  args toarray new string
catch  exception e
e printstacktrace
throw new hiveexception e
if  ret    0
throw new hiveexception     ret
else
console printinfo     copysource       copydest
console printinfo     tmppath       intermediateextracteddir
if  fs exists intermediateextracteddir
throw new hiveexception
fs rename tmppath  intermediateextracteddir
catch  exception e
throw new hiveexception e
// at this point, we know that the extracted files are in the intermediate
// extracted dir, or in the the original directory.
if   pathexists intermediatearchiveddir
try
console printinfo     originaldir       intermediatearchiveddir
fs rename originaldir  intermediatearchiveddir
catch  ioexception e
throw new hiveexception e
else
console printinfo intermediatearchiveddir
// if there is a failure from here to until when the metadata is changed,
// the partition will be empty or throw errors on read.
// if the original location exists here, then it must be the extracted files
// because in the previous step, we moved the previous original location
// (containing the archived version of the files) to intermediatearchivedir
if   pathexists originaldir
try
console printinfo     intermediateextracteddir       originaldir
fs rename intermediateextracteddir  originaldir
catch  ioexception e
throw new hiveexception e
else
console printinfo originaldir
for partition p  partitions
setunarchived p
try
db alterpartition tblname  p
catch  invalidoperationexception e
throw new hiveexception e
// if a failure happens here, the intermediate archive files won't be
// deleted. the user will need to call unarchive again to clear those up.
if pathexists intermediatearchiveddir
deletedir intermediatearchiveddir
if recovery
console printinfo
return 0
private void checkarchiveproperty int partspeclevel
boolean recovery  partition p  throws hiveexception
if   archiveutils isarchived p      recovery
throw new hiveexception     p getname
int archivelevel   archiveutils getarchivinglevel p
if  partspeclevel > archivelevel
throw new hiveexception     p getname
archivelevel
partspeclevel
/**
* metastorecheck, see if the data in the metastore matches what is on the
* dfs. current version checks for tables and partitions that are either
* missing on disk on in the metastore.
*
* @param db
*          the database in question.
* @param msckdesc
*          information about the tables and partitions we want to check for.
* @return returns 0 when execution succeeds and above 0 if it fails.
*/
private int msck hive db  msckdesc msckdesc
checkresult result   new checkresult
list<string> repairoutput   new arraylist<string>
try
hivemetastorechecker checker   new hivemetastorechecker db
table t   db newtable msckdesc gettablename
checker checkmetastore t getdbname    t gettablename    msckdesc getpartspecs    result
if  msckdesc isrepairpartitions
table table   db gettable msckdesc gettablename
for  checkresult partitionresult part   result getpartitionsnotinms
try
db createpartition table  warehouse makespecfromname part
getpartitionname
repairoutput add
msckdesc gettablename         part getpartitionname
catch  exception e
log warn    e
catch  hiveexception e
log warn    e
return 1
catch  ioexception e
log warn    e
return 1
finally
bufferedwriter resultout   null
try
path resfile   new path msckdesc getresfile
filesystem fs   resfile getfilesystem conf
resultout   new bufferedwriter new outputstreamwriter fs
create resfile
boolean firstwritten   false
firstwritten    writemsckresult result gettablesnotinms
resultout  firstwritten
firstwritten    writemsckresult result gettablesnotonfs
resultout  firstwritten
firstwritten    writemsckresult result getpartitionsnotinms
resultout  firstwritten
firstwritten    writemsckresult result getpartitionsnotonfs
resultout  firstwritten
for  string rout   repairoutput
if  firstwritten
resultout write terminator
else
firstwritten   true
resultout write rout
catch  ioexception e
log warn    e
return 1
finally
if  resultout    null
try
resultout close
catch  ioexception e
log warn    e
return 1
return 0
/**
* write the result of msck to a writer.
*
* @param result
*          the result we're going to write
* @param msg
*          message to write.
* @param out
*          writer to write to
* @param wrote
*          if any previous call wrote data
* @return true if something was written
* @throws ioexception
*           in case the writing fails
*/
private boolean writemsckresult list<? extends object> result  string msg
writer out  boolean wrote  throws ioexception
if   result isempty
if  wrote
out write terminator
out write msg
for  object entry   result
out write separator
out write entry tostring
return true
return false
/**
* write a list of partitions to a file.
*
* @param db
*          the database in question.
* @param showparts
*          these are the partitions we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showpartitions hive db  showpartitionsdesc showparts  throws hiveexception
// get the partitions for the table and populate the output
string tabname   showparts gettabname
table tbl   null
list<string> parts   null
tbl   db gettable tabname
if   tbl ispartitioned
throw new hiveexception errormsg table_not_partitioned  tabname
if  showparts getpartspec      null
parts   db getpartitionnames tbl getdbname
tbl gettablename    showparts getpartspec     short   1
else
parts   db getpartitionnames tbl getdbname    tbl gettablename     short   1
// write the results in the file
dataoutputstream outstream   null
try
path resfile   new path showparts getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
formatter showtablepartitons outstream  parts
outstream close
outstream   null
catch  exception e
throw new hiveexception e  errormsg generic_error      tabname
finally
ioutils closestream outstream
return 0
/**
* write a statement of how to create a table to a file.
*
* @param db
*          the database in question.
* @param showcreatetbl
*          this is the table we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showcreatetable hive db  showcreatetabledesc showcreatetbl  throws hiveexception
// get the create table statement for the table and populate the output
final string external
final string list_columns
final string tbl_comment
final string list_partitions
final string sort_bucket
final string row_format
final string tbl_location
final string tbl_properties
string tablename   showcreatetbl gettablename
table tbl   db gettable tablename  false
dataoutput outstream   null
list<string> duplicateprops   new arraylist<string>
try
path resfile   new path showcreatetbl getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
if  tbl isview
string createtab_stmt       tablename       tbl getviewexpandedtext
outstream writebytes createtab_stmt tostring
fsdataoutputstream  outstream  close
outstream   null
return 0
st createtab_stmt   new st     external
tablename
list_columns
tbl_comment
list_partitions
sort_bucket
row_format
tbl_location
tbl_properties
// for cases where the table is external
string tbl_external
if  tbl gettabletype      tabletype external_table
duplicateprops add
tbl_external
// columns
string tbl_columns
list<fieldschema> cols   tbl getcols
list<string> columns   new arraylist<string>
for  fieldschema col   cols
string columndesc       col getname         col gettype
if  col getcomment      null
columndesc   columndesc       escapehivecommand col getcomment
columns add columndesc
tbl_columns   stringutils join columns
// table comment
string tbl_comment
string tabcomment   tbl getproperty
if  tabcomment    null
duplicateprops add
tbl_comment       escapehivecommand tabcomment
// partitions
string tbl_partitions
list<fieldschema> partkeys   tbl getpartitionkeys
if  partkeys size   > 0
tbl_partitions
list<string> partcols   new arraylist<string>
for  fieldschema partkey   partkeys
string partcoldesc       partkey getname         partkey gettype
if  partkey getcomment      null
partcoldesc   partcoldesc
escapehivecommand partkey getcomment
partcols add partcoldesc
tbl_partitions    stringutils join partcols
tbl_partitions
// clusters (buckets)
string tbl_sort_bucket
list<string> buckcols   tbl getbucketcols
if  buckcols size   > 0
duplicateprops add
tbl_sort_bucket
tbl_sort_bucket    stringutils join buckcols
tbl_sort_bucket
list<order> sortcols   tbl getsortcols
if  sortcols size   > 0
tbl_sort_bucket
// order
list<string> sortkeys   new arraylist<string>
for  order sortcol   sortcols
string sortkeydesc       sortcol getcol
if  sortcol getorder      basesemanticanalyzer hive_column_order_asc
sortkeydesc   sortkeydesc
else if  sortcol getorder      basesemanticanalyzer hive_column_order_desc
sortkeydesc   sortkeydesc
sortkeys add sortkeydesc
tbl_sort_bucket    stringutils join sortkeys
tbl_sort_bucket
tbl_sort_bucket        tbl getnumbuckets
// row format (serde)
string tbl_row_format
storagedescriptor sd   tbl getttable   getsd
serdeinfo serdeinfo   sd getserdeinfo
tbl_row_format
if  tbl getstoragehandler      null
if  serdeinfo getparameterssize   > 1
// there is a "serialization.format" property by default,
// even with a delimited row format.
// but our result will only cover the following four delimiters.
tbl_row_format
map<string  string> delims   serdeinfo getparameters
// warn:
// if the four delimiters all exist in a create table query,
// this following order needs to be strictly followed,
// or the query will fail with a parseexception.
if  delims containskey serdeconstants field_delim
tbl_row_format
escapehivecommand stringescapeutils escapejava delims get
serdeconstants field_delim
if  delims containskey serdeconstants collection_delim
tbl_row_format
escapehivecommand stringescapeutils escapejava delims get
serdeconstants collection_delim
if  delims containskey serdeconstants mapkey_delim
tbl_row_format
escapehivecommand stringescapeutils escapejava delims get
serdeconstants mapkey_delim
if  delims containskey serdeconstants line_delim
tbl_row_format
escapehivecommand stringescapeutils escapejava delims get
serdeconstants line_delim
else
tbl_row_format
escapehivecommand serdeinfo getserializationlib
tbl_row_format
escapehivecommand sd getinputformat
tbl_row_format
escapehivecommand sd getoutputformat
else
duplicateprops add org apache hadoop hive metastore api hive_metastoreconstants meta_table_storage
tbl_row_format
escapehivecommand serdeinfo getserializationlib
tbl_row_format        escapehivecommand tbl getparameters   get
org apache hadoop hive metastore api hive_metastoreconstants meta_table_storage
// serde properties
if  serdeinfo getparameterssize   > 0
tbl_row_format
list<string> serdecols   new arraylist<string>
for  map entry<string  string> entry   serdeinfo getparameters   entryset
serdecols add     entry getkey
escapehivecommand stringescapeutils escapejava entry getvalue
tbl_row_format    stringutils join serdecols
tbl_row_format
string tbl_location       escapehivecommand sd getlocation
// table properties
string tbl_properties
map<string  string> properties   tbl getparameters
if  properties size   > 0
list<string> realprops   new arraylist<string>
for  string key   properties keyset
if  properties get key     null     duplicateprops contains key
realprops add     key
escapehivecommand stringescapeutils escapejava properties get key
tbl_properties    stringutils join realprops
createtab_stmt add external  tbl_external
createtab_stmt add list_columns  tbl_columns
createtab_stmt add tbl_comment  tbl_comment
createtab_stmt add list_partitions  tbl_partitions
createtab_stmt add sort_bucket  tbl_sort_bucket
createtab_stmt add row_format  tbl_row_format
createtab_stmt add tbl_location  tbl_location
createtab_stmt add tbl_properties  tbl_properties
outstream writebytes createtab_stmt render
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log info     stringifyexception e
return 1
catch  ioexception e
log info     stringifyexception e
return 1
catch  exception e
throw new hiveexception e
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* write a list of indexes to a file.
*
* @param db
*          the database in question.
* @param showindexes
*          these are the indexes we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showindexes hive db  showindexesdesc showindexes  throws hiveexception
// get the indexes for the table and populate the output
string tablename   showindexes gettablename
table tbl   null
list<index> indexes   null
tbl   db gettable tablename
indexes   db getindexes tbl getdbname    tbl gettablename     short   1
// write the results in the file
dataoutput outstream   null
try
path resfile   new path showindexes getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
if  showindexes isformatted
// column headers
outstream writebytes metadataformatutils getindexcolumnsheader
outstream write terminator
outstream write terminator
for  index index   indexes
outstream writebytes metadataformatutils getallcolumnsinformation index
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log info     stringifyexception e
throw new hiveexception e tostring
catch  ioexception e
log info     stringifyexception e
throw new hiveexception e tostring
catch  exception e
throw new hiveexception e tostring
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* write a list of the available databases to a file.
*
* @param showdatabasesdesc
*          these are the databases we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showdatabases hive db  showdatabasesdesc showdatabasesdesc  throws hiveexception
// get the databases for the desired pattern - populate the output stream
list<string> databases   null
if  showdatabasesdesc getpattern      null
log info     showdatabasesdesc getpattern
databases   db getdatabasesbypattern showdatabasesdesc getpattern
else
databases   db getalldatabases
log info     databases size
// write the results in the file
dataoutputstream outstream   null
try
path resfile   new path showdatabasesdesc getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
formatter showdatabases outstream  databases
outstream close
outstream   null
catch  exception e
throw new hiveexception e  errormsg generic_error
finally
ioutils closestream outstream
return 0
/**
* write a list of the tables in the database to a file.
*
* @param db
*          the database in question.
* @param showtbls
*          these are the tables we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showtables hive db  showtablesdesc showtbls  throws hiveexception
// get the tables for the desired pattenn - populate the output stream
list<string> tbls   null
string dbname   showtbls getdbname
if   db databaseexists dbname
throw new hiveexception errormsg database_not_exists  dbname
if  showtbls getpattern      null
log info     showtbls getpattern
tbls   db gettablesbypattern dbname  showtbls getpattern
log info     tbls size
else
tbls   db getalltables dbname
// write the results in the file
dataoutputstream outstream   null
try
path resfile   new path showtbls getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
sortedset<string> sortedtbls   new treeset<string> tbls
formatter showtables outstream  sortedtbls
outstream close
outstream   null
catch  exception e
throw new hiveexception e  errormsg generic_error      dbname
finally
ioutils closestream outstream
return 0
public int showcolumns hive db  showcolumnsdesc showcols
throws hiveexception
string dbname   showcols getdbname
string tablename   showcols gettablename
table table   null
if  dbname    null
table   db gettable tablename
else
table   db gettable dbname  tablename
// write the results in the file
dataoutput outstream   null
try
path resfile   new path showcols getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
list<fieldschema> cols   table getcols
cols addall table getpartcols
outstream writebytes
metadataformatutils getallcolumnsinformation cols  false
fsdataoutputstream  outstream  close
outstream   null
catch  ioexception e
throw new hiveexception e  errormsg generic_error
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* write a list of the user defined functions to a file.
*
* @param showfuncs
*          are the functions we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showfunctions showfunctionsdesc showfuncs  throws hiveexception
// get the tables for the desired pattenn - populate the output stream
set<string> funcs   null
if  showfuncs getpattern      null
log info     showfuncs getpattern
funcs   functionregistry getfunctionnames showfuncs getpattern
log info     funcs size
else
funcs   functionregistry getfunctionnames
// write the results in the file
dataoutput outstream   null
try
path resfile   new path showfuncs getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
sortedset<string> sortedfuncs   new treeset<string> funcs
// to remove the primitive types
sortedfuncs removeall serdeconstants primitivetypes
iterator<string> iterfuncs   sortedfuncs iterator
while  iterfuncs hasnext
// create a row per table name
outstream writebytes iterfuncs next
outstream write terminator
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log warn     stringifyexception e
return 1
catch  ioexception e
log warn     stringifyexception e
return 1
catch  exception e
throw new hiveexception e
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* write a list of the current locks to a file.
*
* @param showlocks
*          the locks we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showlocks showlocksdesc showlocks  throws hiveexception
context ctx   drivercontext getctx
hivelockmanager lockmgr   ctx gethivelockmgr
boolean isext   showlocks isext
if  lockmgr    null
throw new hiveexception
// write the results in the file
dataoutput outstream   null
try
path resfile   new path showlocks getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
list<hivelock> locks   null
if  showlocks gettablename      null
locks   lockmgr getlocks false  isext
else
locks   lockmgr getlocks gethiveobject showlocks gettablename
showlocks getpartspec
true  isext
collections sort locks  new comparator<hivelock>
@override
public int compare hivelock o1  hivelock o2
int cmp   o1 gethivelockobject   getname   compareto o2 gethivelockobject   getname
if  cmp    0
if  o1 gethivelockmode      o2 gethivelockmode
return cmp
// exclusive locks occur before shared locks
if  o1 gethivelockmode      hivelockmode exclusive
return  1
return  1
return cmp
iterator<hivelock> locksiter   locks iterator
while  locksiter hasnext
hivelock lock   locksiter next
outstream writebytes lock gethivelockobject   getdisplayname
outstream write separator
outstream writebytes lock gethivelockmode   tostring
if  isext
hivelockobjectdata lockdata   lock gethivelockobject   getdata
if  lockdata    null
outstream write terminator
outstream writebytes     lockdata getqueryid
outstream write terminator
outstream writebytes     lockdata getlocktime
outstream write terminator
outstream writebytes     lockdata getlockmode
outstream write terminator
outstream writebytes     lockdata getquerystr
outstream write terminator
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log warn     stringifyexception e
return 1
catch  ioexception e
log warn     stringifyexception e
return 1
catch  exception e
throw new hiveexception e tostring
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* lock the table/partition specified
*
* @param locktbl
*          the table/partition to be locked along with the mode
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int locktable locktabledesc locktbl  throws hiveexception
context ctx   drivercontext getctx
hivelockmanager lockmgr   ctx gethivelockmgr
if  lockmgr    null
throw new hiveexception
hivelockmode mode   hivelockmode valueof locktbl getmode
string tabname   locktbl gettablename
table  tbl   db gettable metastoreutils default_database_name  tabname
if  tbl    null
throw new hiveexception     tabname
map<string  string> partspec   locktbl getpartspec
hivelockobjectdata lockdata
new hivelockobjectdata locktbl getqueryid
string valueof system currenttimemillis
locktbl getquerystr
if  partspec    null
hivelock lck   lockmgr lock new hivelockobject tbl  lockdata   mode  true
if  lck    null
return 1
return 0
partition par   db getpartition tbl  partspec  false
if  par    null
throw new hiveexception     partspec       tabname
hivelock lck   lockmgr lock new hivelockobject par  lockdata   mode  true
if  lck    null
return 1
return 0
private hivelockobject gethiveobject string tabname
map<string  string> partspec  throws hiveexception
table  tbl   db gettable tabname
if  tbl    null
throw new hiveexception     tabname
hivelockobject obj   null
if   partspec    null
obj   new hivelockobject tbl  null
else
partition par   db getpartition tbl  partspec  false
if  par    null
throw new hiveexception     partspec       tabname
obj   new hivelockobject par  null
return obj
/**
* unlock the table/partition specified
*
* @param unlocktbl
*          the table/partition to be unlocked
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int unlocktable unlocktabledesc unlocktbl  throws hiveexception
context ctx   drivercontext getctx
hivelockmanager lockmgr   ctx gethivelockmgr
if  lockmgr    null
throw new hiveexception
string tabname   unlocktbl gettablename
hivelockobject obj   gethiveobject tabname  unlocktbl getpartspec
list<hivelock> locks   lockmgr getlocks obj  false  false
if   locks    null      locks isempty
throw new hiveexception     tabname
iterator<hivelock> locksiter   locks iterator
while  locksiter hasnext
hivelock lock   locksiter next
lockmgr unlock lock
return 0
/**
* shows a description of a function.
*
* @param descfunc
*          is the function we are describing
* @throws hiveexception
*/
private int describefunction descfunctiondesc descfunc  throws hiveexception
string funcname   descfunc getname
// write the results in the file
dataoutput outstream   null
try
path resfile   new path descfunc getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
// get the function documentation
description desc   null
class<?> funcclass   null
functioninfo functioninfo   functionregistry getfunctioninfo funcname
if  functioninfo    null
funcclass   functioninfo getfunctionclass
if  funcclass    null
desc   funcclass getannotation description class
if  desc    null
outstream writebytes desc value   replace    funcname
if  descfunc isextended
set<string> synonyms   functionregistry getfunctionsynonyms funcname
if  synonyms size   > 0
outstream writebytes     join synonyms
if  desc extended   length   > 0
outstream writebytes
desc extended   replace    funcname
else
if  funcclass    null
outstream writebytes
funcname
else
outstream writebytes     funcname
outstream write terminator
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log warn     stringifyexception e
return 1
catch  ioexception e
log warn     stringifyexception e
return 1
catch  exception e
throw new hiveexception e
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
private int descdatabase descdatabasedesc descdatabase  throws hiveexception
dataoutputstream outstream   null
try
path resfile   new path descdatabase getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
database database   db getdatabase descdatabase getdatabasename
if  database    null
throw new hiveexception errormsg database_not_exists  descdatabase getdatabasename
else
map<string  string> params   null
if descdatabase isext
params   database getparameters
formatter showdatabasedescription outstream
database getname
database getdescription
database getlocationuri
params
outstream close
outstream   null
catch  ioexception e
throw new hiveexception e  errormsg generic_error
finally
ioutils closestream outstream
return 0
/**
* write the status of tables to a file.
*
* @param db
*          the database in question.
* @param showtblstatus
*          tables we are interested in
* @return return 0 when execution succeeds and above 0 if it fails.
*/
private int showtablestatus hive db  showtablestatusdesc showtblstatus  throws hiveexception
// get the tables for the desired pattern - populate the output stream
list<table> tbls   new arraylist<table>
map<string  string> part   showtblstatus getpartspec
partition par   null
if  part    null
table tbl   db gettable showtblstatus getdbname    showtblstatus getpattern
par   db getpartition tbl  part  false
if  par    null
throw new hiveexception     part
showtblstatus getpattern
tbls add tbl
else
log info     showtblstatus getpattern
list<string> tblstr   db gettablesfordb showtblstatus getdbname
showtblstatus getpattern
sortedset<string> sortedtbls   new treeset<string> tblstr
iterator<string> itertbls   sortedtbls iterator
while  itertbls hasnext
// create a row per table name
string tblname   itertbls next
table tbl   db gettable showtblstatus getdbname    tblname
tbls add tbl
log info     tblstr size
// write the results in the file
dataoutputstream outstream   null
try
path resfile   new path showtblstatus getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
formatter showtablestatus outstream  db  conf  tbls  part  par
outstream close
outstream   null
catch  exception e
throw new hiveexception e  errormsg generic_error
finally
ioutils closestream outstream
return 0
/**
* write the properties of a table to a file.
*
* @param db
*          the database in question.
* @param showtblprpt
*          this is the table we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int showtableproperties hive db  showtblpropertiesdesc showtblprpt  throws hiveexception
string tablename   showtblprpt gettablename
// show table properties - populate the output stream
table tbl   db gettable tablename  false
dataoutput outstream   null
try
path resfile   new path showtblprpt getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
if  tbl    null
string errmsg       tablename
outstream write errmsg getbytes
fsdataoutputstream  outstream  close
outstream   null
return 0
log info     tbl gettablename
string propertyname   showtblprpt getpropertyname
if  propertyname    null
string propertyvalue   tbl getproperty propertyname
if  propertyvalue    null
string errmsg       tablename       propertyname
outstream write errmsg getbytes
else
outstream writebytes propertyvalue
else
map<string  string> properties   tbl getparameters
for  string key   properties keyset
writekeyvaluepair outstream  key  properties get key
log info     tbl gettablename
fsdataoutputstream  outstream  close
outstream   null
catch  filenotfoundexception e
log info     stringifyexception e
return 1
catch  ioexception e
log info     stringifyexception e
return 1
catch  exception e
throw new hiveexception e
finally
ioutils closestream  fsdataoutputstream  outstream
return 0
/**
* write the description of a table to a file.
*
* @param db
*          the database in question.
* @param desctbl
*          this is the table we're interested in.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int describetable hive db  desctabledesc desctbl  throws hiveexception
string colpath   desctbl getcolumnpath
string tablename   desctbl gettablename
// describe the table - populate the output stream
table tbl   db gettable tablename  false
partition part   null
dataoutputstream outstream   null
try
path resfile   new path desctbl getresfile
if  tbl    null
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
outstream close
outstream   null
throw new hiveexception errormsg invalid_table  tablename
if  desctbl getpartspec      null
part   db getpartition tbl  desctbl getpartspec    false
if  part    null
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
outstream close
outstream   null
throw new hiveexception errormsg invalid_partition
stringutils join desctbl getpartspec   keyset        tablename
tbl   part gettable
catch  ioexception e
throw new hiveexception e  errormsg generic_error  tablename
finally
ioutils closestream outstream
try
log info     tbl gettablename
path resfile   new path desctbl getresfile
filesystem fs   resfile getfilesystem conf
outstream   fs create resfile
list<fieldschema> cols   null
if  colpath equals tablename
cols    part    null    tbl gettabletype      tabletype virtual_view  ?
tbl getcols     part getcols
if   desctbl isformatted
if  tablename equals colpath
cols addall tbl getpartcols
else
cols   hive getfieldsfromdeserializer colpath  tbl getdeserializer
formatter describetable outstream  colpath  tablename  tbl  part  cols
desctbl isformatted    desctbl isext    desctbl ispretty
log info     tbl gettablename
outstream close
outstream   null
catch  ioexception e
throw new hiveexception e  errormsg generic_error  tablename
finally
ioutils closestream outstream
return 0
public static void writegrantinfo dataoutput outstream
principaltype principaltype  string principalname  string dbname
string tablename  string partname  string columnname
privilegegrantinfo grantinfo  throws ioexception
string privilege   grantinfo getprivilege
long unixtimestamp   grantinfo getcreatetime     1000l
date createtime   new date unixtimestamp
string grantor   grantinfo getgrantor
if  dbname    null
writekeyvaluepair outstream     dbname
if  tablename    null
writekeyvaluepair outstream     tablename
if  partname    null
writekeyvaluepair outstream     partname
if  columnname    null
writekeyvaluepair outstream     columnname
writekeyvaluepair outstream     principalname
writekeyvaluepair outstream         principaltype
writekeyvaluepair outstream     privilege
writekeyvaluepair outstream         createtime
if  grantor    null
writekeyvaluepair outstream     grantor
private static void writekeyvaluepair dataoutput outstream  string key
string value  throws ioexception
outstream write terminator
outstream writebytes key
outstream write separator
outstream writebytes value
outstream write separator
private void setalterprotectmode boolean protectmodeenable
altertabledesc protectmodetype protectmode
protectmode mode
if  protectmode    altertabledesc protectmodetype offline
mode offline   protectmodeenable
else if  protectmode    altertabledesc protectmodetype no_drop
mode nodrop   protectmodeenable
else if  protectmode    altertabledesc protectmodetype no_drop_cascade
mode nodropcascade   protectmodeenable
/**
* alter a given table.
*
* @param db
*          the database in question.
* @param altertbl
*          this is the table we're altering.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int altertable hive db  altertabledesc altertbl  throws hiveexception
// alter the table
table tbl   db gettable altertbl getoldname
partition part   null
list<partition> allpartitions   null
if  altertbl getpartspec      null
if  altertbl getop      altertabledesc altertabletypes alterprotectmode
part   db getpartition tbl  altertbl getpartspec    false
if  part    null
throw new hiveexception errormsg invalid_partition
stringutils join altertbl getpartspec   keyset             altertbl getoldname
else
allpartitions   db getpartitions tbl  altertbl getpartspec
table oldtbl   tbl copy
if  altertbl getop      altertabledesc altertabletypes rename
tbl settablename altertbl getnewname
else if  altertbl getop      altertabledesc altertabletypes addcols
list<fieldschema> newcols   altertbl getnewcols
list<fieldschema> oldcols   tbl getcols
if  tbl getserializationlib   equals
console
printinfo
tbl setserializationlib lazysimpleserde class getname
tbl getttable   getsd   setcols newcols
else
// make sure the columns does not already exist
iterator<fieldschema> iternewcols   newcols iterator
while  iternewcols hasnext
fieldschema newcol   iternewcols next
string newcolname   newcol getname
iterator<fieldschema> iteroldcols   oldcols iterator
while  iteroldcols hasnext
string oldcolname   iteroldcols next   getname
if  oldcolname equalsignorecase newcolname
throw new hiveexception errormsg duplicate_column_names  newcolname
oldcols add newcol
tbl getttable   getsd   setcols oldcols
else if  altertbl getop      altertabledesc altertabletypes renamecolumn
list<fieldschema> oldcols   tbl getcols
list<fieldschema> newcols   new arraylist<fieldschema>
iterator<fieldschema> iteroldcols   oldcols iterator
string oldname   altertbl getoldcolname
string newname   altertbl getnewcolname
string type   altertbl getnewcoltype
string comment   altertbl getnewcolcomment
boolean first   altertbl getfirst
string aftercol   altertbl getaftercol
fieldschema column   null
boolean found   false
int position    1
if  first
position   0
int i   1
while  iteroldcols hasnext
fieldschema col   iteroldcols next
string oldcolname   col getname
if  oldcolname equalsignorecase newname
oldcolname equalsignorecase oldname
throw new hiveexception errormsg duplicate_column_names  newname
else if  oldcolname equalsignorecase oldname
col setname newname
if  type    null     type trim   equals
col settype type
if  comment    null
col setcomment comment
found   true
if  first     aftercol    null     aftercol trim   equals
column   col
continue
if  aftercol    null     aftercol trim   equals
oldcolname equalsignorecase aftercol
position   i
i
newcols add col
// did not find the column
if   found
throw new hiveexception errormsg invalid_column  oldname
// after column is not null, but we did not find it.
if   aftercol    null     aftercol trim   equals        position < 0
throw new hiveexception errormsg invalid_column  aftercol
if  position >  0
newcols add position  column
tbl getttable   getsd   setcols newcols
else if  altertbl getop      altertabledesc altertabletypes replacecols
// change serde to lazysimpleserde if it is columnsetserde
if  tbl getserializationlib   equals
console
printinfo
tbl setserializationlib lazysimpleserde class getname
else if   tbl getserializationlib   equals
metadatatypedcolumnsetserde class getname
tbl getserializationlib   equals lazysimpleserde class getname
tbl getserializationlib   equals columnarserde class getname
tbl getserializationlib   equals dynamicserde class getname
throw new hiveexception errormsg cannot_replace_columns  altertbl getoldname
tbl getttable   getsd   setcols altertbl getnewcols
else if  altertbl getop      altertabledesc altertabletypes addprops
tbl getttable   getparameters   putall altertbl getprops
else if  altertbl getop      altertabledesc altertabletypes dropprops
iterator<string> keyitr   altertbl getprops   keyset   iterator
while  keyitr hasnext
tbl getttable   getparameters   remove keyitr next
else if  altertbl getop      altertabledesc altertabletypes addserdeprops
if  part    null
part gettpartition   getsd   getserdeinfo   getparameters   putall
altertbl getprops
else
tbl getttable   getsd   getserdeinfo   getparameters   putall
altertbl getprops
else if  altertbl getop      altertabledesc altertabletypes addserde
string serdename   altertbl getserdename
if  part    null
part gettpartition   getsd   getserdeinfo   setserializationlib serdename
if   altertbl getprops      null      altertbl getprops   size   > 0
part gettpartition   getsd   getserdeinfo   getparameters   putall
altertbl getprops
part gettpartition   getsd   setcols part gettpartition   getsd   getcols
else
tbl setserializationlib altertbl getserdename
if   altertbl getprops      null      altertbl getprops   size   > 0
tbl getttable   getsd   getserdeinfo   getparameters   putall
altertbl getprops
tbl setfields hive getfieldsfromdeserializer tbl gettablename    tbl
getdeserializer
else if  altertbl getop      altertabledesc altertabletypes addfileformat
if part    null
part gettpartition   getsd   setinputformat altertbl getinputformat
part gettpartition   getsd   setoutputformat altertbl getoutputformat
if  altertbl getserdename      null
part gettpartition   getsd   getserdeinfo   setserializationlib
altertbl getserdename
else
tbl getttable   getsd   setinputformat altertbl getinputformat
tbl getttable   getsd   setoutputformat altertbl getoutputformat
if  altertbl getserdename      null
tbl setserializationlib altertbl getserdename
else if  altertbl getop      altertabledesc altertabletypes alterprotectmode
boolean protectmodeenable   altertbl isprotectmodeenable
altertabledesc protectmodetype protectmode   altertbl getprotectmodetype
protectmode mode   null
if  allpartitions    null
for  partition tmppart  allpartitions
mode   tmppart getprotectmode
setalterprotectmode protectmodeenable  protectmode  mode
tmppart setprotectmode mode
else
mode   tbl getprotectmode
setalterprotectmode protectmodeenable protectmode  mode
tbl setprotectmode mode
else if  altertbl getop      altertabledesc altertabletypes addclustersortcolumn
// validate sort columns and bucket columns
list<string> columns   utilities getcolumnnamesfromfieldschema tbl
getcols
if   altertbl isturnoffsorting
utilities validatecolumnnames columns  altertbl getbucketcolumns
if  altertbl getsortcolumns      null
utilities validatecolumnnames columns  utilities
getcolumnnamesfromsortcols altertbl getsortcolumns
storagedescriptor sd   part    null ? tbl getttable   getsd     part gettpartition   getsd
if  altertbl isturnoffsorting
sd setsortcols new arraylist<order>
else if  altertbl getnumberbuckets       1
// -1 buckets means to turn off bucketing
sd setbucketcols new arraylist<string>
sd setnumbuckets  1
sd setsortcols new arraylist<order>
else
sd setbucketcols altertbl getbucketcolumns
sd setnumbuckets altertbl getnumberbuckets
sd setsortcols altertbl getsortcolumns
else if  altertbl getop      altertabledesc altertabletypes alterlocation
string newlocation   altertbl getnewlocation
try
uri locuri   new uri newlocation
if   locuri isabsolute      locuri getscheme      null
locuri getscheme   trim   equals
throw new hiveexception errormsg bad_location_value  newlocation
if  part    null
part setlocation newlocation
else
tbl setdatalocation locuri
catch  urisyntaxexception e
throw new hiveexception e
else if  altertbl getop      altertabledesc altertabletypes addskewedby
/* validation's been done at compile time. no validation is needed here. */
list<string> skewedcolnames   null
list<list<string>> skewedvalues   null
if  altertbl isturnoffskewed
/* convert skewed table to non-skewed table. */
skewedcolnames   new arraylist<string>
skewedvalues   new arraylist<list<string>>
else
skewedcolnames   altertbl getskewedcolnames
skewedvalues   altertbl getskewedcolvalues
if   null    tbl getskewedinfo
/* convert non-skewed table to skewed table. */
skewedinfo skewedinfo   new skewedinfo
skewedinfo setskewedcolnames skewedcolnames
skewedinfo setskewedcolvalues skewedvalues
tbl setskewedinfo skewedinfo
else
tbl setskewedcolnames skewedcolnames
tbl setskewedcolvalues skewedvalues
tbl setstoredassubdirectories altertbl isstoredassubdirectories
else if  altertbl getop      altertabledesc altertabletypes alterskewedlocation
// process location one-by-one
map<list<string> string> locmaps   altertbl getskewedlocations
set<list<string>> keys   locmaps keyset
for list<string> key keys
string newlocation   locmaps get key
try
uri locuri   new uri newlocation
if  part    null
list<string> slk   new arraylist<string> key
part setskewedvaluelocationmap slk  locuri tostring
else
list<string> slk   new arraylist<string> key
tbl setskewedvaluelocationmap slk  locuri tostring
catch  urisyntaxexception e
throw new hiveexception e
else if  altertbl getop      altertabletypes alterbucketnum
if  part    null
if  part getbucketcount      altertbl getnumberbuckets
return 0
part setbucketcount altertbl getnumberbuckets
else
if  tbl getnumbuckets      altertbl getnumberbuckets
return 0
tbl setnumbuckets altertbl getnumberbuckets
else
throw new hiveexception errormsg unsupported_alter_tbl_op  altertbl getop   tostring
if  part    null    allpartitions    null
updatemodifiedparameters tbl getttable   getparameters    conf
tbl checkvalidity
else if  part    null
updatemodifiedparameters part getparameters    conf
else
for  partition tmppart  allpartitions
updatemodifiedparameters tmppart getparameters    conf
try
if  part    null    allpartitions    null
db altertable altertbl getoldname    tbl
else if  part    null
db alterpartition tbl gettablename    part
else
db alterpartitions tbl gettablename    allpartitions
catch  invalidoperationexception e
log info     stringifyexception e
throw new hiveexception e  errormsg generic_error
// this is kind of hacky - the read entity contains the old table, whereas
// the write entity
// contains the new table. this is needed for rename - both the old and the
// new table names are
// passed
if part    null
work getinputs   add new readentity part
work getoutputs   add new writeentity part
else if  allpartitions    null
for  partition tmppart  allpartitions
work getinputs   add new readentity tmppart
work getoutputs   add new writeentity tmppart
else
work getinputs   add new readentity oldtbl
work getoutputs   add new writeentity tbl
return 0
/**
* drop a given table.
*
* @param db
*          the database in question.
* @param droptbl
*          this is the table we're dropping.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int droptable hive db  droptabledesc droptbl
throws hiveexception
// we need to fetch the table before it is dropped so that it can be passed
// to
// post-execution hook
table tbl   null
try
tbl   db gettable droptbl gettablename
catch  invalidtableexception e
// drop table is idempotent
if  droptbl getpartspecs      null
// this is a true drop table
if  tbl    null
if  tbl isview
if   droptbl getexpectview
if  droptbl getifexists
return 0
throw new hiveexception
else
if  droptbl getexpectview
if  droptbl getifexists
return 0
throw new hiveexception
if  tbl    null     tbl candrop
throw new hiveexception     tbl gettablename
int partitionbatchsize   hiveconf getintvar conf
confvars metastore_batch_retrieve_table_partition_max
// we should check that all the partitions of the table can be dropped
if  tbl    null    tbl ispartitioned
list<string> partitionnames   db getpartitionnames tbl gettablename     short  1
for int i 0  i < partitionnames size    i   partitionbatchsize
list<string> partnames   partitionnames sublist i  math min i partitionbatchsize
partitionnames size
list<partition> listpartitions   db getpartitionsbynames tbl  partnames
for  partition p  listpartitions
if   p candrop
throw new hiveexception     tbl gettablename
p getname
// drop the table
db droptable droptbl gettablename
if  tbl    null
work getoutputs   add new writeentity tbl
else
// this is actually an alter table drop partition
list<partition> partstodelete   new arraylist<partition>
for  partitionspec partspec   droptbl getpartspecs
list<partition> partitions   null
// getpartitionsbyfilter only works for string columns.
// till that is fixed, only equality will work for non-string columns.
if  droptbl isstringpartitioncolumns
try
partitions   db getpartitionsbyfilter tbl  partspec tostring
catch  exception e
throw new hiveexception e
else
partitions   db getpartitions tbl  partspec getpartspecwithoutoperator
// this is to prevent dropping archived partition which is archived in a
// different level the drop command specified.
int partprefixtodrop   0
for  fieldschema fs   tbl getpartcols
if  partspec existskey fs getname
partprefixtodrop    1
else
break
if   droptbl getignoreprotection
for  partition p   partitions
if   p candrop
throw new hiveexception     tbl gettablename
p getname
else if  archiveutils isarchived p
int partachivelevel   archiveutils getarchivinglevel p
// trying to drop partitions inside a har, disallow it.
if  partachivelevel < partprefixtodrop
throw new hiveexception
p getname
partstodelete addall partitions
// drop all existing partitions from the list
for  partition partition   partstodelete
console printinfo     partition getname
db droppartition droptbl gettablename    partition getvalues    true
work getoutputs   add new writeentity partition
return 0
/**
* update last_modified_by and last_modified_time parameters in parameter map.
*
* @param params
*          parameters.
* @param user
*          user that is doing the updating.
*/
private boolean updatemodifiedparameters map<string  string> params  hiveconf conf  throws hiveexception
string user   null
try
user   conf getuser
catch  ioexception e
throw new hiveexception e  errormsg generic_error
params put    user
params put    long tostring system currenttimemillis     1000
return true
/**
* check if the given serde is valid.
*/
private void validateserde string serdename  throws hiveexception
try
deserializer d   serdeutils lookupdeserializer serdename
if  d    null
log debug     serdename
catch  serdeexception e
throw new hiveexception     serdename  e
/**
* create a database
* @param db
* @param crtdb
* @return always returns 0
* @throws hiveexception
*/
private int createdatabase hive db  createdatabasedesc crtdb
throws hiveexception
database database   new database
database setname crtdb getname
database setdescription crtdb getcomment
database setlocationuri crtdb getlocationuri
database setparameters crtdb getdatabaseproperties
try
db createdatabase database  crtdb getifnotexists
catch  alreadyexistsexception ex
//it would be better if alreadyexistsexception had an errorcode field....
throw new hiveexception ex  errormsg databsae_already_exists  crtdb getname
return 0
/**
* drop a database
* @param db
* @param dropdb
* @return always returns 0
* @throws hiveexception
*/
private int dropdatabase hive db  dropdatabasedesc dropdb
throws hiveexception
try
db dropdatabase dropdb getdatabasename    true  dropdb getifexists    dropdb iscasdade
catch  nosuchobjectexception ex
throw new hiveexception ex  errormsg database_not_exists  dropdb getdatabasename
return 0
/**
* switch to a different database
* @param db
* @param switchdb
* @return always returns 0
* @throws hiveexception
*/
private int switchdatabase hive db  switchdatabasedesc switchdb
throws hiveexception
string dbname   switchdb getdatabasename
if   db databaseexists dbname
throw new hiveexception errormsg database_not_exists  dbname
sessionstate get   setcurrentdatabase dbname
// set database specific parameters
database database   db getdatabase dbname
assert database    null
map<string  string> dbparams   database getparameters
if  dbparams    null
for  hiveconf confvars var  hiveconf dbvars
string newvalue   dbparams get var varname
if  newvalue    null
log info     var varname
conf getvar var        newvalue
conf setvar var  newvalue
return 0
/**
* create a new table.
*
* @param db
*          the database in question.
* @param crttbl
*          this is the table we're creating.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int createtable hive db  createtabledesc crttbl  throws hiveexception
// create the table
table tbl   db newtable crttbl gettablename
if  crttbl gettblprops      null
tbl getttable   getparameters   putall crttbl gettblprops
if  crttbl getpartcols      null
tbl setpartcols crttbl getpartcols
if  crttbl getnumbuckets       1
tbl setnumbuckets crttbl getnumbuckets
if  crttbl getstoragehandler      null
tbl setproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_storage
crttbl getstoragehandler
hivestoragehandler storagehandler   tbl getstoragehandler
/*
* we use lazysimpleserde by default.
*
* if the user didn't specify a serde, and any of the columns are not simple
* types, we will have to use dynamicserde instead.
*/
if  crttbl getsername      null
if  storagehandler    null
log info     crttbl gettablename
tbl setserializationlib org apache hadoop hive serde2 lazy lazysimpleserde class getname
else
string serdeclassname   storagehandler getserdeclass   getname
log info     serdeclassname
crttbl gettablename
tbl setserializationlib serdeclassname
else
// let's validate that the serde exists
validateserde crttbl getsername
tbl setserializationlib crttbl getsername
if  crttbl getfielddelim      null
tbl setserdeparam serdeconstants field_delim  crttbl getfielddelim
tbl setserdeparam serdeconstants serialization_format  crttbl getfielddelim
if  crttbl getfieldescape      null
tbl setserdeparam serdeconstants escape_char  crttbl getfieldescape
if  crttbl getcollitemdelim      null
tbl setserdeparam serdeconstants collection_delim  crttbl getcollitemdelim
if  crttbl getmapkeydelim      null
tbl setserdeparam serdeconstants mapkey_delim  crttbl getmapkeydelim
if  crttbl getlinedelim      null
tbl setserdeparam serdeconstants line_delim  crttbl getlinedelim
if  crttbl getserdeprops      null
iterator<entry<string  string>> iter   crttbl getserdeprops   entryset
iterator
while  iter hasnext
entry<string  string> m   iter next
tbl setserdeparam m getkey    m getvalue
if  crttbl getcols      null
tbl setfields crttbl getcols
if  crttbl getbucketcols      null
tbl setbucketcols crttbl getbucketcols
if  crttbl getsortcols      null
tbl setsortcols crttbl getsortcols
if  crttbl getcomment      null
tbl setproperty    crttbl getcomment
if  crttbl getlocation      null
tbl setdatalocation new path crttbl getlocation    touri
if  crttbl getskewedcolnames      null
tbl setskewedcolnames crttbl getskewedcolnames
if  crttbl getskewedcolvalues      null
tbl setskewedcolvalues crttbl getskewedcolvalues
tbl setstoredassubdirectories crttbl isstoredassubdirectories
tbl setinputformatclass crttbl getinputformat
tbl setoutputformatclass crttbl getoutputformat
tbl getttable   getsd   setinputformat
tbl getinputformatclass   getname
tbl getttable   getsd   setoutputformat
tbl getoutputformatclass   getname
if  crttbl isexternal
tbl setproperty
tbl settabletype tabletype external_table
// if the sorted columns is a superset of bucketed columns, store this fact.
// it can be later used to
// optimize some group-by queries. note that, the order does not matter as
// long as it in the first
// 'n' columns where 'n' is the length of the bucketed columns.
if   tbl getbucketcols      null      tbl getsortcols      null
list<string> bucketcols   tbl getbucketcols
list<order> sortcols   tbl getsortcols
if   sortcols size   > 0      sortcols size   >  bucketcols size
boolean found   true
iterator<string> iterbucketcols   bucketcols iterator
while  iterbucketcols hasnext
string bucketcol   iterbucketcols next
boolean colfound   false
for  int i   0  i < bucketcols size    i
if  bucketcol equals sortcols get i  getcol
colfound   true
break
if  colfound    false
found   false
break
if  found
tbl setproperty
int rc   setgenerictableattributes tbl
if  rc    0
return rc
// create the table
db createtable tbl  crttbl getifnotexists
work getoutputs   add new writeentity tbl
return 0
/**
* create a new table like an existing table.
*
* @param db
*          the database in question.
* @param crttbl
*          this is the table we're creating.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int createtablelike hive db  createtablelikedesc crttbl  throws hiveexception
// get the existing table
table oldtbl   db gettable crttbl getliketablename
table tbl
if  oldtbl gettabletype      tabletype virtual_view
string targettablename   crttbl gettablename
tbl db newtable targettablename
if  crttbl gettblprops      null
tbl getttable   getparameters   putall crttbl gettblprops
tbl settabletype tabletype managed_table
if  crttbl isexternal
tbl setproperty
tbl settabletype tabletype external_table
tbl setfields oldtbl getcols
tbl setpartcols oldtbl getpartcols
if  crttbl getdefaultsername      null
log info     crttbl gettablename
tbl setserializationlib org apache hadoop hive serde2 lazy lazysimpleserde class getname
else
// let's validate that the serde exists
validateserde crttbl getdefaultsername
tbl setserializationlib crttbl getdefaultsername
if  crttbl getdefaultserdeprops      null
iterator<entry<string  string>> iter   crttbl getdefaultserdeprops   entryset
iterator
while  iter hasnext
entry<string  string> m   iter next
tbl setserdeparam m getkey    m getvalue
tbl setinputformatclass crttbl getdefaultinputformat
tbl setoutputformatclass crttbl getdefaultoutputformat
tbl getttable   getsd   setinputformat
tbl getinputformatclass   getname
tbl getttable   getsd   setoutputformat
tbl getoutputformatclass   getname
else
tbl oldtbl
// find out database name and table name of target table
string targettablename   crttbl gettablename
table newtable   db newtable targettablename
tbl setdbname newtable getdbname
tbl settablename newtable gettablename
if  crttbl getlocation      null
tbl setdatalocation new path crttbl getlocation    touri
else
tbl unsetdatalocation
map<string  string> params   tbl getparameters
// we should copy only those table parameters that are specified in the config.
string paramsstr   hiveconf getvar conf  hiveconf confvars ddl_ctl_parameters_whitelist
if  paramsstr    null
list<string> paramslist   arrays aslist paramsstr split
params keyset   retainall paramslist
else
params clear
if  crttbl gettblprops      null
params putall crttbl gettblprops
if  crttbl isexternal
tbl setproperty
tbl settabletype tabletype external_table
else
tbl getparameters   remove
// reset owner and creation time
int rc   setgenerictableattributes tbl
if  rc    0
return rc
// create the table
db createtable tbl  crttbl getifnotexists
work getoutputs   add new writeentity tbl
return 0
/**
* create a new view.
*
* @param db
*          the database in question.
* @param crtview
*          this is the view we're creating.
* @return returns 0 when execution succeeds and above 0 if it fails.
* @throws hiveexception
*           throws this exception if an unexpected error occurs.
*/
private int createview hive db  createviewdesc crtview  throws hiveexception
table oldview   db gettable crtview getviewname    false
if  crtview getorreplace      oldview    null
// replace existing view
// remove the existing partition columns from the field schema
oldview setvieworiginaltext crtview getvieworiginaltext
oldview setviewexpandedtext crtview getviewexpandedtext
oldview setfields crtview getschema
if  crtview getcomment      null
oldview setproperty    crtview getcomment
if  crtview gettblprops      null
oldview getttable   getparameters   putall crtview gettblprops
oldview setpartcols crtview getpartcols
oldview checkvalidity
try
db altertable crtview getviewname    oldview
catch  invalidoperationexception e
throw new hiveexception e
work getoutputs   add new writeentity oldview
else
// create new view
table tbl   db newtable crtview getviewname
tbl settabletype tabletype virtual_view
tbl setserializationlib null
tbl clearserdeinfo
tbl setvieworiginaltext crtview getvieworiginaltext
tbl setviewexpandedtext crtview getviewexpandedtext
tbl setfields crtview getschema
if  crtview getcomment      null
tbl setproperty    crtview getcomment
if  crtview gettblprops      null
tbl getttable   getparameters   putall crtview gettblprops
if  crtview getpartcols      null
tbl setpartcols crtview getpartcols
int rc   setgenerictableattributes tbl
if  rc    0
return rc
db createtable tbl  crtview getifnotexists
work getoutputs   add new writeentity tbl
return 0
private int truncatetable hive db  truncatetabledesc truncatetabledesc  throws hiveexception
if  truncatetabledesc getcolumnindexes      null
columntruncatework truncatework   new columntruncatework
truncatetabledesc getcolumnindexes    truncatetabledesc getinputdir
truncatetabledesc getoutputdir
truncatework setlistbucketingctx truncatetabledesc getlbctx
truncatework setmappercannotspanpartns true
drivercontext drivercxt   new drivercontext
columntruncatetask taskexec   new columntruncatetask
taskexec initialize db getconf    null  drivercxt
taskexec setwork truncatework
taskexec setqueryplan this getqueryplan
return taskexec execute drivercxt
string tablename   truncatetabledesc gettablename
map<string  string> partspec   truncatetabledesc getpartspec
table table   db gettable tablename  true
try
// this is not transactional
for  path location   getlocations db  table  partspec
filesystem fs   location getfilesystem conf
fs delete location  true
fs mkdirs location
catch  exception e
throw new hiveexception e  errormsg generic_error
return 0
private int exchangetablepartition hive db
altertableexchangepartition exchangepartition  throws hiveexception
map<string  string> partitionspecs   exchangepartition getpartitionspecs
table desttable   exchangepartition getdestinationtable
table sourcetable   exchangepartition getsourcetable
db exchangetablepartitions partitionspecs  sourcetable getdbname
sourcetable gettablename   desttable getdbname
desttable gettablename
return 0
private list<path> getlocations hive db  table table  map<string  string> partspec
throws hiveexception
list<path> locations   new arraylist<path>
if  partspec    null
if  table ispartitioned
for  partition partition   db getpartitions table
locations add partition getpartitionpath
else
locations add table getpath
else
for  partition partition   db getpartitionsbynames table  partspec
locations add partition getpartitionpath
return locations
private int setgenerictableattributes table tbl  throws hiveexception
try
tbl setowner conf getuser
catch  ioexception e
throw new hiveexception e  errormsg generic_error
// set create time
tbl setcreatetime  int   system currenttimemillis     1000
return 0
private string escapehivecommand string str
stringbuilder sb   new stringbuilder
for  int i   0  i < str length    i
char c   str charat i
if  c         c
sb append
sb append c
return sb tostring
@override
public stagetype gettype
return stagetype ddl
@override
public string getname
return