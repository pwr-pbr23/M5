/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive conf
import java io bytearrayoutputstream
import java io file
import java io ioexception
import java io inputstream
import java io printstream
import java net url
import java util arraylist
import java util hashmap
import java util iterator
import java util list
import java util map
import java util map entry
import java util properties
import java util regex matcher
import java util regex pattern
import javax security auth login loginexception
import org apache commons lang stringutils
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop hive shims shimloader
import org apache hadoop mapred jobconf
import org apache hadoop security usergroupinformation
import org apache hadoop util shell
/**
* hive configuration.
*/
public class hiveconf extends configuration
protected string hivejar
protected properties origprop
protected string auxjars
private static final log l4j   logfactory getlog hiveconf class
private static url hivedefaulturl   null
private static url hivesiteurl   null
private static byte confvarbytearray   null
private static final map<string  confvars> vars   new hashmap<string  confvars>
private final list<string> restrictlist   new arraylist<string>
static
classloader classloader   thread currentthread   getcontextclassloader
if  classloader    null
classloader   hiveconf class getclassloader
hivedefaulturl   classloader getresource
// look for hive-site.xml on the classpath and log its location if found.
hivesiteurl   classloader getresource
for  confvars confvar   confvars values
vars put confvar varname  confvar
/**
* metastore related options that the db is initialized against. when a conf
* var in this is list is changed, the metastore instance for the cli will
* be recreated so that the change will take effect.
*/
public static final hiveconf confvars metavars
hiveconf confvars metastoredirectory
hiveconf confvars metastorewarehouse
hiveconf confvars metastoreuris
hiveconf confvars metastorethriftconnectionretries
hiveconf confvars metastorethriftfailureretries
hiveconf confvars metastore_client_connect_retry_delay
hiveconf confvars metastore_client_socket_timeout
hiveconf confvars metastorepwd
hiveconf confvars metastoreconnecturlhook
hiveconf confvars metastoreconnecturlkey
hiveconf confvars metastoreattempts
hiveconf confvars metastoreinterval
hiveconf confvars metastoreforcereloadconf
hiveconf confvars metastoreserverminthreads
hiveconf confvars metastoreservermaxthreads
hiveconf confvars metastore_tcp_keep_alive
hiveconf confvars metastore_int_original
hiveconf confvars metastore_int_archived
hiveconf confvars metastore_int_extracted
hiveconf confvars metastore_kerberos_keytab_file
hiveconf confvars metastore_kerberos_principal
hiveconf confvars metastore_use_thrift_sasl
hiveconf confvars metastore_cache_pinobjtypes
hiveconf confvars metastore_connection_pooling_type
hiveconf confvars metastore_validate_tables
hiveconf confvars metastore_validate_columns
hiveconf confvars metastore_validate_constraints
hiveconf confvars metastore_store_manager_type
hiveconf confvars metastore_auto_create_schema
hiveconf confvars metastore_auto_start_mechanism_mode
hiveconf confvars metastore_transaction_isolation
hiveconf confvars metastore_cache_level2
hiveconf confvars metastore_cache_level2_type
hiveconf confvars metastore_identifier_factory
hiveconf confvars metastore_plugin_registry_bundle_check
hiveconf confvars metastore_authorization_storage_auth_checks
hiveconf confvars metastore_batch_retrieve_max
hiveconf confvars metastore_event_listeners
hiveconf confvars metastore_event_clean_freq
hiveconf confvars metastore_event_expiry_duration
hiveconf confvars metastore_raw_store_impl
hiveconf confvars metastore_end_function_listeners
hiveconf confvars metastore_part_inherit_tbl_props
hiveconf confvars metastore_batch_retrieve_table_partition_max
hiveconf confvars metastore_init_hooks
hiveconf confvars metastore_pre_event_listeners
hiveconf confvars hmshandlerattempts
hiveconf confvars hmshandlerinterval
hiveconf confvars hmshandlerforcereloadconf
hiveconf confvars metastore_partition_name_whitelist_pattern
hiveconf confvars metastore_disallow_incompatible_col_type_changes
/**
* dbvars are the parameters can be set per database. if these
* parameters are set as a database property, when switching to that
* database, the hiveconf variable will be changed. the change of these
* parameters will effectively change the dfs and mapreduce clusters
* for different databases.
*/
public static final hiveconf confvars dbvars
hiveconf confvars hadoopbin
hiveconf confvars hadoopjt
hiveconf confvars metastorewarehouse
hiveconf confvars scratchdir
/**
* the conf variables that depends on current user
*/
public static final hiveconf confvars uservars
hiveconf confvars scratchdir
hiveconf confvars localscratchdir
hiveconf confvars downloaded_resources_dir
hiveconf confvars hivehistoryfileloc
/**
* confvars.
*
* these are the default configuration properties for hive. each hiveconf
* object is initialized as follows:
*
* 1) hadoop configuration properties are applied.
* 2) confvar properties with non-null values are overlayed.
* 3) hive-site.xml properties are overlayed.
*
* warning: think twice before adding any hadoop configuration properties
* with non-null values to this list as they will override any values defined
* in the underlying hadoop configuration.
*/
public static enum confvars
// ql execution stuff
scriptwrapper    null
plan
scratchdir        system getproperty
localscratchdir    system getproperty      file separator   system getproperty
scratchdirpermission
submitviachild    false
scripterrorlimit    100000
allowpartialconsump    false
streamreporterperfix
streamreporterenabled    true
compressresult    false
compressintermediate    false
compressintermediatecodec
compressintermediatetype
bytesperreducer     long   1000   1000   1000
maxreducers    999
preexechooks
postexechooks
onfailurehooks
clientstatspublishers
execparallel    false      parallel query launching
execparallethreadnumber    8
hivespeculativeexecreducers    true
hivecounterspullinterval    1000l
dynamicpartitioning    true
dynamicpartitioningmode
dynamicpartitionmaxparts    1000
dynamicpartitionmaxpartspernode    100
maxcreatedfiles    100000l
downloaded_resources_dir
system getproperty      file separator
defaultpartitionname
default_zookeeper_partition_name
// whether to show a link to the most failed task + debugging tips
show_job_fail_debug_info    true
job_debug_capture_stacktraces    true
job_debug_timeout    30000
tasklog_debug_timeout    20000
output_file_extension    null
// should hive determine whether to run in local mode automatically ?
localmodeauto    false
// if yes:
// run in local mode only if input bytes is less than this. 128mb by default
localmodemaxbytes    134217728l
// run in local mode only if number of tasks (for map and reduce each) is
// less than this
localmodemaxinputfiles    4
// if true, drop table/view does not fail if table/view doesn't exist and if exists is
// not specified
dropignoresnonexistent    true
// ignore the mapjoin hint
hiveignoremapjoinhint    true
// hadoop configuration properties
// properties with null values are ignored and exist only for the purpose of giving us
// a symbolic name to reference in the hive source code. properties with non-null
// values will override any values set in the underlying hadoop configuration.
hadoopbin    findhadoopbinary
hadoopfs    null
hive_fs_har_impl
hadoopmapfilename    null
hadoopmapredinputdir    null
hadoopmapredinputdirrecursive    false
hadoopjt    null
mapredmaxsplitsize    256000000l
mapredminsplitsize    1l
mapredminsplitsizepernode    1l
mapredminsplitsizeperrack    1l
// the number of reduce tasks per job. hadoop sets this value to 1 by default
// by setting this property to -1, hive will automatically determine the correct
// number of reducers.
hadoopnumreducers     1
hadoopjobname    null
hadoopspeculativeexecreducers    true
// metastore stuff. be sure to update hiveconf.metavars when you add
// something here!
metastoredirectory
metastorewarehouse
metastoreuris
// number of times to retry a connection to a thrift metastore server
metastorethriftconnectionretries    3
// number of times to retry a thrift metastore call upon failure
metastorethriftfailureretries    1
// number of seconds the client should wait between connection attempts
metastore_client_connect_retry_delay    1
// socket timeout for the client connection (in seconds)
metastore_client_socket_timeout    20
metastorepwd
// class name of jdo connection url hook
metastoreconnecturlhook
metastoremultithreaded    true
// name of the connection url in the configuration
metastoreconnecturlkey
// number of attempts to retry connecting after there is a jdo datastore err
metastoreattempts    1
// number of miliseconds to wait between attepting
metastoreinterval    1000
// whether to force reloading of the metastore configuration (including
// the connection url, before the next metastore query that accesses the
// datastore. once reloaded, this value is reset to false. used for
// testing only.
metastoreforcereloadconf    false
// number of attempts to retry connecting after there is a jdo datastore err
hmshandlerattempts    1
// number of miliseconds to wait between attepting
hmshandlerinterval    1000
// whether to force reloading of the hmshandler configuration (including
// the connection url, before the next metastore query that accesses the
// datastore. once reloaded, this value is reset to false. used for
// testing only.
hmshandlerforcereloadconf    false
metastoreserverminthreads    200
metastoreservermaxthreads    100000
metastore_tcp_keep_alive    true
// intermediate dir suffixes used for archiving. not important what they
// are, as long as collisions are avoided
metastore_int_original
metastore_int_archived
metastore_int_extracted
metastore_kerberos_keytab_file
metastore_kerberos_principal
metastore_use_thrift_sasl    false
metastore_use_thrift_framed_transport    false
metastore_cluster_delegation_token_store_cls
metastore_cluster_delegation_token_store_zk_connectstr
metastore_cluster_delegation_token_store_zk_znode
metastore_cluster_delegation_token_store_zk_acl
metastore_cache_pinobjtypes
metastore_connection_pooling_type
metastore_validate_tables    false
metastore_validate_columns    false
metastore_validate_constraints    false
metastore_store_manager_type
metastore_auto_create_schema    true
metastore_fixed_datastore    false
metastore_schema_verification    false
metastore_auto_start_mechanism_mode
metastore_transaction_isolation
metastore_cache_level2    false
metastore_cache_level2_type
metastore_identifier_factory
metastore_use_legacy_value_strategy    true
metastore_plugin_registry_bundle_check
metastore_batch_retrieve_max    300
metastore_batch_retrieve_table_partition_max
1000
// a comma separated list of hooks which implement metastoreinitlistener and will be run at
// the beginning of hmshandler initialization
metastore_init_hooks
metastore_pre_event_listeners
metastore_event_listeners
// should we do checks against the storage (usually hdfs) for operations like drop_partition
metastore_authorization_storage_auth_checks    false
metastore_event_clean_freq   0l
metastore_event_expiry_duration   0l
metastore_execute_set_ugi    false
metastore_partition_name_whitelist_pattern
metastore_try_direct_sql    true
metastore_disallow_incompatible_col_type_changes
false
// default parameters for creating tables
newtabledefaultpara
// parameters to copy over when creating a table with create table like.
ddl_ctl_parameters_whitelist
metastore_raw_store_impl
metastore_connection_driver
metastore_manager_factory_class
metastore_detach_all_on_commit    true
metastore_non_transactional_read    true
metastore_connection_user_name
metastore_end_function_listeners
metastore_part_inherit_tbl_props
// parameters for exporting metadata on table drop (requires the use of the)
// org.apache.hadoop.hive.ql.parse.metadataexportlistener preevent listener
metadata_export_location
move_exported_metadata_to_trash    true
// cli
cliignoreerrors    false
cliprintcurrentdb    false
cliprompt
cliprettyoutputnumcols     1
hive_metastore_fs_handler_cls
// things we log in the jobconf
// session identifier
hivesessionid
// whether session is running in silent mode or not
hivesessionsilent    false
// whether to enable history for this session
hive_session_history_enabled    false
// query being executed (multiple per session)
hivequerystring
// id of query being executed (multiple per session)
hivequeryid
// id of the mapred plan being executed (multiple per query)
hiveplanid
// max jobname length
hivejobnamelength    50
// hive jar
hivejar
hiveauxjars
// hive added files and jars
hiveaddedfiles
hiveaddedjars
hiveaddedarchives
// for hive script operator
hives_auto_progress_timeout    0
hivetablename
hivepartitionname
hivescriptautoprogress    false
hivescriptidenvvar
hivescripttruncateenv    false
hivemapredmode
hivealias
hivemapsideaggregate    true
hivegroupbyskew    false
hive_optimize_multi_groupby_common_distincts
true
hivejoinemitinterval    1000
hivejoincachesize    25000
// hive.mapjoin.bucket.cache.size has been replaced by hive.smbjoin.cache.row,
// need to remove by hive .13. also, do not change default (see smb operator)
hivemapjoinbucketcachesize    100
hivesmbjoincacherows    10000
hivegroupbymapinterval    100000
hivemapaggrhashmemory     float  0 5
hivemapjoinfollowedbymapaggrhashmemory     float  0 3
hivemapaggrmemorythreshold     float  0 9
hivemapaggrhashminreduction     float  0 5
hivemultigroupbysinglereducer    true
hive_map_groupby_sort    false
hive_map_groupby_sort_testmode    false
hive_groupby_orderby_position_alias    false
hive_new_job_grouping_set_cardinality    30
// for hive udtf operator
hiveudtfautoprogress    false
// default file format for create table statement
// options: textfile, sequencefile
hivedefaultfileformat
hivequeryresultfileformat
hivecheckfileformat    true
// default serde for rcfile
hivedefaultrcfileserde
//location of hive run time structured log file
hivehistoryfileloc    system getproperty      file separator   system getproperty
// whether to log the plan's progress every time a job's progress is checked
hive_log_incremental_plan_progress    true
// the interval between logging the plan's progress in milliseconds
hive_log_incremental_plan_progress_interval    60000l
// default serde and record reader for user scripts
hivescriptserde
hivescriptrecordreader
hivescriptrecordwriter
hivescriptescape    false
hivebinaryrecordmax    1000
// hwi
hivehwilistenhost
hivehwilistenport
hivehwiwarfile    system getenv
// mapper/reducer memory in local mode
hivehadoopmaxmem    0
//small table file size
hivesmalltablesfilesize   25000000l     25m
// random number for split sampling
hivesamplerandomnum    0
// test mode in hive mode
hivetestmode    false
hivetestmodeprefix
hivetestmodesamplefreq    32
hivetestmodenosample
hivemergemapfiles    true
hivemergemapredfiles    false
hivemergemapfilessize     long   256   1000   1000
hivemergemapfilesavgsize     long   16   1000   1000
hivemergercfileblocklevel    true
hivemergeinputformatblocklevel
hivemergecurrentjobhasdynamicpartitions
false
hiveuseexplicitrcfileheader    true
hiveusercfilesynccache    true
// maximum fraction of heap that can be used by orc file writers
hive_orc_file_memory_pool    0 5f      50%
// define the version of the file to write
hive_orc_write_format    null
hive_orc_dictionary_key_size_threshold    0 8f
hiveskewjoin    false
hiveconvertjoin    true
hiveconvertjoinnoconditionaltask    true
hiveconvertjoinnoconditionaltaskthreshold
10000000l
hiveskewjoinkey    100000
hiveskewjoinmapjoinnummaptask    10000
hiveskewjoinmapjoinminsplit    33554432l     32m
hivesendheartbeat    1000
hivelimitmaxrowsize    100000l
hivelimitoptlimitfile    10
hivelimitoptenable    false
hivelimitoptmaxfetch    50000
hivelimitpushdownmemoryusage     1f
hivehashtablethreshold    100000
hivehashtableloadfactor     float  0 75
hivehashtablefollowbygbymaxmemoryusage     float  0 55
hivehashtablemaxmemoryusage     float  0 90
hivehashtablescale     long 100000
hivedebuglocaltask   false
hivejobprogress    false
hiveinputformat
hiveenforcebucketing    false
hiveenforcesorting    false
hiveoptimizebucketingsorting    true
hivepartitioner
hiveenforcesortmergebucketmapjoin    false
hiveenforcebucketmapjoin    false
hive_auto_sortmerge_join    false
hive_auto_sortmerge_join_bigtable_selector
hive_auto_sortmerge_join_tomapjoin
false
hivescriptoperatortrust    false
hiverowoffset    false
hive_combine_input_format_supports_splittable    false
// optimizer
hiveoptcp    true      column pruner
hiveoptindexfilter    false      automatically use indexes
hiveindexautoupdate    false     automatically update stale indexes
hiveoptppd    true      predicate pushdown
hiveppdrecognizetransitivity    true      predicate pushdown
hiveppdremoveduplicatefilters    true
hivemetadataonlyqueries    true
// push predicates down to storage handlers
hiveoptppd_storage    true
hiveoptgroupby    true      optimize group by
hiveoptbucketmapjoin    false      optimize bucket map join
hiveoptsortmergebucketmapjoin    false      try to use sorted merge bucket map join
hiveoptreducededuplication    true
hiveoptreducededuplicationminreducer    4
hivesamplingfororderby    false
hivesamplingnumberfororderby    1000
hivesamplingpercentfororderby    0 1f
// whether to optimize union followed by select followed by filesink
// it creates sub-directories in the final output, so should not be turned on in systems
// where mapreduce-1501 is not present
hive_optimize_union_remove    false
hiveoptcorrelation    false      exploit intra query correlations
// whether hadoop map-reduce supports sub-directories. it was added by mapreduce-1501.
// some optimizations can only be performed if the version of hadoop being used supports
// sub-directories
hive_hadoop_supports_subdirectories    false
// optimize skewed join by changing the query plan at compile time
hive_optimize_skewjoin_compiletime    false
// indexes
hiveoptindexfilter_compact_minsize     long  5   1024   1024   1024      5g
hiveoptindexfilter_compact_maxsize     long   1      infinity
hive_index_compact_query_max_entries     long  10000000      10m
hive_index_compact_query_max_size     long  10   1024   1024   1024      10g
hive_index_compact_binary_search    true
// statistics
hivestatsautogather    true
hivestatsdbclass
other options are jdbc mysql and hbase as defined in statssetupconst java
hivestatsjdbcdriver
jdbc driver specific to the dbclass
hivestatsdbconnectionstring
automatically create database
hive_stats_default_publisher
default stats publisher if none of jdbc hbase is specified
hive_stats_default_aggregator
default stats aggregator if none of jdbc hbase is specified
hive_stats_jdbc_timeout
30      default timeout in sec for jdbc connection   sql statements
hive_stats_atomic
false      whether to update metastore stats only if all stats are available
hive_stats_retries_max
0          maximum # of retries to insert select delete the stats db
hive_stats_retries_wait
3000       # milliseconds to wait before the next retry
hive_stats_collect_rawdatasize    true
// should the raw data size be collected when analyzing tables
client_stats_counters
//subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). non-display names should be used".
hive_stats_reliable    false
// collect table access keys information for operators that can benefit from bucketing
hive_stats_collect_tablekeys    false
// collect column access information
hive_stats_collect_scancols    false
// standard error allowed for ndv estimates. a lower value indicates higher accuracy and a
// higher compute cost.
hive_stats_ndv_error     float 20 0
hive_stats_key_prefix_max_length    200
hive_stats_key_prefix           internal usage only
// concurrency
hive_support_concurrency    false
hive_lock_manager
hive_lock_numretries    100
hive_unlock_numretries    10
hive_lock_sleep_between_retries    60
hive_lock_mapred_only    false
hive_zookeeper_quorum
hive_zookeeper_client_port
hive_zookeeper_session_timeout    600 1000
hive_zookeeper_namespace
hive_zookeeper_clean_extra_nodes    false
// for hbase storage handler
hive_hbase_wal_enabled    true
// for har files
hivearchiveenabled    false
//enable/disable gbtoidx rewrite rule
hiveoptgbyusingindex    false
hiveouterjoinsupportsfilters    true
// 'minimal', 'more' (and 'all' later)
hivefetchtaskconversion
hivefetchtaskaggr    false
// serde for fetchtask
hivefetchoutputserde
hiveexprevaluationcache    true
// hive variables
hivevariablesubstitute    true
hivevariablesubstitutedepth    40
hiveconfvalidation    true
semantic_analyzer_hook
hive_authorization_enabled    false
hive_authorization_manager
hive_authenticator_manager
hive_metastore_authorization_manager
hive_metastore_authenticator_manager
hive_authorization_table_user_grants
hive_authorization_table_group_grants
hive_authorization_table_role_grants
hive_authorization_table_owner_grants
// print column names in output
hive_cli_print_header    false
hive_error_on_empty_partition    false
hive_index_ignore_hdfs_loc    false
hive_exim_uri_scheme_wl
// temporary variable for testing. this is added just to turn off this feature in case of a bug in
// deployment. it has not been documented in hive-default.xml intentionally, this should be removed
// once the feature is stable
hive_mapper_cannot_span_multiple_partitions    false
hive_rework_mapredwork    false
hive_concatenate_check_index     true
hive_io_exception_handlers
// logging configuration
hive_log4j_file
hive_exec_log4j_file
// prefix used to auto generated column aliases (this should be started with '_')
hive_autogen_columnalias_prefix_label
hive_autogen_columnalias_prefix_includefuncname
false
// the class responsible for logging client side performance metrics
// must be a subclass of org.apache.hadoop.hive.ql.log.perflogger
hive_perf_logger
// whether to delete the scratchdir while startup
hive_start_cleanup_scratchdir    false
hive_insert_into_multilevel_dirs    false
hive_warehouse_subdir_inherit_perms    false
// whether insert into external tables is allowed
hive_insert_into_external_tables    true
// a comma separated list of hooks which implement hivedriverrunhook and will be run at the
// beginning and end of driver.run, these will be run in the order specified
hive_driver_run_hooks
hive_ddl_output_format    null
hive_entity_separator
// binary or http
hive_server2_transport_mode
// http (over thrift) transport settings
hive_server2_thrift_http_port    10001
hive_server2_thrift_http_path
hive_server2_thrift_http_min_worker_threads    5
hive_server2_thrift_http_max_worker_threads    500
// binary transport settings
hive_server2_thrift_port    10000
hive_server2_thrift_bind_host
hive_server2_thrift_sasl_qop
hive_server2_thrift_min_worker_threads    5
hive_server2_thrift_max_worker_threads    500
// configuration for async thread pool in sessionmanager
// number of async threads
hive_server2_async_exec_threads    50
// number of seconds hiveserver2 shutdown will wait for async threads to terminate
hive_server2_async_exec_shutdown_timeout    10
// hiveserver2 auth configuration
hive_server2_authentication
hive_server2_kerberos_keytab
hive_server2_kerberos_principal
hive_server2_plain_ldap_url    null
hive_server2_plain_ldap_basedn    null
hive_server2_plain_ldap_domain    null
hive_server2_custom_authentication_class    null
hive_server2_enable_doas    true
hive_server2_table_type_mapping
hive_server2_session_hook
hive_conf_restricted_list    null
// if this is set all move tasks at the end of a multi-insert query will only begin once all
// outputs are ready
hive_multi_insert_move_tasks_share_dependencies
false
// if this is set, when writing partitions, the metadata will include the bucketing/sorting
// properties with which the data was written if any (this will not overwrite the metadata
// inherited from the table if the table is bucketed/sorted)
hive_infer_bucket_sort    false
// if this is set, when setting the number of reducers for the map reduce task which writes the
// final output files, it will choose a number which is a power of two.  the number of reducers
// may be set to a power of two, only to be followed by a merge task meaning preventing
// anything from being inferred.
hive_infer_bucket_sort_num_buckets_power_two
false
/* the following section contains all configurations used for list bucketing feature.*/
/* this is not for clients. but only for block merge task. */
/* this is used by blockmergetask to send out flag to rcfilemergemapper */
/* about alter table...concatenate and list bucketing case. */
hivemergecurrentjobconcatenatelistbucketing
true
/* this is not for clients. but only for block merge task. */
/* this is used by blockmergetask to send out flag to rcfilemergemapper */
/* about depth of list bucketing. */
hivemergecurrentjobconcatenatelistbucketingdepth
0
// enable list bucketing optimizer. default value is false so that we disable it by default.
hiveoptlistbucketing    false
// allow tcp keep alive socket option for for hiveserver or a maximum timeout for the socket.
server_read_socket_timeout    10
server_tcp_keep_alive    true
// whether to show the unquoted partition names in query results.
hive_decode_partition_name    false
hive_type_check_on_insert    true
public final string varname
public final string defaultval
public final int defaultintval
public final long defaultlongval
public final float defaultfloatval
public final class<?> valclass
public final boolean defaultboolval
private final vartype type
confvars string varname  string defaultval
this varname   varname
this valclass   string class
this defaultval   defaultval
this defaultintval    1
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   false
this type   vartype string
confvars string varname  int defaultintval
this varname   varname
this valclass   integer class
this defaultval   integer tostring defaultintval
this defaultintval   defaultintval
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   false
this type   vartype int
confvars string varname  long defaultlongval
this varname   varname
this valclass   long class
this defaultval   long tostring defaultlongval
this defaultintval    1
this defaultlongval   defaultlongval
this defaultfloatval    1
this defaultboolval   false
this type   vartype long
confvars string varname  float defaultfloatval
this varname   varname
this valclass   float class
this defaultval   float tostring defaultfloatval
this defaultintval    1
this defaultlongval    1
this defaultfloatval   defaultfloatval
this defaultboolval   false
this type   vartype float
confvars string varname  boolean defaultboolval
this varname   varname
this valclass   boolean class
this defaultval   boolean tostring defaultboolval
this defaultintval    1
this defaultlongval    1
this defaultfloatval    1
this defaultboolval   defaultboolval
this type   vartype boolean
public boolean istype string value
return type istype value
public string typestring
return type typestring
@override
public string tostring
return varname
private static string findhadoopbinary
string val   system getenv
// in hadoop 1.x and hadoop 2.x hadoop_home is gone and replaced with hadoop_prefix
if  val    null
val   system getenv
// and if all else fails we can at least try /usr/bin/hadoop
val    val    null ? file separator       val
file separator       file separator
// launch hadoop command file on windows.
return val    shell windows ?
enum vartype
string   @override
void checktype string value  throws exception
int   @override
void checktype string value  throws exception   integer valueof value
long   @override
void checktype string value  throws exception   long valueof value
float   @override
void checktype string value  throws exception   float valueof value
boolean   @override
void checktype string value  throws exception   boolean valueof value
boolean istype string value
try   checktype value     catch  exception e    return false
return true
string typestring     return name   touppercase
abstract void checktype string value  throws exception
/**
* writes the default confvars out to a byte array and returns an input
* stream wrapping that byte array.
*
* we need this in order to initialize the confvar properties
* in the underling configuration object using the addresource(inputstream)
* method.
*
* it is important to use a loopingbytearrayinputstream because it turns out
* addresource(inputstream) is broken since configuration tries to read the
* entire contents of the same inputstream repeatedly without resetting it.
* loopingbytearrayinputstream has special logic to handle this.
*/
private static synchronized inputstream getconfvarinputstream
if  confvarbytearray    null
try
// create a hadoop configuration without inheriting default settings.
configuration conf   new configuration false
applydefaultnonnullconfvars conf
bytearrayoutputstream confvarbaos   new bytearrayoutputstream
conf writexml confvarbaos
confvarbytearray   confvarbaos tobytearray
catch  exception e
// we're pretty screwed if we can't load the default conf vars
throw new runtimeexception    e
return new loopingbytearrayinputstream confvarbytearray
public void verifyandset string name  string value  throws illegalargumentexception
if  restrictlist contains name
throw new illegalargumentexception     name
set name  value
public static int getintvar configuration conf  confvars var
assert  var valclass    integer class
return conf getint var varname  var defaultintval
public static void setintvar configuration conf  confvars var  int val
assert  var valclass    integer class
conf setint var varname  val
public int getintvar confvars var
return getintvar this  var
public void setintvar confvars var  int val
setintvar this  var  val
public static long getlongvar configuration conf  confvars var
assert  var valclass    long class
return conf getlong var varname  var defaultlongval
public static long getlongvar configuration conf  confvars var  long defaultval
return conf getlong var varname  defaultval
public static void setlongvar configuration conf  confvars var  long val
assert  var valclass    long class
conf setlong var varname  val
public long getlongvar confvars var
return getlongvar this  var
public void setlongvar confvars var  long val
setlongvar this  var  val
public static float getfloatvar configuration conf  confvars var
assert  var valclass    float class
return conf getfloat var varname  var defaultfloatval
public static float getfloatvar configuration conf  confvars var  float defaultval
return conf getfloat var varname  defaultval
public static void setfloatvar configuration conf  confvars var  float val
assert  var valclass    float class
shimloader gethadoopshims   setfloatconf conf  var varname  val
public float getfloatvar confvars var
return getfloatvar this  var
public void setfloatvar confvars var  float val
setfloatvar this  var  val
public static boolean getboolvar configuration conf  confvars var
assert  var valclass    boolean class
return conf getboolean var varname  var defaultboolval
public static boolean getboolvar configuration conf  confvars var  boolean defaultval
return conf getboolean var varname  defaultval
public static void setboolvar configuration conf  confvars var  boolean val
assert  var valclass    boolean class
conf setboolean var varname  val
public boolean getboolvar confvars var
return getboolvar this  var
public void setboolvar confvars var  boolean val
setboolvar this  var  val
public static string getvar configuration conf  confvars var
assert  var valclass    string class
return conf get var varname  var defaultval
public static string getvar configuration conf  confvars var  string defaultval
return conf get var varname  defaultval
public static void setvar configuration conf  confvars var  string val
assert  var valclass    string class
conf set var varname  val
public static confvars getconfvars string name
return vars get name
public string getvar confvars var
return getvar this  var
public void setvar confvars var  string val
setvar this  var  val
public void logvars printstream ps
for  confvars one   confvars values
ps println one varname         get one varname     null  ? get one varname
public hiveconf
super
initialize this getclass
public hiveconf class<?> cls
super
initialize cls
public hiveconf configuration other  class<?> cls
super other
initialize cls
/**
* copy constructor
*/
public hiveconf hiveconf other
super other
hivejar   other hivejar
auxjars   other auxjars
origprop    properties other origprop clone
public properties getallproperties
return getproperties this
private static properties getproperties configuration conf
iterator<map entry<string  string>> iter   conf iterator
properties p   new properties
while  iter hasnext
map entry<string  string> e   iter next
p setproperty e getkey    e getvalue
return p
private void initialize class<?> cls
hivejar    new jobconf cls   getjar
// preserve the original configuration
origprop   getallproperties
// overlay the confvars. note that this ignores confvars with null values
addresource getconfvarinputstream
// overlay hive-site.xml if it exists
if  hivesiteurl    null
addresource hivesiteurl
// overlay the values of any system properties whose names appear in the list of confvars
applysystemproperties
if this get    null     null
l4j warn
// if the running class was loaded directly (through eclipse) rather than through a
// jar then this would be needed
if  hivejar    null
hivejar   this get confvars hivejar varname
if  auxjars    null
auxjars   this get confvars hiveauxjars varname
if  getboolvar confvars metastore_schema_verification
setboolvar confvars metastore_auto_create_schema  false
setboolvar confvars metastore_fixed_datastore  true
// setup list of conf vars that are not allowed to change runtime
string restrictliststr   this get confvars hive_conf_restricted_list tostring
if  restrictliststr    null
for  string entry   restrictliststr split
restrictlist add entry
restrictlist add confvars hive_conf_restricted_list tostring
/**
* apply system properties to this object if the property name is defined in confvars
* and the value is non-null and not an empty string.
*/
private void applysystemproperties
map<string  string> systemproperties   getconfsystemproperties
for  entry<string  string> systemproperty   systemproperties entryset
this set systemproperty getkey    systemproperty getvalue
/**
* this method returns a mapping from config variable name to its value for all config variables
* which have been set using system properties
*/
public static map<string  string> getconfsystemproperties
map<string  string> systemproperties   new hashmap<string  string>
for  confvars onevar   confvars values
if  system getproperty onevar varname     null
if  system getproperty onevar varname  length   > 0
systemproperties put onevar varname  system getproperty onevar varname
return systemproperties
/**
* overlays confvar properties with non-null values
*/
private static void applydefaultnonnullconfvars configuration conf
for  confvars var   confvars values
if  var defaultval    null
// don't override confvars with null values
continue
conf set var varname  var defaultval
public properties getchangedproperties
properties ret   new properties
properties newprop   getallproperties
for  object one   newprop keyset
string oneprop    string  one
string oldvalue   origprop getproperty oneprop
if   stringutils equals oldvalue  newprop getproperty oneprop
ret setproperty oneprop  newprop getproperty oneprop
return  ret
public string getjar
return hivejar
/**
* @return the auxjars
*/
public string getauxjars
return auxjars
/**
* @param auxjars the auxjars to set
*/
public void setauxjars string auxjars
this auxjars   auxjars
setvar this  confvars hiveauxjars  auxjars
public url gethivedefaultlocation
return hivedefaulturl
public url gethivesitelocation
return hivesiteurl
/**
* @return the user name set in hadoop.job.ugi param or the current user from system
* @throws ioexception
*/
public string getuser   throws ioexception
try
usergroupinformation ugi   shimloader gethadoopshims
getugiforconf this
return ugi getusername
catch  loginexception le
throw new ioexception le
public static string getcolumninternalname int pos
return     pos
public static int getpositionfrominternalname string internalname
pattern internalpattern   pattern compile
matcher m   internalpattern matcher internalname
if   m matches
return  1
else
return integer parseint m group 1