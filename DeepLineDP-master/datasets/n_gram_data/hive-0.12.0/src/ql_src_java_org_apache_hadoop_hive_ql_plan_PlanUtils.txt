/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql plan
import java io serializable
import java util arraylist
import java util collections
import java util comparator
import java util linkedhashmap
import java util list
import java util map
import java util properties
import java util set
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop hive conf hiveconf
import org apache hadoop hive metastore metastoreutils
import org apache hadoop hive metastore api fieldschema
import org apache hadoop hive ql exec columninfo
import org apache hadoop hive ql exec rowschema
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql hooks readentity
import org apache hadoop hive ql io hivefileformatutils
import org apache hadoop hive ql io hiveoutputformat
import org apache hadoop hive ql io hivepassthroughoutputformat
import org apache hadoop hive ql io ignorekeytextoutputformat
import org apache hadoop hive ql io rcfileinputformat
import org apache hadoop hive ql io rcfileoutputformat
import org apache hadoop hive ql metadata hive
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql metadata hivestoragehandler
import org apache hadoop hive ql metadata hiveutils
import org apache hadoop hive ql parse semanticexception
import org apache hadoop hive ql parse typecheckprocfactory
import org apache hadoop hive ql session sessionstate
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 delimitedjsonserde
import org apache hadoop hive serde2 deserializer
import org apache hadoop hive serde2 metadatatypedcolumnsetserde
import org apache hadoop hive serde2 binarysortable binarysortableserde
import org apache hadoop hive serde2 columnar columnarserde
import org apache hadoop hive serde2 lazy lazysimpleserde
import org apache hadoop hive serde2 lazybinary lazybinaryserde
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive serde2 typeinfo typeinfofactory
import org apache hadoop mapred inputformat
import org apache hadoop mapred jobconf
import org apache hadoop mapred sequencefileinputformat
import org apache hadoop mapred sequencefileoutputformat
import org apache hadoop mapred textinputformat
/**
* planutils.
*
*/
public final class planutils
protected static final log log   logfactory getlog
private static long countformapjoindumpfileprefix   0
/**
* expressiontypes.
*
*/
public static enum expressiontypes
field  jexl
public static long getcountformapjoindumpfileprefix
return countformapjoindumpfileprefix
@suppresswarnings
public static mapredwork getmapredwork
try
mapredwork work   new mapredwork
work getmapwork   sethadoopsupportssplittable hive get   getconf   getboolvar
hiveconf confvars hive_combine_input_format_supports_splittable
return work
catch  hiveexception ex
throw new runtimeexception ex
public static tabledesc getdefaulttabledesc createtabledesc localdirectorydesc
string cols  string coltypes
tabledesc tabledesc   getdefaulttabledesc integer tostring utilities ctrlacode   cols
coltypes  false
if  localdirectorydesc    null
return tabledesc
try
if  localdirectorydesc getfielddelim      null
tabledesc getproperties   setproperty
serdeconstants field_delim  localdirectorydesc getfielddelim
tabledesc getproperties   setproperty
serdeconstants serialization_format  localdirectorydesc getfielddelim
if  localdirectorydesc getlinedelim      null
tabledesc getproperties   setproperty
serdeconstants line_delim  localdirectorydesc getlinedelim
if  localdirectorydesc getcollitemdelim      null
tabledesc getproperties   setproperty
serdeconstants collection_delim  localdirectorydesc getcollitemdelim
if  localdirectorydesc getmapkeydelim      null
tabledesc getproperties   setproperty
serdeconstants mapkey_delim  localdirectorydesc getmapkeydelim
if  localdirectorydesc getfieldescape     null
tabledesc getproperties   setproperty
serdeconstants escape_char  localdirectorydesc getfieldescape
if  localdirectorydesc getsername      null
tabledesc setserdeclassname localdirectorydesc getsername
tabledesc getproperties   setproperty
serdeconstants serialization_lib  localdirectorydesc getsername
tabledesc setdeserializerclass
class<? extends deserializer>  class forname localdirectorydesc getsername
if  localdirectorydesc getoutputformat      null
tabledesc setoutputfileformatclass class forname localdirectorydesc getoutputformat
catch  classnotfoundexception e
// mimicking behaviour in createtabledesc tabledesc creation
// returning null table description for output.
e printstacktrace
return null
return tabledesc
/**
* generate the table descriptor of metadatatypedcolumnsetserde with the
* separatorcode and column names (comma separated string).
*/
public static tabledesc getdefaulttabledesc string separatorcode
string columns
return getdefaulttabledesc separatorcode  columns  false
/**
* generate the table descriptor of given serde with the separatorcode and
* column names (comma separated string).
*/
public static tabledesc gettabledesc
class<? extends deserializer> serdeclass  string separatorcode
string columns
return gettabledesc serdeclass  separatorcode  columns  false
/**
* generate the table descriptor of metadatatypedcolumnsetserde with the
* separatorcode and column names (comma separated string), and whether the
* last column should take the rest of the line.
*/
public static tabledesc getdefaulttabledesc string separatorcode
string columns  boolean lastcolumntakesrestoftheline
return getdefaulttabledesc separatorcode  columns  null
lastcolumntakesrestoftheline
/**
* generate the table descriptor of the serde specified with the separatorcode
* and column names (comma separated string), and whether the last column
* should take the rest of the line.
*/
public static tabledesc gettabledesc
class<? extends deserializer> serdeclass  string separatorcode
string columns  boolean lastcolumntakesrestoftheline
return gettabledesc serdeclass  separatorcode  columns  null
lastcolumntakesrestoftheline
/**
* generate the table descriptor of metadatatypedcolumnsetserde with the
* separatorcode and column names (comma separated string), and whether the
* last column should take the rest of the line.
*/
public static tabledesc getdefaulttabledesc string separatorcode
string columns  string columntypes  boolean lastcolumntakesrestoftheline
return gettabledesc lazysimpleserde class  separatorcode  columns
columntypes  lastcolumntakesrestoftheline
public static tabledesc gettabledesc
class<? extends deserializer> serdeclass  string separatorcode
string columns  string columntypes  boolean lastcolumntakesrestoftheline
return gettabledesc serdeclass  separatorcode  columns  columntypes
lastcolumntakesrestoftheline  false
public static tabledesc gettabledesc
class<? extends deserializer> serdeclass  string separatorcode
string columns  string columntypes  boolean lastcolumntakesrestoftheline
boolean usedelimitedjson
return gettabledesc serdeclass  separatorcode  columns  columntypes
lastcolumntakesrestoftheline  usedelimitedjson
public static tabledesc gettabledesc
class<? extends deserializer> serdeclass  string separatorcode
string columns  string columntypes  boolean lastcolumntakesrestoftheline
boolean usedelimitedjson  string fileformat
properties properties   utilities makeproperties
serdeconstants serialization_format  separatorcode  serdeconstants list_columns
columns
if   separatorcode equals integer tostring utilities ctrlacode
properties setproperty serdeconstants field_delim  separatorcode
if  columntypes    null
properties setproperty serdeconstants list_column_types  columntypes
if  lastcolumntakesrestoftheline
properties setproperty serdeconstants serialization_last_column_takes_rest
// it is not a very clean way, and should be modified later - due to
// compatiblity reasons,
// user sees the results as json for custom scripts and has no way for
// specifying that.
// right now, it is hard-coded in the code
if  usedelimitedjson
serdeclass   delimitedjsonserde class
class inputformat  outputformat
// get the input & output file formats
if    equalsignorecase fileformat
inputformat   sequencefileinputformat class
outputformat   sequencefileoutputformat class
else if    equalsignorecase fileformat
inputformat   rcfileinputformat class
outputformat   rcfileoutputformat class
assert serdeclass    columnarserde class
else      use textfile by default
inputformat   textinputformat class
outputformat   ignorekeytextoutputformat class
return new tabledesc serdeclass  inputformat  outputformat  properties
public static tabledesc getdefaultqueryoutputtabledesc string cols  string coltypes
string fileformat
tabledesc tbldesc   gettabledesc lazysimpleserde class      utilities ctrlacode  cols  coltypes
false  false  fileformat
//enable escaping
tbldesc getproperties   setproperty serdeconstants escape_char
//enable extended nesting levels
tbldesc getproperties   setproperty
lazysimpleserde serialization_extend_nesting_levels
return tbldesc
/**
* generate a table descriptor from a createtabledesc.
*/
public static tabledesc gettabledesc createtabledesc crttbldesc  string cols
string coltypes
class<? extends deserializer> serdeclass   lazysimpleserde class
string separatorcode   integer tostring utilities ctrlacode
string columns   cols
string columntypes   coltypes
boolean lastcolumntakesrestoftheline   false
tabledesc ret
try
if  crttbldesc getsername      null
class c   class forname crttbldesc getsername
serdeclass   c
if  crttbldesc getfielddelim      null
separatorcode   crttbldesc getfielddelim
ret   gettabledesc serdeclass  separatorcode  columns  columntypes
lastcolumntakesrestoftheline  false
// set other table properties
properties properties   ret getproperties
if  crttbldesc getcollitemdelim      null
properties setproperty serdeconstants collection_delim  crttbldesc
getcollitemdelim
if  crttbldesc getmapkeydelim      null
properties setproperty serdeconstants mapkey_delim  crttbldesc
getmapkeydelim
if  crttbldesc getfieldescape      null
properties setproperty serdeconstants escape_char  crttbldesc
getfieldescape
if  crttbldesc getlinedelim      null
properties setproperty serdeconstants line_delim  crttbldesc getlinedelim
if  crttbldesc gettablename      null    crttbldesc getdatabasename      null
properties setproperty org apache hadoop hive metastore api hive_metastoreconstants meta_table_name
crttbldesc getdatabasename         crttbldesc gettablename
// replace the default input & output file format with those found in
// crttbldesc
class c1   class forname crttbldesc getinputformat
class c2   class forname crttbldesc getoutputformat
class<? extends inputformat> in_class   c1
class<? extends hiveoutputformat> out_class   c2
ret setinputfileformatclass in_class
ret setoutputfileformatclass out_class
catch  classnotfoundexception e
e printstacktrace
return null
return ret
/**
* generate the table descriptor of metadatatypedcolumnsetserde with the
* separatorcode. metadatatypedcolumnsetserde is used because lazysimpleserde
* does not support a table with a single column "col" with type
* "array<string>".
*/
public static tabledesc getdefaulttabledesc string separatorcode
return new tabledesc metadatatypedcolumnsetserde class
textinputformat class  ignorekeytextoutputformat class  utilities
makeproperties
org apache hadoop hive serde serdeconstants serialization_format
separatorcode
/**
* generate the table descriptor for reduce key.
*/
public static tabledesc getreducekeytabledesc list<fieldschema> fieldschemas
string order
return new tabledesc binarysortableserde class
sequencefileinputformat class  sequencefileoutputformat class
utilities makeproperties serdeconstants list_columns  metastoreutils
getcolumnnamesfromfieldschema fieldschemas
serdeconstants list_column_types  metastoreutils
getcolumntypesfromfieldschema fieldschemas
serdeconstants serialization_sort_order  order
/**
* generate the table descriptor for map-side join key.
*/
public static tabledesc getmapjoinkeytabledesc list<fieldschema> fieldschemas
return new tabledesc lazybinaryserde class  sequencefileinputformat class
sequencefileoutputformat class  utilities makeproperties
metastoreutils getcolumnnamesfromfieldschema fieldschemas
metastoreutils
getcolumntypesfromfieldschema fieldschemas
serdeconstants escape_char
/**
* generate the table descriptor for map-side join key.
*/
public static tabledesc getmapjoinvaluetabledesc
list<fieldschema> fieldschemas
return new tabledesc lazybinaryserde class  sequencefileinputformat class
sequencefileoutputformat class  utilities makeproperties
metastoreutils getcolumnnamesfromfieldschema fieldschemas
metastoreutils
getcolumntypesfromfieldschema fieldschemas
serdeconstants escape_char
/**
* generate the table descriptor for intermediate files.
*/
public static tabledesc getintermediatefiletabledesc
list<fieldschema> fieldschemas
return new tabledesc lazybinaryserde class  sequencefileinputformat class
sequencefileoutputformat class  utilities makeproperties
serdeconstants list_columns  metastoreutils
getcolumnnamesfromfieldschema fieldschemas
serdeconstants list_column_types  metastoreutils
getcolumntypesfromfieldschema fieldschemas
serdeconstants escape_char
/**
* generate the table descriptor for intermediate files.
*/
public static tabledesc getreducevaluetabledesc list<fieldschema> fieldschemas
return new tabledesc lazybinaryserde class  sequencefileinputformat class
sequencefileoutputformat class  utilities makeproperties
serdeconstants list_columns  metastoreutils
getcolumnnamesfromfieldschema fieldschemas
serdeconstants list_column_types  metastoreutils
getcolumntypesfromfieldschema fieldschemas
serdeconstants escape_char
/**
* convert the columnlist to fieldschema list.
*
* adds uniontype for distinctcolindices.
*/
public static list<fieldschema> getfieldschemasfromcolumnlistwithlength
list<exprnodedesc> cols  list<list<integer>> distinctcolindices
list<string> outputcolumnnames  int length
string fieldprefix
// last one for union column.
list<fieldschema> schemas   new arraylist<fieldschema> length   1
for  int i   0  i < length  i
schemas add metastoreutils getfieldschemafromtypeinfo
fieldprefix   outputcolumnnames get i   cols get i  gettypeinfo
list<typeinfo> uniontypes   new arraylist<typeinfo>
for  list<integer> distinctcols   distinctcolindices
list<string> names   new arraylist<string>
list<typeinfo> types   new arraylist<typeinfo>
int numexprs   0
for  int i   distinctcols
names add hiveconf getcolumninternalname numexprs
types add cols get i  gettypeinfo
numexprs
uniontypes add typeinfofactory getstructtypeinfo names  types
if  outputcolumnnames size     length > 0
schemas add metastoreutils getfieldschemafromtypeinfo
fieldprefix   outputcolumnnames get length
typeinfofactory getuniontypeinfo uniontypes
return schemas
/**
* convert the columnlist to fieldschema list.
*/
public static list<fieldschema> getfieldschemasfromcolumnlist
list<exprnodedesc> cols  list<string> outputcolumnnames  int start
string fieldprefix
list<fieldschema> schemas   new arraylist<fieldschema> cols size
for  int i   0  i < cols size    i
schemas add metastoreutils getfieldschemafromtypeinfo fieldprefix
outputcolumnnames get i   start   cols get i  gettypeinfo
return schemas
/**
* convert the columnlist to fieldschema list.
*/
public static list<fieldschema> getfieldschemasfromcolumnlist
list<exprnodedesc> cols  string fieldprefix
list<fieldschema> schemas   new arraylist<fieldschema> cols size
for  int i   0  i < cols size    i
schemas add metastoreutils getfieldschemafromtypeinfo fieldprefix   i
cols get i  gettypeinfo
return schemas
/**
* convert the rowschema to fieldschema list.
*/
public static list<fieldschema> getfieldschemasfromrowschema rowschema row
string fieldprefix
arraylist<columninfo> c   row getsignature
return getfieldschemasfromcolumninfo c  fieldprefix
/**
* convert the columninfo to fieldschema.
*/
public static list<fieldschema> getfieldschemasfromcolumninfo
arraylist<columninfo> cols  string fieldprefix
if   cols    null      cols size      0
return new arraylist<fieldschema>
list<fieldschema> schemas   new arraylist<fieldschema> cols size
for  int i   0  i < cols size    i
string name   cols get i  getinternalname
if  name equals integer valueof i  tostring
name   fieldprefix   name
schemas add metastoreutils getfieldschemafromtypeinfo name  cols get i
gettype
return schemas
public static list<fieldschema> sortfieldschemas list<fieldschema> schema
collections sort schema  new comparator<fieldschema>
@override
public int compare fieldschema o1  fieldschema o2
return o1 getname   compareto o2 getname
return schema
/**
* create the reduce sink descriptor.
*
* @param keycols
*          the columns to be stored in the key
* @param valuecols
*          the columns to be stored in the value
* @param outputcolumnnames
*          the output columns names
* @param tag
*          the tag for this reducesink
* @param partitioncols
*          the columns for partitioning.
* @param numreducers
*          the number of reducers, set to -1 for automatic inference based on
*          input data size.
* @return the reducesinkdesc object.
*/
public static reducesinkdesc getreducesinkdesc
arraylist<exprnodedesc> keycols  arraylist<exprnodedesc> valuecols
list<string> outputcolumnnames  boolean includekeycols  int tag
arraylist<exprnodedesc> partitioncols  string order  int numreducers
return getreducesinkdesc keycols  keycols size    valuecols
new arraylist<list<integer>>
includekeycols ? outputcolumnnames sublist 0  keycols size
new arraylist<string>
includekeycols ? outputcolumnnames sublist keycols size
outputcolumnnames size      outputcolumnnames
includekeycols  tag  partitioncols  order  numreducers
/**
* create the reduce sink descriptor.
*
* @param keycols
*          the columns to be stored in the key
* @param numkeys
*          number of distribution key numbers. equals to group-by-key
*          numbers usually.
* @param valuecols
*          the columns to be stored in the value
* @param distinctcolindices
*          column indices for distinct aggregate parameters
* @param outputkeycolumnnames
*          the output key columns names
* @param outputvaluecolumnnames
*          the output value columns names
* @param tag
*          the tag for this reducesink
* @param partitioncols
*          the columns for partitioning.
* @param numreducers
*          the number of reducers, set to -1 for automatic inference based on
*          input data size.
* @return the reducesinkdesc object.
*/
public static reducesinkdesc getreducesinkdesc
final arraylist<exprnodedesc> keycols  int numkeys
arraylist<exprnodedesc> valuecols
list<list<integer>> distinctcolindices
list<string> outputkeycolumnnames
list<string> outputvaluecolumnnames
boolean includekeycols  int tag
arraylist<exprnodedesc> partitioncols  string order  int numreducers
tabledesc keytable   null
tabledesc valuetable   null
arraylist<string> outputkeycols   new arraylist<string>
arraylist<string> outputvalcols   new arraylist<string>
if  includekeycols
list<fieldschema> keyschema   getfieldschemasfromcolumnlistwithlength
keycols  distinctcolindices  outputkeycolumnnames  numkeys
if  order length   < outputkeycolumnnames size
order   order
keytable   getreducekeytabledesc keyschema  order
outputkeycols addall outputkeycolumnnames
else
keytable   getreducekeytabledesc getfieldschemasfromcolumnlist
keycols     order
for  int i   0  i < keycols size    i
outputkeycols add     i
valuetable   getreducevaluetabledesc getfieldschemasfromcolumnlist
valuecols  outputvaluecolumnnames  0
outputvalcols addall outputvaluecolumnnames
return new reducesinkdesc keycols  numkeys  valuecols  outputkeycols
distinctcolindices  outputvalcols
tag  partitioncols  numreducers  keytable
valuetable
/**
* create the reduce sink descriptor.
*
* @param keycols
*          the columns to be stored in the key
* @param valuecols
*          the columns to be stored in the value
* @param outputcolumnnames
*          the output columns names
* @param tag
*          the tag for this reducesink
* @param numpartitionfields
*          the first numpartitionfields of keycols will be partition columns.
*          if numpartitionfields=-1, then partition randomly.
* @param numreducers
*          the number of reducers, set to -1 for automatic inference based on
*          input data size.
* @return the reducesinkdesc object.
*/
public static reducesinkdesc getreducesinkdesc
arraylist<exprnodedesc> keycols  arraylist<exprnodedesc> valuecols
list<string> outputcolumnnames  boolean includekey  int tag
int numpartitionfields  int numreducers  throws semanticexception
return getreducesinkdesc keycols  keycols size    valuecols
new arraylist<list<integer>>
includekey ? outputcolumnnames sublist 0  keycols size
new arraylist<string>
includekey ?
outputcolumnnames sublist keycols size    outputcolumnnames size
outputcolumnnames
includekey  tag  numpartitionfields  numreducers
/**
* create the reduce sink descriptor.
*
* @param keycols
*          the columns to be stored in the key
* @param numkeys  number of distribution keys. equals to group-by-key
*        numbers usually.
* @param valuecols
*          the columns to be stored in the value
* @param distinctcolindices
*          column indices for distinct aggregates
* @param outputkeycolumnnames
*          the output key columns names
* @param outputvaluecolumnnames
*          the output value columns names
* @param tag
*          the tag for this reducesink
* @param numpartitionfields
*          the first numpartitionfields of keycols will be partition columns.
*          if numpartitionfields=-1, then partition randomly.
* @param numreducers
*          the number of reducers, set to -1 for automatic inference based on
*          input data size.
* @return the reducesinkdesc object.
*/
public static reducesinkdesc getreducesinkdesc
arraylist<exprnodedesc> keycols  int numkeys
arraylist<exprnodedesc> valuecols
list<list<integer>> distinctcolindices
list<string> outputkeycolumnnames  list<string> outputvaluecolumnnames
boolean includekey  int tag
int numpartitionfields  int numreducers  throws semanticexception
arraylist<exprnodedesc> partitioncols   null
if  numpartitionfields >  keycols size
partitioncols   keycols
else if  numpartitionfields >  0
partitioncols   new arraylist<exprnodedesc> numpartitionfields
for  int i   0  i < numpartitionfields  i
partitioncols add keycols get i
else
// numpartitionfields = -1 means random partitioning
partitioncols   new arraylist<exprnodedesc> 1
partitioncols add typecheckprocfactory defaultexprprocessor
getfuncexprnodedesc
stringbuilder order   new stringbuilder
for  int i   0  i < keycols size    i
order append
return getreducesinkdesc keycols  numkeys  valuecols  distinctcolindices
outputkeycolumnnames  outputvaluecolumnnames  includekey  tag
partitioncols  order tostring    numreducers
/**
* loads the storage handler (if one exists) for the given table
* and invokes {@link hivestoragehandler#configureinputjobproperties(tabledesc, java.util.map)}.
*
* @param tabledesc table descriptor
*/
public static void configureinputjobpropertiesforstoragehandler tabledesc tabledesc
configurejobpropertiesforstoragehandler true tabledesc
/**
* loads the storage handler (if one exists) for the given table
* and invokes {@link hivestoragehandler#configureoutputjobproperties(tabledesc, java.util.map)}.
*
* @param tabledesc table descriptor
*/
public static void configureoutputjobpropertiesforstoragehandler tabledesc tabledesc
configurejobpropertiesforstoragehandler false tabledesc
private static void configurejobpropertiesforstoragehandler boolean input
tabledesc tabledesc
if  tabledesc    null
return
try
hivestoragehandler storagehandler
hiveutils getstoragehandler
hive get   getconf
tabledesc getproperties   getproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_storage
if  storagehandler    null
map<string  string> jobproperties   new linkedhashmap<string  string>
if input
try
storagehandler configureinputjobproperties
tabledesc
jobproperties
catch abstractmethoderror e
log debug
e
storagehandler configuretablejobproperties tabledesc  jobproperties
else
try
storagehandler configureoutputjobproperties
tabledesc
jobproperties
catch abstractmethoderror e
log debug
e
storagehandler configuretablejobproperties tabledesc  jobproperties
if  tabledesc getoutputfileformatclass   getname
hivepassthroughoutputformat hive_passthrough_of_classname
// get the real output format when we register this for the table
jobproperties put
hivepassthroughoutputformat hive_passthrough_storagehandler_of_jobconfkey
hivefileformatutils getrealoutputformatclassname
// job properties are only relevant for non-native tables, so
// for native tables, leave it null to avoid cluttering up
// plans.
if   jobproperties isempty
tabledesc setjobproperties jobproperties
catch  hiveexception ex
throw new runtimeexception ex
public static void configurejobconf tabledesc tabledesc  jobconf jobconf
string handlerclass   tabledesc getproperties   getproperty
org apache hadoop hive metastore api hive_metastoreconstants meta_table_storage
try
hivestoragehandler storagehandler   hiveutils getstoragehandler jobconf  handlerclass
if  storagehandler    null
storagehandler configurejobconf tabledesc  jobconf
catch  hiveexception e
throw new runtimeexception e
public static string stripquotes string val
if   val charat 0          val charat val length     1
val charat 0
val   val substring 1  val length     1
return val
/**
* remove prefix from "path -> alias"
* this is required for testing.
* in order to verify that path is right, we need to display it in expected test result.
* but, mask pattern masks path with some patterns.
* so, we need to remove prefix from path which triggers mask pattern.
* @param origikey
* @return
*/
public static string removeprefixfromwarehouseconfig string origikey
string prefix   sessionstate get   getconf   getvar hiveconf confvars metastorewarehouse
if   prefix    null      prefix length   > 0
//local file system is using pfile:/// {@link proxylocalfilesystem}
prefix   prefix replace
int index   origikey indexof prefix
if  index >  1
origikey   origikey substring index   prefix length
return origikey
private planutils
// prevent instantiation
// add the input 'newinput' to the set of inputs for the query.
// the input may or may not be already present.
// the readentity also contains the parents from it is derived (only populated
// in case of views). the equals method for readentity does not compare the parents
// so that the same input with different parents cannot be added twice. if the input
// is already present, make sure the parents are added.
// consider the query:
// select * from (select * from v2 union all select * from v3) subq;
// where both v2 and v3 depend on v1
// addinput would be called twice for v1 (one with parent v2 and the other with parent v3).
// when addinput is called for the first time for v1, v1 (parent v2) is added to inputs.
// when addinput is called for the second time for v1, the input v1 from inputs is picked up,
// and it's parents are enhanced to include v2 and v3
// the inputs will contain: (v2, no parent), (v3, no parent), (v1, parents(v2, v3))
public static readentity addinput set<readentity> inputs  readentity newinput
// if the input is already present, make sure the new parent is added to the input.
if  inputs contains newinput
for  readentity input   inputs
if  input equals newinput
if   newinput getparents      null       newinput getparents   isempty
input getparents   addall newinput getparents
return input
assert false
else
inputs add newinput
return newinput
// make compile happy
return null