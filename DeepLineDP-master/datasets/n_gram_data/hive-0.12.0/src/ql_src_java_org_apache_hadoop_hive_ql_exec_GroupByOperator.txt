/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive ql exec
import java io serializable
import java lang management managementfactory
import java lang management memorymxbean
import java lang reflect field
import java sql timestamp
import java util arraylist
import java util arrays
import java util hashmap
import java util hashset
import java util iterator
import java util list
import java util map
import java util set
import javolution util fastbitset
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop conf configuration
import org apache hadoop hive conf hiveconf
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql parse opparsecontext
import org apache hadoop hive ql plan aggregationdesc
import org apache hadoop hive ql plan exprnodecolumndesc
import org apache hadoop hive ql plan exprnodeconstantdesc
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan groupbydesc
import org apache hadoop hive ql plan operatordesc
import org apache hadoop hive ql plan api operatortype
import org apache hadoop hive ql udf generic genericudafevaluator
import org apache hadoop hive ql udf generic genericudafevaluator aggregationbuffer
import org apache hadoop hive serde2 lazy bytearrayref
import org apache hadoop hive serde2 lazy lazybinary
import org apache hadoop hive serde2 lazy lazyprimitive
import org apache hadoop hive serde2 lazy lazystring
import org apache hadoop hive serde2 lazy objectinspector primitive lazybinaryobjectinspector
import org apache hadoop hive serde2 lazy objectinspector primitive lazystringobjectinspector
import org apache hadoop hive serde2 objectinspector objectinspector
import org apache hadoop hive serde2 objectinspector objectinspectorfactory
import org apache hadoop hive serde2 objectinspector objectinspectorutils
import org apache hadoop hive serde2 objectinspector objectinspectorutils objectinspectorcopyoption
import org apache hadoop hive serde2 objectinspector primitiveobjectinspector primitivecategory
import org apache hadoop hive serde2 objectinspector structfield
import org apache hadoop hive serde2 objectinspector structobjectinspector
import org apache hadoop hive serde2 objectinspector unionobject
import org apache hadoop hive serde2 typeinfo primitivetypeinfo
import org apache hadoop hive serde2 typeinfo typeinfo
import org apache hadoop hive serde2 typeinfo typeinfoutils
import org apache hadoop io byteswritable
import org apache hadoop io text
/**
* groupby operator implementation.
*/
public class groupbyoperator extends operator<groupbydesc> implements
serializable
private static final log log   logfactory getlog groupbyoperator class
getname
private static final long serialversionuid   1l
private static final int numrowsestimatesize   1000
public static final string counternamehashout
protected transient exprnodeevaluator keyfields
protected transient objectinspector keyobjectinspectors
protected transient exprnodeevaluator aggregationparameterfields
protected transient objectinspector aggregationparameterobjectinspectors
protected transient objectinspector aggregationparameterstandardobjectinspectors
protected transient object aggregationparameterobjects
// so aggregationisdistinct is a boolean array instead of a single number.
protected transient boolean aggregationisdistinct
// map from integer tag to distinct aggrs
transient protected map<integer  set<integer>> distinctkeyaggrs
new hashmap<integer  set<integer>>
// map from integer tag to non-distinct aggrs with key parameters.
transient protected map<integer  set<integer>> nondistinctkeyaggrs
new hashmap<integer  set<integer>>
// list of non-distinct aggrs.
transient protected list<integer> nondistinctaggrs   new arraylist<integer>
// union expr for distinct keys
transient exprnodeevaluator unionexpreval   null
transient genericudafevaluator aggregationevaluators
protected transient arraylist<objectinspector> objectinspectors
transient arraylist<string> fieldnames
// used by sort-based groupby: mode = complete, partial1, partial2,
// mergepartial
protected transient keywrapper currentkeys
protected transient keywrapper newkeys
protected transient aggregationbuffer aggregations
protected transient object aggregationsparameterslastinvoke
// used by hash-based groupby: mode = hash, partials
protected transient hashmap<keywrapper  aggregationbuffer> hashaggregations
// used by hash distinct aggregations when hashgrpkeynotredkey is true
protected transient hashset<keywrapper> keyscurrentgroup
transient boolean firstrow
transient long totalmemory
transient boolean hashaggr
// the reduction is happening on the reducer, and the grouping key and
// reduction keys are different.
// for example: select a, count(distinct b) from t group by a
// the data is sprayed by 'b' and the reducer is grouping it by 'a'
transient boolean groupkeyisnotreducekey
transient boolean firstrowingroup
transient long numrowsinput
transient long numrowshashtbl
transient int groupbymapaggrinterval
transient long numrowscomparehashaggr
transient float minreductionhashaggr
// current key objectinspectors are standard objectinspectors
protected transient objectinspector currentkeyobjectinspectors
// new key objectinspectors are objectinspectors from the parent
transient structobjectinspector newkeyobjectinspector
transient structobjectinspector currentkeyobjectinspector
public static memorymxbean memorymxbean
private long maxmemory
private float memorythreshold
private boolean groupingsetspresent
private int groupingsetsposition
private list<integer> groupingsets
private list<fastbitset> groupingsetsbitset
transient private list<object> newkeysgroupingsets
// for these positions, some variable primitive type (string) is used, so size
// cannot be estimated. sample it at runtime.
transient list<integer> keypositionssize
// for these positions, some variable primitive type (string) is used for the
// aggregation classes
transient list<field> aggrpositions
transient int fixedrowsize
transient long maxhashtblmemory
transient int totalvariablesize
transient int numentriesvarsize
transient int numentrieshashtable
transient int countafterreport       report or forward
transient int heartbeatinterval
public static fastbitset groupingset2bitset int value
fastbitset bits   new fastbitset
int index   0
while  value    0
if  value % 2    0
bits set index
index
value   value >>> 1
return bits
@override
protected void initializeop configuration hconf  throws hiveexception
totalmemory   runtime getruntime   totalmemory
numrowsinput   0
numrowshashtbl   0
heartbeatinterval   hiveconf getintvar hconf
hiveconf confvars hivesendheartbeat
countafterreport   0
groupingsetspresent   conf isgroupingsetspresent
objectinspector rowinspector   inputobjinspectors
// init keyfields
int numkeys   conf getkeys   size
keyfields   new exprnodeevaluator
keyobjectinspectors   new objectinspector
currentkeyobjectinspectors   new objectinspector
for  int i   0  i < numkeys  i
keyfields   exprnodeevaluatorfactory get conf getkeys   get i
keyobjectinspectors   keyfields initialize rowinspector
currentkeyobjectinspectors   objectinspectorutils
getstandardobjectinspector keyobjectinspectors
objectinspectorcopyoption writable
// initialize the constants for the grouping sets, so that they can be re-used for
// each row
if  groupingsetspresent
groupingsets   conf getlistgroupingsets
groupingsetsposition   conf getgroupingsetposition
newkeysgroupingsets   new arraylist<object>
groupingsetsbitset   new arraylist<fastbitset>
for  integer groupingset  groupingsets
// create the mapping corresponding to the grouping set
exprnodeevaluator groupingsetvalueevaluator
exprnodeevaluatorfactory get new exprnodeconstantdesc string valueof groupingset
newkeysgroupingsets add groupingsetvalueevaluator evaluate null
groupingsetsbitset add groupingset2bitset groupingset
// initialize unionexpr for reduce-side
// reduce key has union field as the last field if there are distinct
// aggregates in group-by.
list<? extends structfield> sfs
structobjectinspector  rowinspector  getallstructfieldrefs
if  sfs size   > 0
structfield keyfield   sfs get 0
if  keyfield getfieldname   touppercase   equals
utilities reducefield key name
objectinspector keyobjinspector   keyfield getfieldobjectinspector
if  keyobjinspector instanceof structobjectinspector
list<? extends structfield> keysfs
structobjectinspector  keyobjinspector  getallstructfieldrefs
if  keysfs size   > 0
// the last field is the union field, if any
structfield sf   keysfs get keysfs size     1
if  sf getfieldobjectinspector   getcategory   equals
objectinspector category union
unionexpreval   exprnodeevaluatorfactory get
new exprnodecolumndesc typeinfoutils gettypeinfofromobjectinspector
sf getfieldobjectinspector
keyfield getfieldname         sf getfieldname    null
false
unionexpreval initialize rowinspector
// init aggregationparameterfields
arraylist<aggregationdesc> aggrs   conf getaggregators
aggregationparameterfields   new exprnodeevaluator
aggregationparameterobjectinspectors   new objectinspector
aggregationparameterstandardobjectinspectors   new objectinspector
aggregationparameterobjects   new object
aggregationisdistinct   new boolean
for  int i   0  i < aggrs size    i
aggregationdesc aggr   aggrs get i
arraylist<exprnodedesc> parameters   aggr getparameters
aggregationparameterfields   new exprnodeevaluator
aggregationparameterobjectinspectors   new objectinspector[parameters
size  ]
aggregationparameterstandardobjectinspectors   new objectinspector[parameters
size  ]
aggregationparameterobjects   new object
for  int j   0  j < parameters size    j
aggregationparameterfields   exprnodeevaluatorfactory
get parameters get j
aggregationparameterobjectinspectors   aggregationparameterfields
initialize rowinspector
if  unionexpreval    null
string names   parameters get j  getexprstring   split
// parameters of the form : key.colx:t.coly
if  utilities reducefield key name   equals names
string name   names
int tag   integer parseint name split
if  aggr getdistinct
// is distinct
set<integer> set   distinctkeyaggrs get tag
if  null    set
set   new hashset<integer>
distinctkeyaggrs put tag  set
if   set contains i
set add i
else
set<integer> set   nondistinctkeyaggrs get tag
if  null    set
set   new hashset<integer>
nondistinctkeyaggrs put tag  set
if   set contains i
set add i
else
// will be value._colx
if   nondistinctaggrs contains i
nondistinctaggrs add i
else
if  aggr getdistinct
aggregationisdistinct   true
aggregationparameterstandardobjectinspectors   objectinspectorutils
getstandardobjectinspector
aggregationparameterobjectinspectors
objectinspectorcopyoption writable
aggregationparameterobjects   null
if  parameters size      0
// for ex: count(*)
if   nondistinctaggrs contains i
nondistinctaggrs add i
// init aggregationclasses
aggregationevaluators   new genericudafevaluator[conf getaggregators
size  ]
for  int i   0  i < aggregationevaluators length  i
aggregationdesc agg   conf getaggregators   get i
aggregationevaluators   agg getgenericudafevaluator
// init objectinspectors
int totalfields   keyfields length   aggregationevaluators length
objectinspectors   new arraylist<objectinspector> totalfields
for  exprnodeevaluator keyfield   keyfields
objectinspectors add null
mapredcontext context   mapredcontext get
if  context    null
for  genericudafevaluator genericudafevaluator   aggregationevaluators
context setup genericudafevaluator
for  int i   0  i < aggregationevaluators length  i
objectinspector roi   aggregationevaluators init conf getaggregators
get i  getmode    aggregationparameterobjectinspectors
objectinspectors add roi
aggregationsparameterslastinvoke   new object
if   conf getmode      groupbydesc mode hash    conf getbucketgroup
groupingsetspresent
aggregations   newaggregations
hashaggr   false
else
hashaggregations   new hashmap<keywrapper  aggregationbuffer> 256
aggregations   newaggregations
hashaggr   true
keypositionssize   new arraylist<integer>
aggrpositions   new list
groupbymapaggrinterval   hiveconf getintvar hconf
hiveconf confvars hivegroupbymapinterval
// compare every groupbymapaggrinterval rows
numrowscomparehashaggr   groupbymapaggrinterval
minreductionhashaggr   hiveconf getfloatvar hconf
hiveconf confvars hivemapaggrhashminreduction
groupkeyisnotreducekey   conf getgroupkeynotreductionkey
if  groupkeyisnotreducekey
keyscurrentgroup   new hashset<keywrapper>
fieldnames   conf getoutputcolumnnames
for  int i   0  i < keyfields length  i
objectinspectors set i  currentkeyobjectinspectors
// generate key names
arraylist<string> keynames   new arraylist<string> keyfields length
for  int i   0  i < keyfields length  i
keynames add fieldnames get i
newkeyobjectinspector   objectinspectorfactory
getstandardstructobjectinspector keynames  arrays
aslist keyobjectinspectors
currentkeyobjectinspector   objectinspectorfactory
getstandardstructobjectinspector keynames  arrays
aslist currentkeyobjectinspectors
outputobjinspector   objectinspectorfactory
getstandardstructobjectinspector fieldnames  objectinspectors
keywrapperfactory keywrapperfactory
new keywrapperfactory keyfields  keyobjectinspectors  currentkeyobjectinspectors
newkeys   keywrapperfactory getkeywrapper
firstrow   true
// estimate the number of hash table entries based on the size of each
// entry. since the size of a entry
// is not known, estimate that based on the number of entries
if  hashaggr
computemaxentrieshashaggr hconf
memorymxbean   managementfactory getmemorymxbean
maxmemory   memorymxbean getheapmemoryusage   getmax
memorythreshold   this getconf   getmemorythreshold
initializechildren hconf
/**
* estimate the number of entries in map-side hash table. the user can specify
* the total amount of memory to be used by the map-side hash. by default, all
* available memory is used. the size of each row is estimated, rather
* crudely, and the number of entries are figure out based on that.
*
* @return number of entries that can fit in hash table - useful for map-side
*         aggregation only
**/
private void computemaxentrieshashaggr configuration hconf  throws hiveexception
float memorypercentage   this getconf   getgroupbymemoryusage
maxhashtblmemory    long   memorypercentage   runtime getruntime   maxmemory
estimaterowsize
private static final int javaobjectoverhead   64
private static final int javahashentryoverhead   64
private static final int javasizeprimitivetype   16
private static final int javasizeunknowntype   256
/**
* the size of the element at position 'pos' is returned, if possible. if the
* datatype is of variable length, string, a list of such key positions is
* maintained, and the size for such positions is then actually calculated at
* runtime.
*
* @param pos
*          the position of the key
* @param c
*          the type of the key
* @return the size of this datatype
**/
private int getsize int pos  primitivecategory category
switch  category
case void
case boolean
case byte
case short
case int
case long
case float
case double
return javasizeprimitivetype
case string
keypositionssize add new integer pos
return javaobjectoverhead
case binary
keypositionssize add new integer pos
return javaobjectoverhead
case timestamp
return javaobjectoverhead   javasizeprimitivetype
default
return javasizeunknowntype
/**
* the size of the element at position 'pos' is returned, if possible. if the
* field is of variable length, string, a list of such field names for the
* field position is maintained, and the size for such positions is then
* actually calculated at runtime.
*
* @param pos
*          the position of the key
* @param c
*          the type of the key
* @param f
*          the field to be added
* @return the size of this datatype
**/
private int getsize int pos  class<?> c  field f
if  c isprimitive
c isinstance boolean valueof true
c isinstance byte valueof  byte  0
c isinstance short valueof  short  0
c isinstance integer valueof 0
c isinstance long valueof 0
c isinstance new float 0
c isinstance new double 0
return javasizeprimitivetype
if  c isinstance new timestamp 0
return javaobjectoverhead   javasizeprimitivetype
if  c isinstance new string       c isinstance new bytearrayref
if  aggrpositions    null
aggrpositions   new arraylist<field>
aggrpositions add f
return javaobjectoverhead
return javasizeunknowntype
/**
* @param pos
*          position of the key
* @param typeinfo
*          type of the input
* @return the size of this datatype
**/
private int getsize int pos  typeinfo typeinfo
if  typeinfo instanceof primitivetypeinfo
return getsize pos    primitivetypeinfo  typeinfo  getprimitivecategory
return javasizeunknowntype
/**
* @return the size of each row
**/
private void estimaterowsize   throws hiveexception
// estimate the size of each entry -
// a datatype with unknown size (string/struct etc. - is assumed to be 256
// bytes for now).
// 64 bytes is the overhead for a reference
fixedrowsize   javahashentryoverhead
arraylist<exprnodedesc> keys   conf getkeys
// go over all the keys and get the size of the fields of fixed length. keep
// track of the variable length keys
for  int pos   0  pos < keys size    pos
fixedrowsize    getsize pos  keys get pos  gettypeinfo
// go over all the aggregation classes and and get the size of the fields of
// fixed length. keep track of the variable length
// fields in these aggregation classes.
for  int i   0  i < aggregationevaluators length  i
fixedrowsize    javaobjectoverhead
aggregationbuffer agg   aggregationevaluators getnewaggregationbuffer
if  genericudafevaluator isestimable agg
continue
field farr   objectinspectorutils getdeclarednonstaticfields agg getclass
for  field f   farr
fixedrowsize    getsize i  f gettype    f
protected aggregationbuffer newaggregations   throws hiveexception
aggregationbuffer aggs   new aggregationbuffer
for  int i   0  i < aggregationevaluators length  i
aggs   aggregationevaluators getnewaggregationbuffer
// aggregationclasses[i].reset(aggs[i]);
return aggs
protected void resetaggregations aggregationbuffer aggs  throws hiveexception
for  int i   0  i < aggs length  i
aggregationevaluators reset aggs
/*
* update aggregations. if the aggregation is for distinct, in case of hash
* aggregation, the client tells us whether it is a new entry. for sort-based
* aggregations, the last row is compared with the current one to figure out
* whether it has changed. as a cleanup, the lastinvoke logic can be pushed in
* the caller, and this function can be independent of that. the client should
* always notify whether it is a different row or not.
*
* @param aggs the aggregations to be evaluated
*
* @param row the row being processed
*
* @param rowinspector the inspector for the row
*
* @param hashaggr whether hash aggregation is being performed or not
*
* @param newentryforhashaggr only valid if it is a hash aggregation, whether
* it is a new entry or not
*/
protected void updateaggregations aggregationbuffer aggs  object row
objectinspector rowinspector  boolean hashaggr
boolean newentryforhashaggr  object lastinvoke  throws hiveexception
if  unionexpreval    null
for  int ai   0  ai < aggs length  ai
// calculate the parameters
object o   new object length]
for  int pi   0  pi < aggregationparameterfields length  pi
o   aggregationparameterfields evaluate row
// update the aggregations.
if  aggregationisdistinct
if  hashaggr
if  newentryforhashaggr
aggregationevaluators aggregate aggs  o
else
if  lastinvoke    null
lastinvoke   new object
if  objectinspectorutils compare o
aggregationparameterobjectinspectors  lastinvoke
aggregationparameterstandardobjectinspectors     0
aggregationevaluators aggregate aggs  o
for  int pi   0  pi < o length  pi
lastinvoke   objectinspectorutils copytostandardobject
o  aggregationparameterobjectinspectors
objectinspectorcopyoption writable
else
aggregationevaluators aggregate aggs  o
return
if  distinctkeyaggrs size   > 0
// evaluate union object
unionobject uo    unionobject   unionexpreval evaluate row
int uniontag   uo gettag
// update non-distinct key aggregations : "key._colx:t._coly"
if  nondistinctkeyaggrs get uniontag     null
for  int pos   nondistinctkeyaggrs get uniontag
object o   new object length]
for  int pi   0  pi < aggregationparameterfields length  pi
o   aggregationparameterfields evaluate row
aggregationevaluators aggregate aggs  o
// there may be multi distinct clauses for one column
// update them all.
if  distinctkeyaggrs get uniontag     null
for  int i   distinctkeyaggrs get uniontag
object o   new object length]
for  int pi   0  pi < aggregationparameterfields length  pi
o   aggregationparameterfields evaluate row
if  hashaggr
if  newentryforhashaggr
aggregationevaluators aggregate aggs  o
else
if  lastinvoke    null
lastinvoke   new object
if  objectinspectorutils compare o
aggregationparameterobjectinspectors
lastinvoke
aggregationparameterstandardobjectinspectors     0
aggregationevaluators aggregate aggs  o
for  int pi   0  pi < o length  pi
lastinvoke   objectinspectorutils copytostandardobject
o  aggregationparameterobjectinspectors
objectinspectorcopyoption writable
// update non-distinct value aggregations: 'value._colx'
// these aggregations should be updated only once.
if  uniontag    0
for  int pos   nondistinctaggrs
object o   new object length]
for  int pi   0  pi < aggregationparameterfields length  pi
o   aggregationparameterfields evaluate row
aggregationevaluators aggregate aggs  o
else
for  int ai   0  ai < aggs length  ai
// there is no distinct aggregation,
// update all aggregations
object o   new object length]
for  int pi   0  pi < aggregationparameterfields length  pi
o   aggregationparameterfields evaluate row
aggregationevaluators aggregate aggs  o
@override
public void startgroup   throws hiveexception
firstrowingroup   true
super startgroup
@override
public void endgroup   throws hiveexception
if  groupkeyisnotreducekey
keyscurrentgroup clear
private void processkey object row
objectinspector rowinspector  throws hiveexception
if  hashaggr
newkeys sethashkey
processhashaggr row  rowinspector  newkeys
else
processaggr row  rowinspector  newkeys
firstrowingroup   false
if  countafterreport    0     countafterreport % heartbeatinterval     0
reporter    null
reporter progress
countafterreport   0
@override
public void processop object row  int tag  throws hiveexception
firstrow   false
objectinspector rowinspector   inputobjinspectors
// total number of input rows is needed for hash aggregation only
if  hashaggr     groupkeyisnotreducekey
numrowsinput
// if hash aggregation is not behaving properly, disable it
if  numrowsinput    numrowscomparehashaggr
numrowscomparehashaggr    groupbymapaggrinterval
// map-side aggregation should reduce the entries by at-least half
if  numrowshashtbl > numrowsinput   minreductionhashaggr
log warn     numrowshashtbl
numrowsinput       1 0
numrowshashtbl   numrowsinput
minreductionhashaggr
flushhashtable true
hashaggr   false
else
log trace     numrowshashtbl
numrowsinput       1 0
numrowshashtbl   numrowsinput
minreductionhashaggr
try
countafterreport
newkeys getnewkey row  rowinspector
if  groupingsetspresent
object newkeysarray   newkeys getkeyarray
object clonenewkeysarray   new object
for  int keypos   0  keypos < groupingsetsposition  keypos
clonenewkeysarray   newkeysarray
for  int groupingsetpos   0  groupingsetpos < groupingsets size    groupingsetpos
for  int keypos   0  keypos < groupingsetsposition  keypos
newkeysarray   null
fastbitset bitset   groupingsetsbitset get groupingsetpos
// some keys need to be left to null corresponding to that grouping set.
for  int keypos   bitset nextsetbit 0   keypos >  0
keypos   bitset nextsetbit keypos 1
newkeysarray   clonenewkeysarray
newkeysarray   newkeysgroupingsets get groupingsetpos
processkey row  rowinspector
else
processkey row  rowinspector
catch  hiveexception e
throw e
catch  exception e
throw new hiveexception e
private void processhashaggr object row  objectinspector rowinspector
keywrapper newkeys  throws hiveexception
// prepare aggs for updating
aggregationbuffer aggs   null
boolean newentryforhashaggr   false
// hash-based aggregations
aggs   hashaggregations get newkeys
if  aggs    null
keywrapper newkeyprober   newkeys copykey
aggs   newaggregations
hashaggregations put newkeyprober  aggs
newentryforhashaggr   true
numrowshashtbl       new entry in the hash table
// if the grouping key and the reduction key are different, a set of
// grouping keys for the current reduction key are maintained in
// keyscurrentgroup
// peek into the set to find out if a new grouping key is seen for the given
// reduction key
if  groupkeyisnotreducekey
newentryforhashaggr   keyscurrentgroup add newkeys copykey
// update the aggs
updateaggregations aggs  row  rowinspector  true  newentryforhashaggr  null
// we can only flush after the updateaggregations is done, or the
// potentially new entry "aggs"
// can be flushed out of the hash table.
// based on user-specified parameters, check if the hash table needs to be
// flushed.
// if the grouping key is not the same as reduction key, flushing can only
// happen at boundaries
if    groupkeyisnotreducekey    firstrowingroup
shouldbeflushed newkeys
flushhashtable false
// non-hash aggregation
private void processaggr object row
objectinspector rowinspector
keywrapper newkeys  throws hiveexception
// prepare aggs for updating
aggregationbuffer aggs   null
object lastinvoke   null
//boolean keysareequal = (currentkeys != null && newkeys != null)?
//  newkeystructequalcomparer.areequal(currentkeys, newkeys) : false;
boolean keysareequal    currentkeys    null    newkeys    null ?
newkeys equals currentkeys    false
// forward the current keys if needed for sort-based aggregation
if  currentkeys    null     keysareequal
// this is to optimize queries of the form:
// select count(distinct key) from t
// where t is sorted and bucketized by key
// partial aggregation is performed on the mapper, and the
// reducer gets 1 row (partial result) per mapper.
if   conf isdontresetaggrsdistinct
forward currentkeys getkeyarray    aggregations
countafterreport   0
// need to update the keys?
if  currentkeys    null     keysareequal
if  currentkeys    null
currentkeys   newkeys copykey
else
currentkeys copykey newkeys
// reset the aggregations
// for distincts optimization with sorting/bucketing, perform partial aggregation
if   conf isdontresetaggrsdistinct
resetaggregations aggregations
// clear parameters in last-invoke
for  int i   0  i < aggregationsparameterslastinvoke length  i
aggregationsparameterslastinvoke   null
aggs   aggregations
lastinvoke   aggregationsparameterslastinvoke
// update the aggs
updateaggregations aggs  row  rowinspector  false  false  lastinvoke
/**
* based on user-parameters, should the hash table be flushed.
*
* @param newkeys
*          keys for the row under consideration
**/
private boolean shouldbeflushed keywrapper newkeys
int numentries   hashaggregations size
long usedmemory
float rate
// the fixed size for the aggregation class is already known. get the
// variable portion of the size every numrowsestimatesize rows.
if   numentrieshashtable    0       numentries % numrowsestimatesize     0
//check how much memory left memory
usedmemory   memorymxbean getheapmemoryusage   getused
rate    float  usedmemory    float  maxmemory
if rate > memorythreshold
return true
for  integer pos   keypositionssize
object key   newkeys getkeyarray
// ignore nulls
if  key    null
if  key instanceof lazystring
totalvariablesize
lazyprimitive<lazystringobjectinspector  text>  key
getwritableobject   getlength
else if  key instanceof string
totalvariablesize      string  key  length
else if  key instanceof text
totalvariablesize      text  key  getlength
else if  key instanceof lazybinary
totalvariablesize
lazyprimitive<lazybinaryobjectinspector  byteswritable>  key
getwritableobject   getlength
else if  key instanceof byteswritable
totalvariablesize      byteswritable  key  getlength
else if  key instanceof bytearrayref
totalvariablesize      bytearrayref  key  getdata   length
aggregationbuffer aggs   hashaggregations get newkeys
for  int i   0  i < aggs length  i
aggregationbuffer agg   aggs
if  genericudafevaluator isestimable agg
totalvariablesize      genericudafevaluator abstractaggregationbuffer agg  estimate
continue
if  aggrpositions    null
totalvariablesize    estimatesize agg  aggrpositions
numentriesvarsize
// update the number of entries that can fit in the hash table
numentrieshashtable
int   maxhashtblmemory    fixedrowsize    totalvariablesize   numentriesvarsize
log trace     numentries
numentrieshashtable
// flush if necessary
if  numentries >  numentrieshashtable
return true
return false
private int estimatesize aggregationbuffer agg  list<field> fields
int length   0
for  field f   fields
try
object o   f get agg
if  o instanceof string
length      string o  length
else if  o instanceof bytearrayref
length      bytearrayref o  getdata   length
catch  exception e
// continue.. null out the field?
return length
/**
* flush hash table. this method is used by hash-based aggregations
* @param complete
* @throws hiveexception
*/
private void flushhashtable boolean complete  throws hiveexception
countafterreport   0
// currently, the algorithm flushes 10% of the entries - this can be
// changed in the future
if  complete
iterator<map entry<keywrapper  aggregationbuffer>> iter   hashaggregations
entryset   iterator
while  iter hasnext
map entry<keywrapper  aggregationbuffer> m   iter next
forward m getkey   getkeyarray    m getvalue
hashaggregations clear
hashaggregations   null
log info
return
int oldsize   hashaggregations size
log info     oldsize
iterator<map entry<keywrapper  aggregationbuffer>> iter   hashaggregations
entryset   iterator
int numdel   0
while  iter hasnext
map entry<keywrapper  aggregationbuffer> m   iter next
forward m getkey   getkeyarray    m getvalue
iter remove
numdel
if  numdel   10 >  oldsize
log info     hashaggregations size
return
transient object forwardcache
/**
* forward a record of keys and aggregation results.
*
* @param keys
*          the keys in the record
* @throws hiveexception
*/
protected void forward object keys
aggregationbuffer aggs  throws hiveexception
int totalfields   keys length   aggs length
if  forwardcache    null
forwardcache   new object
for  int i   0  i < keys length  i
forwardcache   keys
for  int i   0  i < aggs length  i
forwardcache   aggregationevaluators evaluate aggs
forward forwardcache  outputobjinspector
/**
* forward all aggregations to children. it is only used by demuxoperator.
* @throws hiveexception
*/
@override
public void flush   throws hiveexception
try
if  hashaggregations    null
log info
hashaggregations size
iterator iter   hashaggregations entryset   iterator
while  iter hasnext
map entry<keywrapper  aggregationbuffer> m    map entry  iter
next
forward m getkey   getkeyarray    m getvalue
iter remove
hashaggregations clear
else if  aggregations    null
// sort-based aggregations
if  currentkeys    null
forward currentkeys getkeyarray    aggregations
currentkeys   null
else
// the groupbyoperator is not initialized, which means there is no
// data
// (since we initialize the operators when we see the first record).
// just do nothing here.
catch  exception e
throw new hiveexception e
/**
* we need to forward all the aggregations to children.
*
*/
@override
public void closeop boolean abort  throws hiveexception
if   abort
try
// put the hash related stats in statsmap if applicable, so that they
// are sent to jt as counters
if  hashaggr    counternametoenum    null
incrcounter counternamehashout  numrowshashtbl
// if there is no grouping key and no row came to this operator
if  firstrow     keyfields length    0
firstrow   false
// there is no grouping key - simulate a null row
// this is based on the assumption that a null row is ignored by
// aggregation functions
for  int ai   0  ai < aggregations length  ai
// o is set to null in order to distinguish no rows at all
object o
if  aggregationparameterfields length > 0
o   new object length]
else
o   null
// calculate the parameters
for  int pi   0  pi < aggregationparameterfields length  pi
o   null
aggregationevaluators aggregate aggregations  o
// create dummy keys - size 0
forward new object  aggregations
else
flush
catch  exception e
throw new hiveexception e
@override
protected list<string> getadditionalcounters
list<string> ctrlist   new arraylist<string>
ctrlist add getwrappedcountername counternamehashout
return ctrlist
// group by contains the columns needed - no need to aggregate from children
public list<string> gencollists
hashmap<operator<? extends operatordesc>  opparsecontext> opparsectx
list<string> collists   new arraylist<string>
arraylist<exprnodedesc> keys   conf getkeys
for  exprnodedesc key   keys
collists   utilities mergeuniqelems collists  key getcols
arraylist<aggregationdesc> aggrs   conf getaggregators
for  aggregationdesc aggr   aggrs
arraylist<exprnodedesc> params   aggr getparameters
for  exprnodedesc param   params
collists   utilities mergeuniqelems collists  param getcols
return collists
/**
* @return the name of the operator
*/
@override
public string getname
return getoperatorname
static public string getoperatorname
return
@override
public operatortype gettype
return operatortype groupby
/**
* we can push the limit above gby (running in reducer), since that will generate single row
* for each group. this doesn't necessarily hold for gby (running in mappers),
* so we don't push limit above it.
*/
@override
public boolean acceptlimitpushdown
return getconf   getmode      groupbydesc mode mergepartial
getconf   getmode      groupbydesc mode complete