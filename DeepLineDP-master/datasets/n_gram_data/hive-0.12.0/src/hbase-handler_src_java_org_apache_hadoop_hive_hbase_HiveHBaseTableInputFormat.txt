/**
* licensed to the apache software foundation (asf) under one
* or more contributor license agreements.  see the notice file
* distributed with this work for additional information
* regarding copyright ownership.  the asf licenses this file
* to you under the apache license, version 2.0 (the
* "license"); you may not use this file except in compliance
* with the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
package org apache hadoop hive hbase
import java io ioexception
import java util arraylist
import java util list
import org apache commons logging log
import org apache commons logging logfactory
import org apache hadoop fs path
import org apache hadoop hbase hbaseconfiguration
import org apache hadoop hbase hconstants
import org apache hadoop hbase client htable
import org apache hadoop hbase client result
import org apache hadoop hbase client scan
import org apache hadoop hbase io immutablebyteswritable
import org apache hadoop hbase mapred tablemapreduceutil
import org apache hadoop hbase mapreduce tableinputformatbase
import org apache hadoop hbase mapreduce tablesplit
import org apache hadoop hbase util bytes
import org apache hadoop hbase util writables
import org apache hadoop hive hbase hbaseserde columnmapping
import org apache hadoop hive ql exec exprnodeconstantevaluator
import org apache hadoop hive ql exec utilities
import org apache hadoop hive ql index indexpredicateanalyzer
import org apache hadoop hive ql index indexsearchcondition
import org apache hadoop hive ql metadata hiveexception
import org apache hadoop hive ql plan exprnodedesc
import org apache hadoop hive ql plan tablescandesc
import org apache hadoop hive serde serdeconstants
import org apache hadoop hive serde2 bytestream
import org apache hadoop hive serde2 columnprojectionutils
import org apache hadoop hive serde2 serdeexception
import org apache hadoop hive serde2 io bytewritable
import org apache hadoop hive serde2 io doublewritable
import org apache hadoop hive serde2 io shortwritable
import org apache hadoop hive serde2 lazy lazyutils
import org apache hadoop hive serde2 objectinspector primitiveobjectinspector
import org apache hadoop hive serde2 objectinspector primitiveobjectinspector primitivecategory
import org apache hadoop hive shims shimloader
import org apache hadoop io booleanwritable
import org apache hadoop io floatwritable
import org apache hadoop io intwritable
import org apache hadoop io longwritable
import org apache hadoop io text
import org apache hadoop mapred inputformat
import org apache hadoop mapred inputsplit
import org apache hadoop mapred jobconf
import org apache hadoop mapred recordreader
import org apache hadoop mapred reporter
import org apache hadoop mapreduce job
import org apache hadoop mapreduce jobcontext
import org apache hadoop mapreduce taskattemptcontext
import org apache hadoop mapreduce lib input fileinputformat
/**
* hivehbasetableinputformat implements inputformat for hbase storage handler
* tables, decorating an underlying hbase tableinputformat with extra hive logic
* such as column pruning and filter pushdown.
*/
public class hivehbasetableinputformat extends tableinputformatbase
implements inputformat<immutablebyteswritable  result>
static final log log   logfactory getlog hivehbasetableinputformat class
@override
public recordreader<immutablebyteswritable  result> getrecordreader
inputsplit split
jobconf jobconf
final reporter reporter  throws ioexception
hbasesplit hbasesplit    hbasesplit  split
tablesplit tablesplit   hbasesplit getsplit
string hbasetablename   jobconf get hbaseserde hbase_table_name
sethtable new htable hbaseconfiguration create jobconf   bytes tobytes hbasetablename
string hbasecolumnsmapping   jobconf get hbaseserde hbase_columns_mapping
boolean docolumnregexmatching   jobconf getboolean hbaseserde hbase_columns_regex_matching  true
list<integer> readcolids   columnprojectionutils getreadcolumnids jobconf
list<columnmapping> columnsmapping   null
try
columnsmapping   hbaseserde parsecolumnsmapping hbasecolumnsmapping  docolumnregexmatching
catch  serdeexception e
throw new ioexception e
if  columnsmapping size   < readcolids size
throw new ioexception
boolean addall    readcolids size      0
scan scan   new scan
boolean empty   true
// the list of families that have been added to the scan
list<string> addedfamilies   new arraylist<string>
if   addall
for  int i   readcolids
columnmapping colmap   columnsmapping get i
if  colmap hbaserowkey
continue
if  colmap qualifiername    null
scan addfamily colmap familynamebytes
addedfamilies add colmap familyname
else
if  addedfamilies contains colmap familyname
// add only if the corresponding family has not already been added
scan addcolumn colmap familynamebytes  colmap qualifiernamebytes
empty   false
// the hbase table's row key maps to a hive table column. in the corner case when only the
// row key column is selected in hive, the hbase scan will be empty i.e. no column family/
// column qualifier will have been added to the scan. we arbitrarily add at least one column
// to the hbase scan so that we can retrieve all of the row keys and return them as the hive
// tables column projection.
if  empty
for  int i   0  i < columnsmapping size    i
columnmapping colmap   columnsmapping get i
if  colmap hbaserowkey
continue
if  colmap qualifiername    null
scan addfamily colmap familynamebytes
else
scan addcolumn colmap familynamebytes  colmap qualifiernamebytes
if   addall
break
string scancache   jobconf get hbaseserde hbase_scan_cache
if  scancache    null
scan setcaching integer valueof scancache
string scancacheblocks   jobconf get hbaseserde hbase_scan_cacheblocks
if  scancacheblocks    null
scan setcacheblocks boolean valueof scancacheblocks
string scanbatch   jobconf get hbaseserde hbase_scan_batch
if  scanbatch    null
scan setbatch integer valueof scanbatch
// if hive's optimizer gave us a filter to process, convert it to the
// hbase scan form now.
int ikey    1
try
ikey   hbaseserde getrowkeycolumnoffset columnsmapping
catch  serdeexception e
throw new ioexception e
tablesplit   convertfilter jobconf  scan  tablesplit  ikey
getstorageformatofkey columnsmapping get ikey  mappingspec
jobconf get hbaseserde hbase_table_default_storage_type
setscan scan
job job   new job jobconf
taskattemptcontext tac   shimloader gethadoopshims   newtaskattemptcontext
job getconfiguration    reporter
final org apache hadoop mapreduce recordreader<immutablebyteswritable  result>
recordreader   createrecordreader tablesplit  tac
return new recordreader<immutablebyteswritable  result>
@override
public void close   throws ioexception
recordreader close
@override
public immutablebyteswritable createkey
return new immutablebyteswritable
@override
public result createvalue
return new result
@override
public long getpos   throws ioexception
return 0
@override
public float getprogress   throws ioexception
float progress   0 0f
try
progress   recordreader getprogress
catch  interruptedexception e
throw new ioexception e
return progress
@override
public boolean next immutablebyteswritable rowkey  result value  throws ioexception
boolean next   false
try
next   recordreader nextkeyvalue
if  next
rowkey set recordreader getcurrentvalue   getrow
writables copywritable recordreader getcurrentvalue    value
catch  interruptedexception e
throw new ioexception e
return next
/**
* converts a filter (which has been pushed down from hive's optimizer)
* into corresponding restrictions on the hbase scan.  the
* filter should already be in a form which can be fully converted.
*
* @param jobconf configuration for the scan
*
* @param scan the hbase scan object to restrict
*
* @param tablesplit the hbase table split to restrict, or null
* if calculating splits
*
* @param ikey 0-based offset of key column within hive table
*
* @return converted table split if any
*/
private tablesplit convertfilter
jobconf jobconf
scan scan
tablesplit tablesplit
int ikey  boolean iskeybinary
throws ioexception
string filterexprserialized
jobconf get tablescandesc filter_expr_conf_str
if  filterexprserialized    null
return tablesplit
exprnodedesc filterexpr
utilities deserializeexpression filterexprserialized  jobconf
string colname   jobconf get serdeconstants list_columns  split
string coltype   jobconf get serdeconstants list_column_types  split
indexpredicateanalyzer analyzer   newindexpredicateanalyzer colname coltype  iskeybinary
list<indexsearchcondition> searchconditions
new arraylist<indexsearchcondition>
exprnodedesc residualpredicate
analyzer analyzepredicate filterexpr  searchconditions
// there should be no residual since we already negotiated
// that earlier in hbasestoragehandler.decomposepredicate.
if  residualpredicate    null
throw new runtimeexception
residualpredicate getexprstring
// there should be exactly one predicate since we already
// negotiated that also.
if  searchconditions size   < 1    searchconditions size   > 2
throw new runtimeexception
// convert the search condition into a restriction on the hbase scan
byte  startrow   hconstants empty_start_row  stoprow   hconstants empty_end_row
for  indexsearchcondition sc   searchconditions
exprnodeconstantevaluator eval   new exprnodeconstantevaluator sc getconstantdesc
primitiveobjectinspector objinspector
object writable
try
objinspector    primitiveobjectinspector eval initialize null
writable   eval evaluate null
catch  classcastexception cce
throw new ioexception
sc getconstantdesc   gettypestring
catch  hiveexception e
throw new ioexception e
byte  constantval   getconstantval writable  objinspector  iskeybinary
string comparisonop   sc getcomparisonop
if   equals comparisonop
startrow   constantval
stoprow   getnextba constantval
else if    equals comparisonop
stoprow   constantval
else if
equals comparisonop
startrow   constantval
else if
equals comparisonop
startrow   getnextba constantval
else if
equals comparisonop
stoprow   getnextba constantval
else
throw new ioexception comparisonop
if  tablesplit    null
tablesplit   new tablesplit
tablesplit gettablename
startrow
stoprow
tablesplit getregionlocation
scan setstartrow startrow
scan setstoprow stoprow
return tablesplit
private byte getconstantval object writable  primitiveobjectinspector poi
boolean iskeybinary  throws ioexception
if   iskeybinary
// key is stored in text format. get bytes representation of constant also of
// text format.
byte startrow
bytestream output serializestream   new bytestream output
lazyutils writeprimitiveutf8 serializestream  writable  poi  false   byte  0  null
startrow   new byte
system arraycopy serializestream getdata    0  startrow  0  serializestream getcount
return startrow
primitivecategory pc   poi getprimitivecategory
switch  poi getprimitivecategory
case int
return bytes tobytes   intwritable writable  get
case boolean
return bytes tobytes   booleanwritable writable  get
case long
return bytes tobytes   longwritable writable  get
case float
return bytes tobytes   floatwritable writable  get
case double
return bytes tobytes   doublewritable writable  get
case short
return bytes tobytes   shortwritable writable  get
case string
return bytes tobytes   text writable  tostring
case byte
return bytes tobytes   bytewritable writable  get
default
throw new ioexception     pc
private byte getnextba byte current
// startrow is inclusive while stoprow is exclusive,
//this util method returns very next bytearray which will occur after the current one
// by padding current one with a trailing 0 byte.
byte next   new byte
system arraycopy current  0  next  0  current length
return next
/**
* instantiates a new predicate analyzer suitable for
* determining how to push a filter down into the hbase scan,
* based on the rules for what kinds of pushdown we currently support.
*
* @param keycolumnname name of the hive column mapped to the hbase row key
*
* @return preconfigured predicate analyzer
*/
static indexpredicateanalyzer newindexpredicateanalyzer
string keycolumnname  string keycoltype  boolean iskeybinary
indexpredicateanalyzer analyzer   new indexpredicateanalyzer
// we can always do equality predicate. just need to make sure we get appropriate
// ba representation of constant of filter condition.
analyzer addcomparisonop
// we can do other comparisons only if storage format in hbase is either binary
// or we are dealing with string types since there lexographic ordering will suffice.
if iskeybinary     keycoltype equalsignorecase
analyzer addcomparisonop
analyzer addcomparisonop
analyzer addcomparisonop
analyzer addcomparisonop
// and only on the key column
analyzer clearallowedcolumnnames
analyzer allowcolumnname keycolumnname
return analyzer
@override
public inputsplit getsplits jobconf jobconf  int numsplits  throws ioexception
//obtain delegation tokens for the job
tablemapreduceutil initcredentials jobconf
string hbasetablename   jobconf get hbaseserde hbase_table_name
sethtable new htable hbaseconfiguration create jobconf   bytes tobytes hbasetablename
string hbasecolumnsmapping   jobconf get hbaseserde hbase_columns_mapping
boolean docolumnregexmatching   jobconf getboolean hbaseserde hbase_columns_regex_matching  true
if  hbasecolumnsmapping    null
throw new ioexception
list<columnmapping> columnsmapping   null
try
columnsmapping   hbaseserde parsecolumnsmapping hbasecolumnsmapping docolumnregexmatching
catch  serdeexception e
throw new ioexception e
int ikey
try
ikey   hbaseserde getrowkeycolumnoffset columnsmapping
catch  serdeexception e
throw new ioexception e
scan scan   new scan
// the list of families that have been added to the scan
list<string> addedfamilies   new arraylist<string>
// review:  are we supposed to be applying the getreadcolumnids
// same as in getrecordreader?
for  int i   0  i <columnsmapping size    i
columnmapping colmap   columnsmapping get i
if  colmap hbaserowkey
continue
if  colmap qualifiername    null
scan addfamily colmap familynamebytes
addedfamilies add colmap familyname
else
if  addedfamilies contains colmap familyname
// add the column only if the family has not already been added
scan addcolumn colmap familynamebytes  colmap qualifiernamebytes
// take filter pushdown into account while calculating splits; this
// allows us to prune off regions immediately.  note that although
// the javadoc for the superclass getsplits says that it returns one
// split per region, the implementation actually takes the scan
// definition into account and excludes regions which don't satisfy
// the start/stop row conditions (hbase-1829).
convertfilter jobconf  scan  null  ikey
getstorageformatofkey columnsmapping get ikey  mappingspec
jobconf get hbaseserde hbase_table_default_storage_type
setscan scan
job job   new job jobconf
jobcontext jobcontext   shimloader gethadoopshims   newjobcontext job
path  tablepaths   fileinputformat getinputpaths jobcontext
list<org apache hadoop mapreduce inputsplit> splits
super getsplits jobcontext
inputsplit  results   new inputsplit
for  int i   0  i < splits size    i
results   new hbasesplit  tablesplit  splits get i   tablepaths
return results
private boolean getstorageformatofkey string spec  string defaultformat  throws ioexception
string mapinfo   spec split
boolean tblleveldefault     equalsignorecase defaultformat  ? true   false
switch  mapinfo length
case 1
return tblleveldefault
case 2
string storagetype   mapinfo
if storagetype equals
return tblleveldefault
else if    startswith storagetype
return false
else if    startswith storagetype
return true
default
throw new ioexception     spec