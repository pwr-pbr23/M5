package org apache lucene analysis compound
/**
* licensed to the apache software foundation (asf) under one or more
* contributor license agreements.  see the notice file distributed with
* this work for additional information regarding copyright ownership.
* the asf licenses this file to you under the apache license, version 2.0
* (the "license"); you may not use this file except in compliance with
* the license.  you may obtain a copy of the license at
*
*     http://www.apache.org/licenses/license-2.0
*
* unless required by applicable law or agreed to in writing, software
* distributed under the license is distributed on an "as is" basis,
* without warranties or conditions of any kind, either express or implied.
* see the license for the specific language governing permissions and
* limitations under the license.
*/
import java io file
import java io fileinputstream
import java io inputstreamreader
import java io reader
import java util set
import org apache lucene analysis token
import org apache lucene analysis tokenfilter     for javadocs
import org apache lucene analysis tokenstream
import org apache lucene analysis compound hyphenation hyphenation
import org apache lucene analysis compound hyphenation hyphenationtree
import org xml sax inputsource
/**
* a {@link tokenfilter} that decomposes compound words found in many germanic languages.
* <p>
* "donaudampfschiff" becomes donau, dampf, schiff so that you can find
* "donaudampfschiff" even when you only enter "schiff". it uses a hyphenation
* grammar and a word dictionary to achieve this.
* </p>
*/
public class hyphenationcompoundwordtokenfilter extends
compoundwordtokenfilterbase
private hyphenationtree hyphenator
/**
*
* @param input the {@link tokenstream} to process
* @param hyphenator the hyphenation pattern tree to use for hyphenation
* @param dictionary the word dictionary to match against
* @param minwordsize only words longer than this get processed
* @param minsubwordsize only subwords longer than this get to the output
*        stream
* @param maxsubwordsize only subwords shorter than this get to the output
*        stream
* @param onlylongestmatch add only the longest matching subword to the stream
*/
public hyphenationcompoundwordtokenfilter tokenstream input
hyphenationtree hyphenator  string dictionary  int minwordsize
int minsubwordsize  int maxsubwordsize  boolean onlylongestmatch
this input  hyphenator  makedictionary dictionary   minwordsize
minsubwordsize  maxsubwordsize  onlylongestmatch
/**
*
* @param input the {@link tokenstream} to process
* @param hyphenator the hyphenation pattern tree to use for hyphenation
* @param dictionary the word dictionary to match against
*/
public hyphenationcompoundwordtokenfilter tokenstream input
hyphenationtree hyphenator  string dictionary
this input  hyphenator  makedictionary dictionary   default_min_word_size
default_min_subword_size  default_max_subword_size  false
/**
*
* @param input the {@link tokenstream} to process
* @param hyphenator the hyphenation pattern tree to use for hyphenation
* @param dictionary the word dictionary to match against. if this is a {@link org.apache.lucene.analysis.chararrayset chararrayset} it must have set ignorecase=false and only contain
*        lower case strings.
*/
public hyphenationcompoundwordtokenfilter tokenstream input
hyphenationtree hyphenator  set dictionary
this input  hyphenator  dictionary  default_min_word_size
default_min_subword_size  default_max_subword_size  false
/**
*
* @param input the {@link tokenstream} to process
* @param hyphenator the hyphenation pattern tree to use for hyphenation
* @param dictionary the word dictionary to match against. if this is a {@link org.apache.lucene.analysis.chararrayset chararrayset} it must have set ignorecase=false and only contain
*        lower case strings.
* @param minwordsize only words longer than this get processed
* @param minsubwordsize only subwords longer than this get to the output
*        stream
* @param maxsubwordsize only subwords shorter than this get to the output
*        stream
* @param onlylongestmatch add only the longest matching subword to the stream
*/
public hyphenationcompoundwordtokenfilter tokenstream input
hyphenationtree hyphenator  set dictionary  int minwordsize
int minsubwordsize  int maxsubwordsize  boolean onlylongestmatch
super input  dictionary  minwordsize  minsubwordsize  maxsubwordsize
onlylongestmatch
this hyphenator   hyphenator
/**
* create a hyphenator tree
*
* @param hyphenationfilename the filename of the xml grammar to load
* @return an object representing the hyphenation patterns
* @throws exception
*/
public static hyphenationtree gethyphenationtree string hyphenationfilename
throws exception
return gethyphenationtree new file hyphenationfilename
/**
* create a hyphenator tree
*
* @param hyphenationfile the file of the xml grammar to load
* @return an object representing the hyphenation patterns
* @throws exception
*/
public static hyphenationtree gethyphenationtree file hyphenationfile
throws exception
return gethyphenationtree new inputstreamreader new fileinputstream
hyphenationfile
/**
* create a hyphenator tree
*
* @param hyphenationreader the reader of the xml grammar to load from
* @return an object representing the hyphenation patterns
* @throws exception
*/
public static hyphenationtree gethyphenationtree reader hyphenationreader
throws exception
hyphenationtree tree   new hyphenationtree
tree loadpatterns new inputsource hyphenationreader
return tree
@override
protected void decomposeinternal final token token
// get the hyphenation points
hyphenation hyphens   hyphenator hyphenate token termbuffer    0  token
termlength    1  1
// no hyphen points found -> exit
if  hyphens    null
return
final int hyp   hyphens gethyphenationpoints
char lowercasetermbuffer makelowercasecopy token termbuffer
for  int i   0  i < hyp length    i
int remaining   hyp length   i
int start   hyp
token longestmatchtoken   null
for  int j   1  j < remaining  j
int partlength   hyp   start
// if the part is longer than maxsubwordsize we
// are done with this round
if  partlength > this maxsubwordsize
break
// we only put subwords to the token stream
// that are longer than minpartsize
if  partlength < this minsubwordsize
continue
// check the dictionary
if  dictionary contains lowercasetermbuffer  start  partlength
if  this onlylongestmatch
if  longestmatchtoken    null
if  longestmatchtoken termlength   < partlength
longestmatchtoken   createtoken start  partlength  token
else
longestmatchtoken   createtoken start  partlength  token
else
tokens add createtoken start  partlength  token
else if  dictionary contains lowercasetermbuffer  start
partlength   1
// check the dictionary again with a word that is one character
// shorter
// to avoid problems with genitive 's characters and other binding
// characters
if  this onlylongestmatch
if  longestmatchtoken    null
if  longestmatchtoken termlength   < partlength   1
longestmatchtoken   createtoken start  partlength   1  token
else
longestmatchtoken   createtoken start  partlength   1  token
else
tokens add createtoken start  partlength   1  token
if  this onlylongestmatch    longestmatchtoken  null
tokens add longestmatchtoken